{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import lightning as L\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Network import Generator, Discriminator\n",
    "from Signal_Generator import *\n",
    "from Signal_Analyzer import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(10):\n",
    "    SG = Signal_Generator(num_sources=1, noise_amplitude=1)\n",
    "    signals = SG.generating_signal()\n",
    "    params = SG.printing_parameters()\n",
    "    signal = signals['Signal'].values\n",
    "\n",
    "    signal_tensor = torch.tensor(signal, dtype=torch.float).unsqueeze(0).to(device)\n",
    "    params_tensor = torch.tensor(params, dtype=torch.float).to(device)\n",
    "\n",
    "    dataset.append((signal_tensor, params_tensor))\n",
    "\n",
    "train_loader = utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "num_latent_variables = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "z = torch.randn(1, num_latent_variables, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, dataset, num_latent_variables, lr):\n",
    "        self.dataset = dataset\n",
    "        self.num_latent_variables = num_latent_variables\n",
    "        self.lr = lr\n",
    "\n",
    "        # Networks\n",
    "        self.generator = Generator(in_channels=1, num_latent_variables=num_latent_variables, length=len(signal), num_parameters=len(params)).to(device)\n",
    "        self.discriminator = Discriminator(input_channels=1, length=len(signal), num_parameters=len(params)).to(device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_g = optim.Adam(self.generator.parameters(), lr=self.lr)\n",
    "        self.optimizer_d = optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "\n",
    "    def adversarial_loss(self, output_d, y):\n",
    "        return self.criterion(output_d, y)\n",
    "    \n",
    "    def train_generator(self, signal_tensor, z):\n",
    "        generated_params = self.generator(signal_tensor, z)\n",
    "        fake_output = self.discriminator(signal_tensor, generated_params)\n",
    "        g_loss = self.adversarial_loss(fake_output, torch.ones_like(fake_output))\n",
    "\n",
    "        self.optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_g.step()\n",
    "\n",
    "        return g_loss.item()\n",
    "    \n",
    "    def train_discriminator(self, signal_tensor, params_tensor,z):\n",
    "        fake_params = self.generator(signal_tensor, z).detach()\n",
    "        real_output = self.discriminator(signal_tensor, params_tensor)\n",
    "        fake_output = self.discriminator(signal_tensor, fake_params)\n",
    "\n",
    "        real_loss = self.adversarial_loss(real_output, torch.ones_like(real_output))\n",
    "        fake_loss = self.adversarial_loss(fake_output, torch.zeros_like(fake_output))\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        self.optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_d.step()\n",
    "\n",
    "        return d_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Discriminator loss: 0.5592765212059021\n",
      "Epoch 1, Discriminator loss: 0.4497222900390625\n",
      "Epoch 2, Discriminator loss: 0.08255192637443542\n",
      "Epoch 3, Discriminator loss: 0.08791612833738327\n",
      "Epoch 4, Discriminator loss: 0.06052801385521889\n",
      "Epoch 5, Discriminator loss: 0.02179647982120514\n",
      "Epoch 6, Discriminator loss: 0.026659339666366577\n",
      "Epoch 7, Discriminator loss: 0.02404129132628441\n",
      "Epoch 8, Discriminator loss: 0.013821477070450783\n",
      "Epoch 9, Discriminator loss: 0.010860225185751915\n",
      "Epoch 10, Discriminator loss: 0.008599200285971165\n",
      "Epoch 11, Discriminator loss: 0.008209150284528732\n",
      "Epoch 12, Discriminator loss: 0.006308882497251034\n",
      "Epoch 13, Discriminator loss: 0.008851406164467335\n",
      "Epoch 14, Discriminator loss: 0.004808965139091015\n",
      "Epoch 15, Discriminator loss: 0.004491616040468216\n",
      "Epoch 16, Discriminator loss: 0.006949243135750294\n",
      "Epoch 17, Discriminator loss: 0.006125535350292921\n",
      "Epoch 18, Discriminator loss: 0.003671565093100071\n",
      "Epoch 19, Discriminator loss: 0.0041117118671536446\n",
      "Epoch 20, Discriminator loss: 0.004491832107305527\n",
      "Epoch 21, Discriminator loss: 0.003176887985318899\n",
      "Epoch 22, Discriminator loss: 0.0028450137469917536\n",
      "Epoch 23, Discriminator loss: 0.0023861804511398077\n",
      "Epoch 24, Discriminator loss: 0.0035121948458254337\n",
      "Epoch 25, Discriminator loss: 0.002842096844688058\n",
      "Epoch 26, Discriminator loss: 0.003190494142472744\n",
      "Epoch 27, Discriminator loss: 0.0019676722586154938\n",
      "Epoch 28, Discriminator loss: 0.0018568267114460468\n",
      "Epoch 29, Discriminator loss: 0.001757354591973126\n",
      "Epoch 30, Discriminator loss: 0.001844096346758306\n",
      "Epoch 31, Discriminator loss: 0.0021773776970803738\n",
      "Epoch 32, Discriminator loss: 0.0017526493174955249\n",
      "Epoch 33, Discriminator loss: 0.0015503531321883202\n",
      "Epoch 34, Discriminator loss: 0.0016071629943326116\n",
      "Epoch 35, Discriminator loss: 0.0014607157791033387\n",
      "Epoch 36, Discriminator loss: 0.0015305507695302367\n",
      "Epoch 37, Discriminator loss: 0.0014942074194550514\n",
      "Epoch 38, Discriminator loss: 0.0014294981956481934\n",
      "Epoch 39, Discriminator loss: 0.001316738547757268\n",
      "Epoch 40, Discriminator loss: 0.002010263968259096\n",
      "Epoch 41, Discriminator loss: 0.001313928747549653\n",
      "Epoch 42, Discriminator loss: 0.0018977592699229717\n",
      "Epoch 43, Discriminator loss: 0.0013344591716304421\n",
      "Epoch 44, Discriminator loss: 0.0012894007377326488\n",
      "Epoch 45, Discriminator loss: 0.0016557929338887334\n",
      "Epoch 46, Discriminator loss: 0.0012804365251213312\n",
      "Epoch 47, Discriminator loss: 0.001678247470408678\n",
      "Epoch 48, Discriminator loss: 0.001201409613713622\n",
      "Epoch 49, Discriminator loss: 0.0011848093708977103\n",
      "Epoch 50, Discriminator loss: 0.001192520372569561\n",
      "Epoch 51, Discriminator loss: 0.0014667925424873829\n",
      "Epoch 52, Discriminator loss: 0.0011321547208353877\n",
      "Epoch 53, Discriminator loss: 0.0011245557107031345\n",
      "Epoch 54, Discriminator loss: 0.001103551359847188\n",
      "Epoch 55, Discriminator loss: 0.001024395227432251\n",
      "Epoch 56, Discriminator loss: 0.0010779215954244137\n",
      "Epoch 57, Discriminator loss: 0.0010574252810329199\n",
      "Epoch 58, Discriminator loss: 0.0010447141248732805\n",
      "Epoch 59, Discriminator loss: 0.0010232252534478903\n",
      "Epoch 60, Discriminator loss: 0.000976855168119073\n",
      "Epoch 61, Discriminator loss: 0.0012405647430568933\n",
      "Epoch 62, Discriminator loss: 0.0011041355319321156\n",
      "Epoch 63, Discriminator loss: 0.0009762020781636238\n",
      "Epoch 64, Discriminator loss: 0.0009074182016775012\n",
      "Epoch 65, Discriminator loss: 0.0009230637806467712\n",
      "Epoch 66, Discriminator loss: 0.001031361403875053\n",
      "Epoch 67, Discriminator loss: 0.0011274421121925116\n",
      "Epoch 68, Discriminator loss: 0.0011485842987895012\n",
      "Epoch 69, Discriminator loss: 0.0008379540522582829\n",
      "Epoch 70, Discriminator loss: 0.0008760327473282814\n",
      "Epoch 71, Discriminator loss: 0.0008669967064633965\n",
      "Epoch 72, Discriminator loss: 0.0010867509990930557\n",
      "Epoch 73, Discriminator loss: 0.0008197068818844855\n",
      "Epoch 74, Discriminator loss: 0.0007819129386916757\n",
      "Epoch 75, Discriminator loss: 0.0008032756159082055\n",
      "Epoch 76, Discriminator loss: 0.0008253451669588685\n",
      "Epoch 77, Discriminator loss: 0.0009955863934010267\n",
      "Epoch 78, Discriminator loss: 0.0008065743604674935\n",
      "Epoch 79, Discriminator loss: 0.0008077832171693444\n",
      "Epoch 80, Discriminator loss: 0.0007943860255181789\n",
      "Epoch 81, Discriminator loss: 0.0007872650166973472\n",
      "Epoch 82, Discriminator loss: 0.0007309136562980711\n",
      "Epoch 83, Discriminator loss: 0.0007215348887257278\n",
      "Epoch 84, Discriminator loss: 0.0007401729235425591\n",
      "Epoch 85, Discriminator loss: 0.0007245719898492098\n",
      "Epoch 86, Discriminator loss: 0.0006909360527060926\n",
      "Epoch 87, Discriminator loss: 0.0006981300539337099\n",
      "Epoch 88, Discriminator loss: 0.0008889380842447281\n",
      "Epoch 89, Discriminator loss: 0.0006843962473794818\n",
      "Epoch 90, Discriminator loss: 0.0007019186159595847\n",
      "Epoch 91, Discriminator loss: 0.0007654518121853471\n",
      "Epoch 92, Discriminator loss: 0.0007580365636385977\n",
      "Epoch 93, Discriminator loss: 0.0006805164739489555\n",
      "Epoch 94, Discriminator loss: 0.0008295337902382016\n",
      "Epoch 95, Discriminator loss: 0.0006718338117934763\n",
      "Epoch 96, Discriminator loss: 0.0006340293912217021\n",
      "Epoch 97, Discriminator loss: 0.0006285421550273895\n",
      "Epoch 98, Discriminator loss: 0.000775083783082664\n",
      "Epoch 99, Discriminator loss: 0.000641692487988621\n",
      "Epoch 100, Discriminator loss: 0.0007801869651302695\n",
      "Epoch 101, Discriminator loss: 0.0006175842136144638\n",
      "Epoch 102, Discriminator loss: 0.0006919902516528964\n",
      "Epoch 103, Discriminator loss: 0.0006190015119500458\n",
      "Epoch 104, Discriminator loss: 0.0007329750806093216\n",
      "Epoch 105, Discriminator loss: 0.0006069153314456344\n",
      "Epoch 106, Discriminator loss: 0.0006689332658424973\n",
      "Epoch 107, Discriminator loss: 0.0005764284869655967\n",
      "Epoch 108, Discriminator loss: 0.0005922776181250811\n",
      "Epoch 109, Discriminator loss: 0.0005871134344488382\n",
      "Epoch 110, Discriminator loss: 0.0005865201819688082\n",
      "Epoch 111, Discriminator loss: 0.0005773940356448293\n",
      "Epoch 112, Discriminator loss: 0.000696553208399564\n",
      "Epoch 113, Discriminator loss: 0.000567638548091054\n",
      "Epoch 114, Discriminator loss: 0.0005634746048599482\n",
      "Epoch 115, Discriminator loss: 0.0005587483756244183\n",
      "Epoch 116, Discriminator loss: 0.0005544875166378915\n",
      "Epoch 117, Discriminator loss: 0.0005494697252288461\n",
      "Epoch 118, Discriminator loss: 0.000535704311914742\n",
      "Epoch 119, Discriminator loss: 0.0005402546375989914\n",
      "Epoch 120, Discriminator loss: 0.0005410789744928479\n",
      "Epoch 121, Discriminator loss: 0.0005863762344233692\n",
      "Epoch 122, Discriminator loss: 0.0006230885628610849\n",
      "Epoch 123, Discriminator loss: 0.0005277005839161575\n",
      "Epoch 124, Discriminator loss: 0.0004987168358638883\n",
      "Epoch 125, Discriminator loss: 0.0005657231085933745\n",
      "Epoch 126, Discriminator loss: 0.000502183276694268\n",
      "Epoch 127, Discriminator loss: 0.0005081533454358578\n",
      "Epoch 128, Discriminator loss: 0.00047868245746940374\n",
      "Epoch 129, Discriminator loss: 0.0005030552274547517\n",
      "Epoch 130, Discriminator loss: 0.00047558307414874434\n",
      "Epoch 131, Discriminator loss: 0.0004906594986096025\n",
      "Epoch 132, Discriminator loss: 0.000534645514562726\n",
      "Epoch 133, Discriminator loss: 0.00047605743748135865\n",
      "Epoch 134, Discriminator loss: 0.0005711992271244526\n",
      "Epoch 135, Discriminator loss: 0.0005224336055107415\n",
      "Epoch 136, Discriminator loss: 0.00047501560766249895\n",
      "Epoch 137, Discriminator loss: 0.00047400855692103505\n",
      "Epoch 138, Discriminator loss: 0.0004705415340140462\n",
      "Epoch 139, Discriminator loss: 0.00046378342085517943\n",
      "Epoch 140, Discriminator loss: 0.00045977154513821006\n",
      "Epoch 141, Discriminator loss: 0.00046104041393846273\n",
      "Epoch 142, Discriminator loss: 0.0004361689498182386\n",
      "Epoch 143, Discriminator loss: 0.0004544639668893069\n",
      "Epoch 144, Discriminator loss: 0.00045157596468925476\n",
      "Epoch 145, Discriminator loss: 0.0004372329276520759\n",
      "Epoch 146, Discriminator loss: 0.000442361255409196\n",
      "Epoch 147, Discriminator loss: 0.00043128919787704945\n",
      "Epoch 148, Discriminator loss: 0.00043986013042740524\n",
      "Epoch 149, Discriminator loss: 0.0004716533876489848\n",
      "Epoch 150, Discriminator loss: 0.00046837347326800227\n",
      "Epoch 151, Discriminator loss: 0.00041077297646552324\n",
      "Epoch 152, Discriminator loss: 0.0004081884981133044\n",
      "Epoch 153, Discriminator loss: 0.0004212759668007493\n",
      "Epoch 154, Discriminator loss: 0.00041863787919282913\n",
      "Epoch 155, Discriminator loss: 0.00040041925967670977\n",
      "Epoch 156, Discriminator loss: 0.00047567064757458866\n",
      "Epoch 157, Discriminator loss: 0.0004130811139475554\n",
      "Epoch 158, Discriminator loss: 0.00047397875459864736\n",
      "Epoch 159, Discriminator loss: 0.0003886198974214494\n",
      "Epoch 160, Discriminator loss: 0.00040470229578204453\n",
      "Epoch 161, Discriminator loss: 0.0004028675612062216\n",
      "Epoch 162, Discriminator loss: 0.0004566567367874086\n",
      "Epoch 163, Discriminator loss: 0.0004002311616204679\n",
      "Epoch 164, Discriminator loss: 0.00038722564931958914\n",
      "Epoch 165, Discriminator loss: 0.0003926582576241344\n",
      "Epoch 166, Discriminator loss: 0.00039298617048189044\n",
      "Epoch 167, Discriminator loss: 0.0003717019280884415\n",
      "Epoch 168, Discriminator loss: 0.0004420190234668553\n",
      "Epoch 169, Discriminator loss: 0.0003671733138617128\n",
      "Epoch 170, Discriminator loss: 0.0003650327562354505\n",
      "Epoch 171, Discriminator loss: 0.00038143477286212146\n",
      "Epoch 172, Discriminator loss: 0.0003594827139750123\n",
      "Epoch 173, Discriminator loss: 0.0004244294832460582\n",
      "Epoch 174, Discriminator loss: 0.00037022255128249526\n",
      "Epoch 175, Discriminator loss: 0.00039591031963936985\n",
      "Epoch 176, Discriminator loss: 0.0003522757615428418\n",
      "Epoch 177, Discriminator loss: 0.00035022670635953546\n",
      "Epoch 178, Discriminator loss: 0.0003641789371613413\n",
      "Epoch 179, Discriminator loss: 0.00040740123949944973\n",
      "Epoch 180, Discriminator loss: 0.0003619664057623595\n",
      "Epoch 181, Discriminator loss: 0.0004058083286508918\n",
      "Epoch 182, Discriminator loss: 0.0003554761060513556\n",
      "Epoch 183, Discriminator loss: 0.00039693297003395855\n",
      "Epoch 184, Discriminator loss: 0.00034328296897001565\n",
      "Epoch 185, Discriminator loss: 0.0003953600535169244\n",
      "Epoch 186, Discriminator loss: 0.0003498156147543341\n",
      "Epoch 187, Discriminator loss: 0.0003303997218608856\n",
      "Epoch 188, Discriminator loss: 0.0003435253747738898\n",
      "Epoch 189, Discriminator loss: 0.00033341842936351895\n",
      "Epoch 190, Discriminator loss: 0.0003821046557277441\n",
      "Epoch 191, Discriminator loss: 0.0003293445915915072\n",
      "Epoch 192, Discriminator loss: 0.0003360271221026778\n",
      "Epoch 193, Discriminator loss: 0.00032539505627937615\n",
      "Epoch 194, Discriminator loss: 0.0003344555152580142\n",
      "Epoch 195, Discriminator loss: 0.0003496663994155824\n",
      "Epoch 196, Discriminator loss: 0.00032636825926601887\n",
      "Epoch 197, Discriminator loss: 0.000324662949424237\n",
      "Epoch 198, Discriminator loss: 0.00032283918699249625\n",
      "Epoch 199, Discriminator loss: 0.00030794451595284045\n",
      "Epoch 200, Discriminator loss: 0.0003192352596670389\n",
      "Epoch 201, Discriminator loss: 0.00031086188391782343\n",
      "Epoch 202, Discriminator loss: 0.0003015815746039152\n",
      "Epoch 203, Discriminator loss: 0.00029990673647262156\n",
      "Epoch 204, Discriminator loss: 0.0002982285222969949\n",
      "Epoch 205, Discriminator loss: 0.00034752217470668256\n",
      "Epoch 206, Discriminator loss: 0.00031100481282919645\n",
      "Epoch 207, Discriminator loss: 0.0003112509148195386\n",
      "Epoch 208, Discriminator loss: 0.0002916226803790778\n",
      "Epoch 209, Discriminator loss: 0.0003032792010344565\n",
      "Epoch 210, Discriminator loss: 0.00033544638426974416\n",
      "Epoch 211, Discriminator loss: 0.0002867895527742803\n",
      "Epoch 212, Discriminator loss: 0.00028522947104647756\n",
      "Epoch 213, Discriminator loss: 0.0002899389364756644\n",
      "Epoch 214, Discriminator loss: 0.0003269600565545261\n",
      "Epoch 215, Discriminator loss: 0.0002977343392558396\n",
      "Epoch 216, Discriminator loss: 0.00032388983527198434\n",
      "Epoch 217, Discriminator loss: 0.00029319158056750894\n",
      "Epoch 218, Discriminator loss: 0.0002758570481091738\n",
      "Epoch 219, Discriminator loss: 0.00031816744012758136\n",
      "Epoch 220, Discriminator loss: 0.000301041582133621\n",
      "Epoch 221, Discriminator loss: 0.00027126161148771644\n",
      "Epoch 222, Discriminator loss: 0.0003124227514490485\n",
      "Epoch 223, Discriminator loss: 0.0003098302404396236\n",
      "Epoch 224, Discriminator loss: 0.000265828100964427\n",
      "Epoch 225, Discriminator loss: 0.0002800569636747241\n",
      "Epoch 226, Discriminator loss: 0.00026375820743851364\n",
      "Epoch 227, Discriminator loss: 0.0002613280084915459\n",
      "Epoch 228, Discriminator loss: 0.00027309227152727544\n",
      "Epoch 229, Discriminator loss: 0.00027395045617595315\n",
      "Epoch 230, Discriminator loss: 0.00029701602761633694\n",
      "Epoch 231, Discriminator loss: 0.0002554847742430866\n",
      "Epoch 232, Discriminator loss: 0.0002673278795555234\n",
      "Epoch 233, Discriminator loss: 0.0002525796298868954\n",
      "Epoch 234, Discriminator loss: 0.0002511722850613296\n",
      "Epoch 235, Discriminator loss: 0.00028910182300023735\n",
      "Epoch 236, Discriminator loss: 0.0002498669782653451\n",
      "Epoch 237, Discriminator loss: 0.00027245195815339684\n",
      "Epoch 238, Discriminator loss: 0.0002609564398881048\n",
      "Epoch 239, Discriminator loss: 0.0002694634022191167\n",
      "Epoch 240, Discriminator loss: 0.0002581970766186714\n",
      "Epoch 241, Discriminator loss: 0.00025830266531556845\n",
      "Epoch 242, Discriminator loss: 0.00024199883046094328\n",
      "Epoch 243, Discriminator loss: 0.00027586513897404075\n",
      "Epoch 244, Discriminator loss: 0.00025299598928540945\n",
      "Epoch 245, Discriminator loss: 0.0002604485198389739\n",
      "Epoch 246, Discriminator loss: 0.0002513959479983896\n",
      "Epoch 247, Discriminator loss: 0.0002489604230504483\n",
      "Epoch 248, Discriminator loss: 0.00026768838870339096\n",
      "Epoch 249, Discriminator loss: 0.0002370162692386657\n",
      "Epoch 250, Discriminator loss: 0.00024253877927549183\n",
      "Epoch 251, Discriminator loss: 0.00022995192557573318\n",
      "Epoch 252, Discriminator loss: 0.00024212327843997627\n",
      "Epoch 253, Discriminator loss: 0.00024219509214162827\n",
      "Epoch 254, Discriminator loss: 0.00023978363606147468\n",
      "Epoch 255, Discriminator loss: 0.0002293430152349174\n",
      "Epoch 256, Discriminator loss: 0.00023497676011174917\n",
      "Epoch 257, Discriminator loss: 0.00022238836390897632\n",
      "Epoch 258, Discriminator loss: 0.00022120244102552533\n",
      "Epoch 259, Discriminator loss: 0.00025174758047796786\n",
      "Epoch 260, Discriminator loss: 0.0002315131714567542\n",
      "Epoch 261, Discriminator loss: 0.00024897672119550407\n",
      "Epoch 262, Discriminator loss: 0.0002150595682905987\n",
      "Epoch 263, Discriminator loss: 0.00023590202908962965\n",
      "Epoch 264, Discriminator loss: 0.00021361316612455994\n",
      "Epoch 265, Discriminator loss: 0.000217002525459975\n",
      "Epoch 266, Discriminator loss: 0.000231990561587736\n",
      "Epoch 267, Discriminator loss: 0.00022250093752518296\n",
      "Epoch 268, Discriminator loss: 0.0002217493747593835\n",
      "Epoch 269, Discriminator loss: 0.00023800786584615707\n",
      "Epoch 270, Discriminator loss: 0.00022037385497242212\n",
      "Epoch 271, Discriminator loss: 0.00021917153208050877\n",
      "Epoch 272, Discriminator loss: 0.00021609183750115335\n",
      "Epoch 273, Discriminator loss: 0.00023313274141401052\n",
      "Epoch 274, Discriminator loss: 0.00022208408336155117\n",
      "Epoch 275, Discriminator loss: 0.00022086032549850643\n",
      "Epoch 276, Discriminator loss: 0.00019998911011498421\n",
      "Epoch 277, Discriminator loss: 0.00021840163390152156\n",
      "Epoch 278, Discriminator loss: 0.00021104590268805623\n",
      "Epoch 279, Discriminator loss: 0.00022552371956408024\n",
      "Epoch 280, Discriminator loss: 0.00021487082995008677\n",
      "Epoch 281, Discriminator loss: 0.00019471763516776264\n",
      "Epoch 282, Discriminator loss: 0.00021247206314001232\n",
      "Epoch 283, Discriminator loss: 0.0002033421624219045\n",
      "Epoch 284, Discriminator loss: 0.0002044361608568579\n",
      "Epoch 285, Discriminator loss: 0.00019469481776468456\n",
      "Epoch 286, Discriminator loss: 0.00021729303989559412\n",
      "Epoch 287, Discriminator loss: 0.0001926285185618326\n",
      "Epoch 288, Discriminator loss: 0.0002057449019048363\n",
      "Epoch 289, Discriminator loss: 0.00019061286002397537\n",
      "Epoch 290, Discriminator loss: 0.00019398982112761587\n",
      "Epoch 291, Discriminator loss: 0.0002114924427587539\n",
      "Epoch 292, Discriminator loss: 0.00021028760238550603\n",
      "Epoch 293, Discriminator loss: 0.00019264899310655892\n",
      "Epoch 294, Discriminator loss: 0.00018134481797460467\n",
      "Epoch 295, Discriminator loss: 0.00019248804892413318\n",
      "Epoch 296, Discriminator loss: 0.00019130164582747966\n",
      "Epoch 297, Discriminator loss: 0.00018868656479753554\n",
      "Epoch 298, Discriminator loss: 0.00018757538055069745\n",
      "Epoch 299, Discriminator loss: 0.0001882798533188179\n",
      "Epoch 300, Discriminator loss: 0.00020156928803771734\n",
      "Epoch 301, Discriminator loss: 0.00018466927576810122\n",
      "Epoch 302, Discriminator loss: 0.00019101101497653872\n",
      "Epoch 303, Discriminator loss: 0.00018425413873046637\n",
      "Epoch 304, Discriminator loss: 0.00017208413919433951\n",
      "Epoch 305, Discriminator loss: 0.00017173259402625263\n",
      "Epoch 306, Discriminator loss: 0.00017805493553169072\n",
      "Epoch 307, Discriminator loss: 0.00018033833475783467\n",
      "Epoch 308, Discriminator loss: 0.00019292396609671414\n",
      "Epoch 309, Discriminator loss: 0.00016813783440738916\n",
      "Epoch 310, Discriminator loss: 0.0001707299961708486\n",
      "Epoch 311, Discriminator loss: 0.00016637827502563596\n",
      "Epoch 312, Discriminator loss: 0.00017424917314201593\n",
      "Epoch 313, Discriminator loss: 0.00017332281277049333\n",
      "Epoch 314, Discriminator loss: 0.00017938984092324972\n",
      "Epoch 315, Discriminator loss: 0.00016257815877906978\n",
      "Epoch 316, Discriminator loss: 0.00018494081450626254\n",
      "Epoch 317, Discriminator loss: 0.00017655747069511563\n",
      "Epoch 318, Discriminator loss: 0.0001637917011976242\n",
      "Epoch 319, Discriminator loss: 0.0001661969581618905\n",
      "Epoch 320, Discriminator loss: 0.00016664159193169326\n",
      "Epoch 321, Discriminator loss: 0.0001657409593462944\n",
      "Epoch 322, Discriminator loss: 0.00016040648915804923\n",
      "Epoch 323, Discriminator loss: 0.00015598123718518764\n",
      "Epoch 324, Discriminator loss: 0.00016308743215631694\n",
      "Epoch 325, Discriminator loss: 0.0001622488780412823\n",
      "Epoch 326, Discriminator loss: 0.00016147398855537176\n",
      "Epoch 327, Discriminator loss: 0.00015629062545485795\n",
      "Epoch 328, Discriminator loss: 0.00015975601854734123\n",
      "Epoch 329, Discriminator loss: 0.0001600573305040598\n",
      "Epoch 330, Discriminator loss: 0.0001591472391737625\n",
      "Epoch 331, Discriminator loss: 0.00015720966621302068\n",
      "Epoch 332, Discriminator loss: 0.00016327336197718978\n",
      "Epoch 333, Discriminator loss: 0.00015554987476207316\n",
      "Epoch 334, Discriminator loss: 0.00015474960673600435\n",
      "Epoch 335, Discriminator loss: 0.0001671829668339342\n",
      "Epoch 336, Discriminator loss: 0.00015192996943369508\n",
      "Epoch 337, Discriminator loss: 0.00015232940495479852\n",
      "Epoch 338, Discriminator loss: 0.00014758863835595548\n",
      "Epoch 339, Discriminator loss: 0.0001435982558177784\n",
      "Epoch 340, Discriminator loss: 0.00014998961705714464\n",
      "Epoch 341, Discriminator loss: 0.0001491818984504789\n",
      "Epoch 342, Discriminator loss: 0.0001413780846633017\n",
      "Epoch 343, Discriminator loss: 0.0001485272659920156\n",
      "Epoch 344, Discriminator loss: 0.00014681540778838098\n",
      "Epoch 345, Discriminator loss: 0.00013949534331914037\n",
      "Epoch 346, Discriminator loss: 0.0001416086161043495\n",
      "Epoch 347, Discriminator loss: 0.00013775285333395004\n",
      "Epoch 348, Discriminator loss: 0.00013733209925703704\n",
      "Epoch 349, Discriminator loss: 0.00014305963122751564\n",
      "Epoch 350, Discriminator loss: 0.00014864993863739073\n",
      "Epoch 351, Discriminator loss: 0.00014163870946504176\n",
      "Epoch 352, Discriminator loss: 0.00014165492029860616\n",
      "Epoch 353, Discriminator loss: 0.00014012714382261038\n",
      "Epoch 354, Discriminator loss: 0.00013821557513438165\n",
      "Epoch 355, Discriminator loss: 0.00013944727834314108\n",
      "Epoch 356, Discriminator loss: 0.00014960308908484876\n",
      "Epoch 357, Discriminator loss: 0.0001311491650994867\n",
      "Epoch 358, Discriminator loss: 0.00014802911027800292\n",
      "Epoch 359, Discriminator loss: 0.00013659836258739233\n",
      "Epoch 360, Discriminator loss: 0.00013170987949706614\n",
      "Epoch 361, Discriminator loss: 0.00014569275663234293\n",
      "Epoch 362, Discriminator loss: 0.00013360285083763301\n",
      "Epoch 363, Discriminator loss: 0.00012966890062671155\n",
      "Epoch 364, Discriminator loss: 0.00013224923168309033\n",
      "Epoch 365, Discriminator loss: 0.00013054642477072775\n",
      "Epoch 366, Discriminator loss: 0.0001316897541983053\n",
      "Epoch 367, Discriminator loss: 0.00013035314623266459\n",
      "Epoch 368, Discriminator loss: 0.0001303519675275311\n",
      "Epoch 369, Discriminator loss: 0.00012296010390855372\n",
      "Epoch 370, Discriminator loss: 0.00012509258522186428\n",
      "Epoch 371, Discriminator loss: 0.00012203669757582247\n",
      "Epoch 372, Discriminator loss: 0.00013223031419329345\n",
      "Epoch 373, Discriminator loss: 0.00013149129517842084\n",
      "Epoch 374, Discriminator loss: 0.00013583307736553252\n",
      "Epoch 375, Discriminator loss: 0.00011953430657740682\n",
      "Epoch 376, Discriminator loss: 0.00012327596778050065\n",
      "Epoch 377, Discriminator loss: 0.00012371812772471458\n",
      "Epoch 378, Discriminator loss: 0.00013283060980029404\n",
      "Epoch 379, Discriminator loss: 0.00012139212049078196\n",
      "Epoch 380, Discriminator loss: 0.00012675861944444478\n",
      "Epoch 381, Discriminator loss: 0.00011818970961030573\n",
      "Epoch 382, Discriminator loss: 0.00013016341836191714\n",
      "Epoch 383, Discriminator loss: 0.00011995778186246753\n",
      "Epoch 384, Discriminator loss: 0.00011828116839751601\n",
      "Epoch 385, Discriminator loss: 0.0001280648575630039\n",
      "Epoch 386, Discriminator loss: 0.00011264144268352538\n",
      "Epoch 387, Discriminator loss: 0.0001145812202594243\n",
      "Epoch 388, Discriminator loss: 0.00012156620505265892\n",
      "Epoch 389, Discriminator loss: 0.00011527072638273239\n",
      "Epoch 390, Discriminator loss: 0.00011032215843442827\n",
      "Epoch 391, Discriminator loss: 0.00012400066771078855\n",
      "Epoch 392, Discriminator loss: 0.00011444816482253373\n",
      "Epoch 393, Discriminator loss: 0.00011293930583633482\n",
      "Epoch 394, Discriminator loss: 0.00010807962098624557\n",
      "Epoch 395, Discriminator loss: 0.00011266034562140703\n",
      "Epoch 396, Discriminator loss: 0.00011211083619855344\n",
      "Epoch 397, Discriminator loss: 0.00011151816579513252\n",
      "Epoch 398, Discriminator loss: 0.00010823443881236017\n",
      "Epoch 399, Discriminator loss: 0.00010944125097012147\n",
      "Epoch 400, Discriminator loss: 0.00010889650729950517\n",
      "Epoch 401, Discriminator loss: 0.00010974227916449308\n",
      "Epoch 402, Discriminator loss: 0.00011675828136503696\n",
      "Epoch 403, Discriminator loss: 0.00010319084685761482\n",
      "Epoch 404, Discriminator loss: 0.00011573352821869776\n",
      "Epoch 405, Discriminator loss: 0.00011125949095003307\n",
      "Epoch 406, Discriminator loss: 0.00011451597674749792\n",
      "Epoch 407, Discriminator loss: 0.00010634686623234302\n",
      "Epoch 408, Discriminator loss: 0.00010447313979966566\n",
      "Epoch 409, Discriminator loss: 0.00010478211333975196\n",
      "Epoch 410, Discriminator loss: 0.00011184767208760604\n",
      "Epoch 411, Discriminator loss: 0.00010783025936689228\n",
      "Epoch 412, Discriminator loss: 0.00010321085574105382\n",
      "Epoch 413, Discriminator loss: 9.802430577110499e-05\n",
      "Epoch 414, Discriminator loss: 9.970125393010676e-05\n",
      "Epoch 415, Discriminator loss: 0.00010560015653027222\n",
      "Epoch 416, Discriminator loss: 0.00010830485553015023\n",
      "Epoch 417, Discriminator loss: 9.60275501711294e-05\n",
      "Epoch 418, Discriminator loss: 0.00010000036854762584\n",
      "Epoch 419, Discriminator loss: 9.529197996016592e-05\n",
      "Epoch 420, Discriminator loss: 0.00010639718675520271\n",
      "Epoch 421, Discriminator loss: 0.00010546029079705477\n",
      "Epoch 422, Discriminator loss: 9.7859330708161e-05\n",
      "Epoch 423, Discriminator loss: 9.307957225246355e-05\n",
      "Epoch 424, Discriminator loss: 0.00010065028618555516\n",
      "Epoch 425, Discriminator loss: 9.641267388360575e-05\n",
      "Epoch 426, Discriminator loss: 9.636166214477271e-05\n",
      "Epoch 427, Discriminator loss: 9.525028144707903e-05\n",
      "Epoch 428, Discriminator loss: 9.850470087258145e-05\n",
      "Epoch 429, Discriminator loss: 9.042730380315334e-05\n",
      "Epoch 430, Discriminator loss: 0.00010054322774522007\n",
      "Epoch 431, Discriminator loss: 9.116993169300258e-05\n",
      "Epoch 432, Discriminator loss: 9.975599823519588e-05\n",
      "Epoch 433, Discriminator loss: 9.171692363452166e-05\n",
      "Epoch 434, Discriminator loss: 9.186784154735506e-05\n",
      "Epoch 435, Discriminator loss: 9.485959162702784e-05\n",
      "Epoch 436, Discriminator loss: 9.770150063559413e-05\n",
      "Epoch 437, Discriminator loss: 8.671198156662285e-05\n",
      "Epoch 438, Discriminator loss: 9.661942021921277e-05\n",
      "Epoch 439, Discriminator loss: 9.61028563324362e-05\n",
      "Epoch 440, Discriminator loss: 8.915032958611846e-05\n",
      "Epoch 441, Discriminator loss: 8.856315980665386e-05\n",
      "Epoch 442, Discriminator loss: 9.139085159404203e-05\n",
      "Epoch 443, Discriminator loss: 8.566865290049464e-05\n",
      "Epoch 444, Discriminator loss: 9.358840907225385e-05\n",
      "Epoch 445, Discriminator loss: 9.308863081969321e-05\n",
      "Epoch 446, Discriminator loss: 8.952025382313877e-05\n",
      "Epoch 447, Discriminator loss: 8.595666440669447e-05\n",
      "Epoch 448, Discriminator loss: 8.548390178475529e-05\n",
      "Epoch 449, Discriminator loss: 8.811379666440189e-05\n",
      "Epoch 450, Discriminator loss: 8.086895104497671e-05\n",
      "Epoch 451, Discriminator loss: 9.013459202833474e-05\n",
      "Epoch 452, Discriminator loss: 8.671886462252587e-05\n",
      "Epoch 453, Discriminator loss: 8.9213193859905e-05\n",
      "Epoch 454, Discriminator loss: 8.093391079455614e-05\n",
      "Epoch 455, Discriminator loss: 8.174462709575891e-05\n",
      "Epoch 456, Discriminator loss: 8.777718176133931e-05\n",
      "Epoch 457, Discriminator loss: 7.820450264262035e-05\n",
      "Epoch 458, Discriminator loss: 8.400602382607758e-05\n",
      "Epoch 459, Discriminator loss: 8.61766457092017e-05\n",
      "Epoch 460, Discriminator loss: 8.569801866542548e-05\n",
      "Epoch 461, Discriminator loss: 8.27482872409746e-05\n",
      "Epoch 462, Discriminator loss: 7.948940037749708e-05\n",
      "Epoch 463, Discriminator loss: 8.462245023110881e-05\n",
      "Epoch 464, Discriminator loss: 7.682535942876711e-05\n",
      "Epoch 465, Discriminator loss: 7.642235141247511e-05\n",
      "Epoch 466, Discriminator loss: 7.464834925485775e-05\n",
      "Epoch 467, Discriminator loss: 7.78583635110408e-05\n",
      "Epoch 468, Discriminator loss: 7.646861195098609e-05\n",
      "Epoch 469, Discriminator loss: 8.178426651284099e-05\n",
      "Epoch 470, Discriminator loss: 7.620287942700088e-05\n",
      "Epoch 471, Discriminator loss: 7.62612180551514e-05\n",
      "Epoch 472, Discriminator loss: 7.48418242437765e-05\n",
      "Epoch 473, Discriminator loss: 7.547119457740337e-05\n",
      "Epoch 474, Discriminator loss: 7.7287302701734e-05\n",
      "Epoch 475, Discriminator loss: 7.468185503967106e-05\n",
      "Epoch 476, Discriminator loss: 7.21877659088932e-05\n",
      "Epoch 477, Discriminator loss: 7.027160609140992e-05\n",
      "Epoch 478, Discriminator loss: 7.81451744842343e-05\n",
      "Epoch 479, Discriminator loss: 7.277866097865626e-05\n",
      "Epoch 480, Discriminator loss: 7.730886864010245e-05\n",
      "Epoch 481, Discriminator loss: 7.236280362121761e-05\n",
      "Epoch 482, Discriminator loss: 6.994058639975265e-05\n",
      "Epoch 483, Discriminator loss: 7.61275805416517e-05\n",
      "Epoch 484, Discriminator loss: 7.032015855656937e-05\n",
      "Epoch 485, Discriminator loss: 6.739950913470238e-05\n",
      "Epoch 486, Discriminator loss: 7.259103585965931e-05\n",
      "Epoch 487, Discriminator loss: 6.920316081959754e-05\n",
      "Epoch 488, Discriminator loss: 6.945361383259296e-05\n",
      "Epoch 489, Discriminator loss: 7.14228954166174e-05\n",
      "Epoch 490, Discriminator loss: 7.312084926525131e-05\n",
      "Epoch 491, Discriminator loss: 7.275401003425941e-05\n",
      "Epoch 492, Discriminator loss: 7.232172356452793e-05\n",
      "Epoch 493, Discriminator loss: 6.465903425123543e-05\n",
      "Epoch 494, Discriminator loss: 7.17850198270753e-05\n",
      "Epoch 495, Discriminator loss: 6.534891144838184e-05\n",
      "Epoch 496, Discriminator loss: 6.504157499875873e-05\n",
      "Epoch 497, Discriminator loss: 7.069415732985362e-05\n",
      "Epoch 498, Discriminator loss: 7.034471491351724e-05\n",
      "Epoch 499, Discriminator loss: 6.401563587132841e-05\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(dataset, num_latent_variables=num_latent_variables, lr=learning_rate)\n",
    "\n",
    "d_loss_list = []\n",
    "# Train the GAN\n",
    "for i in range(500):\n",
    "    for _, (signal_tensor, params_tensor) in enumerate(train_loader):\n",
    "        d_loss = gan.train_discriminator(signal_tensor, params_tensor, z)\n",
    "        d_loss_list.append(d_loss)\n",
    "    print(f\"Epoch {i}, Discriminator loss: {d_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(d_loss_list)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Discriminator loss: 0.5722998380661011\n",
      "Epoch 1, Discriminator loss: 0.25043636560440063\n",
      "Epoch 2, Discriminator loss: 0.12622447311878204\n",
      "Epoch 3, Discriminator loss: 0.051160458475351334\n",
      "Epoch 4, Discriminator loss: 0.032664284110069275\n",
      "Epoch 5, Discriminator loss: 0.021131392568349838\n",
      "Epoch 6, Discriminator loss: 0.025515258312225342\n",
      "Epoch 7, Discriminator loss: 0.0205144714564085\n",
      "Epoch 8, Discriminator loss: 0.013226628303527832\n",
      "Epoch 9, Discriminator loss: 0.00998636893928051\n",
      "Epoch 10, Discriminator loss: 0.009945468977093697\n",
      "Epoch 11, Discriminator loss: 0.008821764960885048\n",
      "Epoch 12, Discriminator loss: 0.007402450777590275\n",
      "Epoch 13, Discriminator loss: 0.007194550707936287\n",
      "Epoch 14, Discriminator loss: 0.006827806122601032\n",
      "Epoch 15, Discriminator loss: 0.005771579220890999\n",
      "Epoch 16, Discriminator loss: 0.00645724032074213\n",
      "Epoch 17, Discriminator loss: 0.007076999172568321\n",
      "Epoch 18, Discriminator loss: 0.00498478626832366\n",
      "Epoch 19, Discriminator loss: 0.004549791105091572\n",
      "Epoch 20, Discriminator loss: 0.004426771774888039\n",
      "Epoch 21, Discriminator loss: 0.005198548547923565\n",
      "Epoch 22, Discriminator loss: 0.005342606455087662\n",
      "Epoch 23, Discriminator loss: 0.00377444364130497\n",
      "Epoch 24, Discriminator loss: 0.004296081140637398\n",
      "Epoch 25, Discriminator loss: 0.004641184117645025\n",
      "Epoch 26, Discriminator loss: 0.00400940328836441\n",
      "Epoch 27, Discriminator loss: 0.0032971855252981186\n",
      "Epoch 28, Discriminator loss: 0.00319910841062665\n",
      "Epoch 29, Discriminator loss: 0.0036635850556194782\n",
      "Epoch 30, Discriminator loss: 0.003515254706144333\n",
      "Epoch 31, Discriminator loss: 0.0028050043620169163\n",
      "Epoch 32, Discriminator loss: 0.0033930963836610317\n",
      "Epoch 33, Discriminator loss: 0.0025161858648061752\n",
      "Epoch 34, Discriminator loss: 0.00330725428648293\n",
      "Epoch 35, Discriminator loss: 0.0023413607850670815\n",
      "Epoch 36, Discriminator loss: 0.0023657530546188354\n",
      "Epoch 37, Discriminator loss: 0.002280937507748604\n",
      "Epoch 38, Discriminator loss: 0.002219662768766284\n",
      "Epoch 39, Discriminator loss: 0.0021973124239593744\n",
      "Epoch 40, Discriminator loss: 0.0021503472235053778\n",
      "Epoch 41, Discriminator loss: 0.0021028942428529263\n",
      "Epoch 42, Discriminator loss: 0.002057021716609597\n",
      "Epoch 43, Discriminator loss: 0.0025784485042095184\n",
      "Epoch 44, Discriminator loss: 0.002299952320754528\n",
      "Epoch 45, Discriminator loss: 0.002250642515718937\n",
      "Epoch 46, Discriminator loss: 0.0019146313425153494\n",
      "Epoch 47, Discriminator loss: 0.0018557764124125242\n",
      "Epoch 48, Discriminator loss: 0.0018734014593064785\n",
      "Epoch 49, Discriminator loss: 0.0018413546495139599\n",
      "Epoch 50, Discriminator loss: 0.0017832971643656492\n",
      "Epoch 51, Discriminator loss: 0.0017662250902503729\n",
      "Epoch 52, Discriminator loss: 0.001709566917270422\n",
      "Epoch 53, Discriminator loss: 0.0017216774867847562\n",
      "Epoch 54, Discriminator loss: 0.001845342805609107\n",
      "Epoch 55, Discriminator loss: 0.0016185264103114605\n",
      "Epoch 56, Discriminator loss: 0.001568794483318925\n",
      "Epoch 57, Discriminator loss: 0.0018672285368666053\n",
      "Epoch 58, Discriminator loss: 0.0017136516980826855\n",
      "Epoch 59, Discriminator loss: 0.0015182967763394117\n",
      "Epoch 60, Discriminator loss: 0.0014731119154021144\n",
      "Epoch 61, Discriminator loss: 0.0014645219780504704\n",
      "Epoch 62, Discriminator loss: 0.0017137122340500355\n",
      "Epoch 63, Discriminator loss: 0.0014099786058068275\n",
      "Epoch 64, Discriminator loss: 0.001659185509197414\n",
      "Epoch 65, Discriminator loss: 0.0013601821847259998\n",
      "Epoch 66, Discriminator loss: 0.0013725768076255918\n",
      "Epoch 67, Discriminator loss: 0.0015855204546824098\n",
      "Epoch 68, Discriminator loss: 0.0013000384205952287\n",
      "Epoch 69, Discriminator loss: 0.001493483898229897\n",
      "Epoch 70, Discriminator loss: 0.0012914747931063175\n",
      "Epoch 71, Discriminator loss: 0.0012700462248176336\n",
      "Epoch 72, Discriminator loss: 0.0012655644677579403\n",
      "Epoch 73, Discriminator loss: 0.0012524917256087065\n",
      "Epoch 74, Discriminator loss: 0.0013968907296657562\n",
      "Epoch 75, Discriminator loss: 0.001213075127452612\n",
      "Epoch 76, Discriminator loss: 0.001207032473757863\n",
      "Epoch 77, Discriminator loss: 0.0011797272600233555\n",
      "Epoch 78, Discriminator loss: 0.0013629280729219317\n",
      "Epoch 79, Discriminator loss: 0.001162754837423563\n",
      "Epoch 80, Discriminator loss: 0.0011400100775063038\n",
      "Epoch 81, Discriminator loss: 0.0011245496571063995\n",
      "Epoch 82, Discriminator loss: 0.001111437100917101\n",
      "Epoch 83, Discriminator loss: 0.0010883049108088017\n",
      "Epoch 84, Discriminator loss: 0.0012651155702769756\n",
      "Epoch 85, Discriminator loss: 0.0011924392310902476\n",
      "Epoch 86, Discriminator loss: 0.0012332105543464422\n",
      "Epoch 87, Discriminator loss: 0.0011939655523747206\n",
      "Epoch 88, Discriminator loss: 0.001038694754242897\n",
      "Epoch 89, Discriminator loss: 0.0010163405677303672\n",
      "Epoch 90, Discriminator loss: 0.0010276214452460408\n",
      "Epoch 91, Discriminator loss: 0.0010054603917524219\n",
      "Epoch 92, Discriminator loss: 0.0010046228999271989\n",
      "Epoch 93, Discriminator loss: 0.0009941923199221492\n",
      "Epoch 94, Discriminator loss: 0.000979571952484548\n",
      "Epoch 95, Discriminator loss: 0.0009538544691167772\n",
      "Epoch 96, Discriminator loss: 0.0009443714516237378\n",
      "Epoch 97, Discriminator loss: 0.0010419694008305669\n",
      "Epoch 98, Discriminator loss: 0.0009419159614481032\n",
      "Epoch 99, Discriminator loss: 0.0010433372808620334\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(dataset, num_latent_variables=num_latent_variables, lr=learning_rate)\n",
    "\n",
    "d_loss_list = []\n",
    "g_loss_list = []\n",
    "\n",
    "# Train the Discriminator and Generator\n",
    "for epoch in range(100):\n",
    "    for _, (signal_tensor, params_tensor) in enumerate(train_loader):\n",
    "        d_loss = gan.train_discriminator(signal_tensor, params_tensor, z)\n",
    "        d_loss_list.append(d_loss)\n",
    "    print(f\"Epoch {epoch}, Discriminator loss: {d_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Generator loss: 0.6553518176078796, Discriminator loss: 0.7012932300567627\n",
      "Epoch 1, Generator loss: 0.7248415350914001, Discriminator loss: 0.6880601644515991\n",
      "Epoch 2, Generator loss: 0.6484228372573853, Discriminator loss: 0.7209388613700867\n",
      "Epoch 3, Generator loss: 0.6551143527030945, Discriminator loss: 0.7161864042282104\n",
      "Epoch 4, Generator loss: 0.654058575630188, Discriminator loss: 0.7087373733520508\n",
      "Epoch 5, Generator loss: 0.6727005839347839, Discriminator loss: 0.7049639225006104\n",
      "Epoch 6, Generator loss: 0.6823297142982483, Discriminator loss: 0.7007737159729004\n",
      "Epoch 7, Generator loss: 1.2063794136047363, Discriminator loss: 0.5413963794708252\n",
      "Epoch 8, Generator loss: 0.36486077308654785, Discriminator loss: 0.859184741973877\n",
      "Epoch 9, Generator loss: 0.840074360370636, Discriminator loss: 0.6640529632568359\n",
      "Epoch 10, Generator loss: 0.7570613026618958, Discriminator loss: 0.6479033827781677\n",
      "Epoch 11, Generator loss: 0.7277777194976807, Discriminator loss: 0.6707155704498291\n",
      "Epoch 12, Generator loss: 0.5667095184326172, Discriminator loss: 0.7689229249954224\n",
      "Epoch 13, Generator loss: 1.180048942565918, Discriminator loss: 0.5084678530693054\n",
      "Epoch 14, Generator loss: 0.5873767733573914, Discriminator loss: 0.7038255929946899\n",
      "Epoch 15, Generator loss: 0.6214898228645325, Discriminator loss: 0.6875813603401184\n",
      "Epoch 16, Generator loss: 0.841584324836731, Discriminator loss: 0.6498652696609497\n",
      "Epoch 17, Generator loss: 0.7350080609321594, Discriminator loss: 0.6971108913421631\n",
      "Epoch 18, Generator loss: 0.5669424533843994, Discriminator loss: 0.703094482421875\n",
      "Epoch 19, Generator loss: 0.9699716567993164, Discriminator loss: 0.6082206964492798\n",
      "Epoch 20, Generator loss: 0.5786539912223816, Discriminator loss: 0.8112655878067017\n",
      "Epoch 21, Generator loss: 0.6016650795936584, Discriminator loss: 0.6927484273910522\n",
      "Epoch 22, Generator loss: 0.9729962348937988, Discriminator loss: 0.5945127010345459\n",
      "Epoch 23, Generator loss: 0.9103958010673523, Discriminator loss: 0.6049553155899048\n",
      "Epoch 24, Generator loss: 0.5924345254898071, Discriminator loss: 0.7784056067466736\n",
      "Epoch 25, Generator loss: 0.570123553276062, Discriminator loss: 0.7088303565979004\n",
      "Epoch 26, Generator loss: 1.038912296295166, Discriminator loss: 0.6087629795074463\n",
      "Epoch 27, Generator loss: 0.7099557518959045, Discriminator loss: 0.8401101231575012\n",
      "Epoch 28, Generator loss: 0.6423559188842773, Discriminator loss: 0.6398863792419434\n",
      "Epoch 29, Generator loss: 0.9136415123939514, Discriminator loss: 0.6423972845077515\n",
      "Epoch 30, Generator loss: 0.8309084177017212, Discriminator loss: 0.6017871499061584\n",
      "Epoch 31, Generator loss: 0.567395806312561, Discriminator loss: 0.7022672891616821\n",
      "Epoch 32, Generator loss: 0.764330267906189, Discriminator loss: 0.644557774066925\n",
      "Epoch 33, Generator loss: 0.8422949910163879, Discriminator loss: 0.785795271396637\n",
      "Epoch 34, Generator loss: 0.5185085535049438, Discriminator loss: 0.720517098903656\n",
      "Epoch 35, Generator loss: 0.910078227519989, Discriminator loss: 0.6097186803817749\n",
      "Epoch 36, Generator loss: 0.83115154504776, Discriminator loss: 0.6081454157829285\n",
      "Epoch 37, Generator loss: 0.7542170882225037, Discriminator loss: 0.6438934803009033\n",
      "Epoch 38, Generator loss: 0.58148592710495, Discriminator loss: 0.705994725227356\n",
      "Epoch 39, Generator loss: 1.3669697046279907, Discriminator loss: 0.6342166662216187\n",
      "Epoch 40, Generator loss: 0.6283915042877197, Discriminator loss: 0.6862654685974121\n",
      "Epoch 41, Generator loss: 0.630556583404541, Discriminator loss: 0.9633790254592896\n",
      "Epoch 42, Generator loss: 0.9184104204177856, Discriminator loss: 0.5572712421417236\n",
      "Epoch 43, Generator loss: 0.9372191429138184, Discriminator loss: 0.6005153656005859\n",
      "Epoch 44, Generator loss: 0.601055383682251, Discriminator loss: 0.6868675947189331\n",
      "Epoch 45, Generator loss: 0.9737670421600342, Discriminator loss: 0.6386412382125854\n",
      "Epoch 46, Generator loss: 0.5177884101867676, Discriminator loss: 0.7589858174324036\n",
      "Epoch 47, Generator loss: 0.7448083162307739, Discriminator loss: 0.7933846712112427\n",
      "Epoch 48, Generator loss: 0.532870352268219, Discriminator loss: 0.7109538912773132\n",
      "Epoch 49, Generator loss: 0.5968010425567627, Discriminator loss: 0.699278712272644\n",
      "Epoch 50, Generator loss: 0.680388867855072, Discriminator loss: 0.6874055862426758\n",
      "Epoch 51, Generator loss: 0.7487462162971497, Discriminator loss: 0.6842846274375916\n",
      "Epoch 52, Generator loss: 0.7657865285873413, Discriminator loss: 0.6828961968421936\n",
      "Epoch 53, Generator loss: 0.7448168992996216, Discriminator loss: 0.6842305660247803\n",
      "Epoch 54, Generator loss: 0.7207983136177063, Discriminator loss: 0.6844233274459839\n",
      "Epoch 55, Generator loss: 0.7249374389648438, Discriminator loss: 0.6669957637786865\n",
      "Epoch 56, Generator loss: 0.6835724711418152, Discriminator loss: 0.6652814745903015\n",
      "Epoch 57, Generator loss: 0.7457247972488403, Discriminator loss: 0.6862385869026184\n",
      "Epoch 58, Generator loss: 0.6589008569717407, Discriminator loss: 0.7246126532554626\n",
      "Epoch 59, Generator loss: 0.5829461216926575, Discriminator loss: 0.6611865758895874\n",
      "Epoch 60, Generator loss: 0.5716152191162109, Discriminator loss: 0.6061539649963379\n",
      "Epoch 61, Generator loss: 0.7669755220413208, Discriminator loss: 0.5811113119125366\n",
      "Epoch 62, Generator loss: 0.8158909678459167, Discriminator loss: 0.554317831993103\n",
      "Epoch 63, Generator loss: 0.8012940287590027, Discriminator loss: 0.6665147542953491\n",
      "Epoch 64, Generator loss: 0.5280928611755371, Discriminator loss: 0.7935030460357666\n",
      "Epoch 65, Generator loss: 0.9351664781570435, Discriminator loss: 0.5872704982757568\n",
      "Epoch 66, Generator loss: 1.3197166919708252, Discriminator loss: 0.5333110094070435\n",
      "Epoch 67, Generator loss: 0.8347005844116211, Discriminator loss: 0.5989866256713867\n",
      "Epoch 68, Generator loss: 0.5194141268730164, Discriminator loss: 0.8736684322357178\n",
      "Epoch 69, Generator loss: 1.2098402976989746, Discriminator loss: 0.4659063220024109\n",
      "Epoch 70, Generator loss: 0.4526057839393616, Discriminator loss: 0.7796014547348022\n",
      "Epoch 71, Generator loss: 1.0055274963378906, Discriminator loss: 0.6576886773109436\n",
      "Epoch 72, Generator loss: 0.6800007820129395, Discriminator loss: 0.7382580041885376\n",
      "Epoch 73, Generator loss: 0.7456963658332825, Discriminator loss: 0.6365479230880737\n",
      "Epoch 74, Generator loss: 0.8480156064033508, Discriminator loss: 0.925997257232666\n",
      "Epoch 75, Generator loss: 1.1240464448928833, Discriminator loss: 0.5071101188659668\n",
      "Epoch 76, Generator loss: 0.6344173550605774, Discriminator loss: 0.7013429403305054\n",
      "Epoch 77, Generator loss: 1.1866716146469116, Discriminator loss: 0.5931918621063232\n",
      "Epoch 78, Generator loss: 0.6296411156654358, Discriminator loss: 0.7298173904418945\n",
      "Epoch 79, Generator loss: 0.9027912020683289, Discriminator loss: 0.7261596918106079\n",
      "Epoch 80, Generator loss: 0.5464690923690796, Discriminator loss: 0.7445170879364014\n",
      "Epoch 81, Generator loss: 1.1578086614608765, Discriminator loss: 0.4541230797767639\n",
      "Epoch 82, Generator loss: 0.6683671474456787, Discriminator loss: 0.7001161575317383\n",
      "Epoch 83, Generator loss: 0.8730664253234863, Discriminator loss: 0.7495405673980713\n",
      "Epoch 84, Generator loss: 0.900120735168457, Discriminator loss: 0.6128579378128052\n",
      "Epoch 85, Generator loss: 0.6417341828346252, Discriminator loss: 0.7025147676467896\n",
      "Epoch 86, Generator loss: 0.9097445011138916, Discriminator loss: 0.5441145896911621\n",
      "Epoch 87, Generator loss: 0.6491414904594421, Discriminator loss: 0.7273409962654114\n",
      "Epoch 88, Generator loss: 0.5572395324707031, Discriminator loss: 0.7849711775779724\n",
      "Epoch 89, Generator loss: 1.2128710746765137, Discriminator loss: 0.6284528970718384\n",
      "Epoch 90, Generator loss: 0.5923337340354919, Discriminator loss: 0.7017441391944885\n",
      "Epoch 91, Generator loss: 0.5922402739524841, Discriminator loss: 0.7025145292282104\n",
      "Epoch 92, Generator loss: 0.8900988698005676, Discriminator loss: 0.6746415495872498\n",
      "Epoch 93, Generator loss: 0.8051267862319946, Discriminator loss: 0.7010350227355957\n",
      "Epoch 94, Generator loss: 0.5992425680160522, Discriminator loss: 0.6994514465332031\n",
      "Epoch 95, Generator loss: 0.8074758052825928, Discriminator loss: 0.6417989730834961\n",
      "Epoch 96, Generator loss: 0.810443103313446, Discriminator loss: 0.6536729335784912\n",
      "Epoch 97, Generator loss: 0.6616598963737488, Discriminator loss: 0.7173138856887817\n",
      "Epoch 98, Generator loss: 0.5588921308517456, Discriminator loss: 0.7491368651390076\n",
      "Epoch 99, Generator loss: 0.7014922499656677, Discriminator loss: 0.6566280126571655\n",
      "Epoch 100, Generator loss: 0.7417883276939392, Discriminator loss: 0.7262847423553467\n",
      "Epoch 101, Generator loss: 0.6947507262229919, Discriminator loss: 0.7474973797798157\n",
      "Epoch 102, Generator loss: 0.7957533001899719, Discriminator loss: 0.6719383001327515\n",
      "Epoch 103, Generator loss: 0.6765775680541992, Discriminator loss: 0.6919398307800293\n",
      "Epoch 104, Generator loss: 0.6039900779724121, Discriminator loss: 0.7665577530860901\n",
      "Epoch 105, Generator loss: 0.6717576384544373, Discriminator loss: 0.7357702255249023\n",
      "Epoch 106, Generator loss: 0.6742008924484253, Discriminator loss: 0.7169995307922363\n",
      "Epoch 107, Generator loss: 0.6581407189369202, Discriminator loss: 0.712809681892395\n",
      "Epoch 108, Generator loss: 0.6604704260826111, Discriminator loss: 0.7143926024436951\n",
      "Epoch 109, Generator loss: 0.6862233281135559, Discriminator loss: 0.7046400904655457\n",
      "Epoch 110, Generator loss: 0.6786409616470337, Discriminator loss: 0.7053189277648926\n",
      "Epoch 111, Generator loss: 0.6790010333061218, Discriminator loss: 0.703896164894104\n",
      "Epoch 112, Generator loss: 0.6837864518165588, Discriminator loss: 0.7014294266700745\n",
      "Epoch 113, Generator loss: 0.6829429864883423, Discriminator loss: 0.7019820213317871\n",
      "Epoch 114, Generator loss: 0.6779767274856567, Discriminator loss: 0.703155517578125\n",
      "Epoch 115, Generator loss: 0.6860223412513733, Discriminator loss: 0.6995205879211426\n",
      "Epoch 116, Generator loss: 0.6853137016296387, Discriminator loss: 0.7002276182174683\n",
      "Epoch 117, Generator loss: 0.6855634450912476, Discriminator loss: 0.6998027563095093\n",
      "Epoch 118, Generator loss: 0.6886707544326782, Discriminator loss: 0.6982079744338989\n",
      "Epoch 119, Generator loss: 0.6876794695854187, Discriminator loss: 0.6980849504470825\n",
      "Epoch 120, Generator loss: 0.6873708963394165, Discriminator loss: 0.698154628276825\n",
      "Epoch 121, Generator loss: 0.6916825771331787, Discriminator loss: 0.6961748600006104\n",
      "Epoch 122, Generator loss: 0.6881279945373535, Discriminator loss: 0.6975084543228149\n",
      "Epoch 123, Generator loss: 0.6886804103851318, Discriminator loss: 0.6972182989120483\n",
      "Epoch 124, Generator loss: 0.6891244649887085, Discriminator loss: 0.6970425844192505\n",
      "Epoch 125, Generator loss: 0.689178466796875, Discriminator loss: 0.6968832015991211\n",
      "Epoch 126, Generator loss: 0.6884454488754272, Discriminator loss: 0.6974610090255737\n",
      "Epoch 127, Generator loss: 0.6903560757637024, Discriminator loss: 0.6965075731277466\n",
      "Epoch 128, Generator loss: 0.6918700933456421, Discriminator loss: 0.695421576499939\n",
      "Epoch 129, Generator loss: 0.6896549463272095, Discriminator loss: 0.6963986158370972\n",
      "Epoch 130, Generator loss: 0.6897666454315186, Discriminator loss: 0.6962859630584717\n",
      "Epoch 131, Generator loss: 0.6897827982902527, Discriminator loss: 0.6962003707885742\n",
      "Epoch 132, Generator loss: 0.6909255385398865, Discriminator loss: 0.6957836747169495\n",
      "Epoch 133, Generator loss: 0.6899100542068481, Discriminator loss: 0.6960012912750244\n",
      "Epoch 134, Generator loss: 0.6902419924736023, Discriminator loss: 0.6958935856819153\n",
      "Epoch 135, Generator loss: 0.691452145576477, Discriminator loss: 0.6952106952667236\n",
      "Epoch 136, Generator loss: 0.6915385723114014, Discriminator loss: 0.6951481103897095\n",
      "Epoch 137, Generator loss: 0.6906387209892273, Discriminator loss: 0.69560706615448\n",
      "Epoch 138, Generator loss: 0.6906726956367493, Discriminator loss: 0.69553542137146\n",
      "Epoch 139, Generator loss: 0.6906792521476746, Discriminator loss: 0.6955037117004395\n",
      "Epoch 140, Generator loss: 0.6906935572624207, Discriminator loss: 0.6955510377883911\n",
      "Epoch 141, Generator loss: 0.6907660961151123, Discriminator loss: 0.695370078086853\n",
      "Epoch 142, Generator loss: 0.6925689578056335, Discriminator loss: 0.6946145296096802\n",
      "Epoch 143, Generator loss: 0.6887537240982056, Discriminator loss: 0.6962617635726929\n",
      "Epoch 144, Generator loss: 0.6917672157287598, Discriminator loss: 0.6949894428253174\n",
      "Epoch 145, Generator loss: 0.689164936542511, Discriminator loss: 0.6960960626602173\n",
      "Epoch 146, Generator loss: 0.692794144153595, Discriminator loss: 0.6944632530212402\n",
      "Epoch 147, Generator loss: 0.691343367099762, Discriminator loss: 0.6951216459274292\n",
      "Epoch 148, Generator loss: 0.6918431520462036, Discriminator loss: 0.694912314414978\n",
      "Epoch 149, Generator loss: 0.6926333904266357, Discriminator loss: 0.6943614482879639\n",
      "Epoch 150, Generator loss: 0.6919673681259155, Discriminator loss: 0.6947147250175476\n",
      "Epoch 151, Generator loss: 0.6914440393447876, Discriminator loss: 0.6949228644371033\n",
      "Epoch 152, Generator loss: 0.6927919983863831, Discriminator loss: 0.6942692995071411\n",
      "Epoch 153, Generator loss: 0.6928125023841858, Discriminator loss: 0.6942405700683594\n",
      "Epoch 154, Generator loss: 0.6915515065193176, Discriminator loss: 0.6949988603591919\n",
      "Epoch 155, Generator loss: 0.6916260123252869, Discriminator loss: 0.6946679353713989\n",
      "Epoch 156, Generator loss: 0.6915785670280457, Discriminator loss: 0.6946249008178711\n",
      "Epoch 157, Generator loss: 0.692298412322998, Discriminator loss: 0.6942748427391052\n",
      "Epoch 158, Generator loss: 0.6916611790657043, Discriminator loss: 0.6945533752441406\n",
      "Epoch 159, Generator loss: 0.6924991607666016, Discriminator loss: 0.6942177414894104\n",
      "Epoch 160, Generator loss: 0.6904737949371338, Discriminator loss: 0.6951406002044678\n",
      "Epoch 161, Generator loss: 0.6921833753585815, Discriminator loss: 0.6941637992858887\n",
      "Epoch 162, Generator loss: 0.6920367479324341, Discriminator loss: 0.6943871974945068\n",
      "Epoch 163, Generator loss: 0.6907714009284973, Discriminator loss: 0.6949813365936279\n",
      "Epoch 164, Generator loss: 0.6919184327125549, Discriminator loss: 0.6943392157554626\n",
      "Epoch 165, Generator loss: 0.6919649839401245, Discriminator loss: 0.6943061351776123\n",
      "Epoch 166, Generator loss: 0.6920987963676453, Discriminator loss: 0.6943381428718567\n",
      "Epoch 167, Generator loss: 0.692611813545227, Discriminator loss: 0.694012463092804\n",
      "Epoch 168, Generator loss: 0.6926403045654297, Discriminator loss: 0.6939889192581177\n",
      "Epoch 169, Generator loss: 0.6921276450157166, Discriminator loss: 0.6941908597946167\n",
      "Epoch 170, Generator loss: 0.6925742030143738, Discriminator loss: 0.6939433813095093\n",
      "Epoch 171, Generator loss: 0.6927955746650696, Discriminator loss: 0.6941050291061401\n",
      "Epoch 172, Generator loss: 0.6924303770065308, Discriminator loss: 0.6942881345748901\n",
      "Epoch 173, Generator loss: 0.6922035813331604, Discriminator loss: 0.6940780878067017\n",
      "Epoch 174, Generator loss: 0.6922590136528015, Discriminator loss: 0.6940406560897827\n",
      "Epoch 175, Generator loss: 0.6922849416732788, Discriminator loss: 0.6940129995346069\n",
      "Epoch 176, Generator loss: 0.6924899816513062, Discriminator loss: 0.6939789056777954\n",
      "Epoch 177, Generator loss: 0.6924862861633301, Discriminator loss: 0.6939530372619629\n",
      "Epoch 178, Generator loss: 0.6927383542060852, Discriminator loss: 0.6937741041183472\n",
      "Epoch 179, Generator loss: 0.6932084560394287, Discriminator loss: 0.6936744451522827\n",
      "Epoch 180, Generator loss: 0.6927051544189453, Discriminator loss: 0.6940391063690186\n",
      "Epoch 181, Generator loss: 0.6926345825195312, Discriminator loss: 0.6939027309417725\n",
      "Epoch 182, Generator loss: 0.6924762725830078, Discriminator loss: 0.6938307285308838\n",
      "Epoch 183, Generator loss: 0.6931260824203491, Discriminator loss: 0.6938043236732483\n",
      "Epoch 184, Generator loss: 0.6925566792488098, Discriminator loss: 0.6937949657440186\n",
      "Epoch 185, Generator loss: 0.6922886371612549, Discriminator loss: 0.693757176399231\n",
      "Epoch 186, Generator loss: 0.6931675672531128, Discriminator loss: 0.6935570240020752\n",
      "Epoch 187, Generator loss: 0.6933512091636658, Discriminator loss: 0.6935409307479858\n",
      "Epoch 188, Generator loss: 0.6932779550552368, Discriminator loss: 0.6936913728713989\n",
      "Epoch 189, Generator loss: 0.6926662921905518, Discriminator loss: 0.6936773061752319\n",
      "Epoch 190, Generator loss: 0.6929168701171875, Discriminator loss: 0.6936772465705872\n",
      "Epoch 191, Generator loss: 0.6930223703384399, Discriminator loss: 0.6935298442840576\n",
      "Epoch 192, Generator loss: 0.6933497786521912, Discriminator loss: 0.6934612989425659\n",
      "Epoch 193, Generator loss: 0.6930054426193237, Discriminator loss: 0.6936042308807373\n",
      "Epoch 194, Generator loss: 0.6933748126029968, Discriminator loss: 0.6934298276901245\n",
      "Epoch 195, Generator loss: 0.6931167840957642, Discriminator loss: 0.6934584379196167\n",
      "Epoch 196, Generator loss: 0.692976713180542, Discriminator loss: 0.693504810333252\n",
      "Epoch 197, Generator loss: 0.6931211948394775, Discriminator loss: 0.6935088038444519\n",
      "Epoch 198, Generator loss: 0.6929187774658203, Discriminator loss: 0.6934528350830078\n",
      "Epoch 199, Generator loss: 0.6927737593650818, Discriminator loss: 0.6936060786247253\n",
      "Epoch 200, Generator loss: 0.6935642957687378, Discriminator loss: 0.6934295892715454\n",
      "Epoch 201, Generator loss: 0.6931803226470947, Discriminator loss: 0.6934137940406799\n",
      "Epoch 202, Generator loss: 0.6930344104766846, Discriminator loss: 0.6933608055114746\n",
      "Epoch 203, Generator loss: 0.6929320096969604, Discriminator loss: 0.6934789419174194\n",
      "Epoch 204, Generator loss: 0.6931588649749756, Discriminator loss: 0.6933268308639526\n",
      "Epoch 205, Generator loss: 0.693228006362915, Discriminator loss: 0.6933040618896484\n",
      "Epoch 206, Generator loss: 0.6935035586357117, Discriminator loss: 0.6932855844497681\n",
      "Epoch 207, Generator loss: 0.6931928396224976, Discriminator loss: 0.6933364868164062\n",
      "Epoch 208, Generator loss: 0.6933719515800476, Discriminator loss: 0.6932449340820312\n",
      "Epoch 209, Generator loss: 0.6935955882072449, Discriminator loss: 0.6931961178779602\n",
      "Epoch 210, Generator loss: 0.6934354305267334, Discriminator loss: 0.6931968331336975\n",
      "Epoch 211, Generator loss: 0.6936362981796265, Discriminator loss: 0.6931636333465576\n",
      "Epoch 212, Generator loss: 0.6932874917984009, Discriminator loss: 0.6931259036064148\n",
      "Epoch 213, Generator loss: 0.693476140499115, Discriminator loss: 0.6931324005126953\n",
      "Epoch 214, Generator loss: 0.6925504803657532, Discriminator loss: 0.6930967569351196\n",
      "Epoch 215, Generator loss: 0.6936230659484863, Discriminator loss: 0.6930723190307617\n",
      "Epoch 216, Generator loss: 0.6938102841377258, Discriminator loss: 0.693081259727478\n",
      "Epoch 217, Generator loss: 0.6933783888816833, Discriminator loss: 0.6930292844772339\n",
      "Epoch 218, Generator loss: 0.6935545206069946, Discriminator loss: 0.6930301189422607\n",
      "Epoch 219, Generator loss: 0.6937006711959839, Discriminator loss: 0.6930079460144043\n",
      "Epoch 220, Generator loss: 0.6934818625450134, Discriminator loss: 0.6930125951766968\n",
      "Epoch 221, Generator loss: 0.6930680274963379, Discriminator loss: 0.6928987503051758\n",
      "Epoch 222, Generator loss: 0.6943333148956299, Discriminator loss: 0.6928801536560059\n",
      "Epoch 223, Generator loss: 0.6937699317932129, Discriminator loss: 0.6929412484169006\n",
      "Epoch 224, Generator loss: 0.6940335035324097, Discriminator loss: 0.6927142143249512\n",
      "Epoch 225, Generator loss: 0.6942885518074036, Discriminator loss: 0.6927846670150757\n",
      "Epoch 226, Generator loss: 0.6943991780281067, Discriminator loss: 0.6927508115768433\n",
      "Epoch 227, Generator loss: 0.6943183541297913, Discriminator loss: 0.6927980184555054\n",
      "Epoch 228, Generator loss: 0.6943439245223999, Discriminator loss: 0.6925440430641174\n",
      "Epoch 229, Generator loss: 0.6938574314117432, Discriminator loss: 0.6928392648696899\n",
      "Epoch 230, Generator loss: 0.6945633292198181, Discriminator loss: 0.6924530863761902\n",
      "Epoch 231, Generator loss: 0.6937676072120667, Discriminator loss: 0.6927578449249268\n",
      "Epoch 232, Generator loss: 0.6940876245498657, Discriminator loss: 0.6926736235618591\n",
      "Epoch 233, Generator loss: 0.6947412490844727, Discriminator loss: 0.692496657371521\n",
      "Epoch 234, Generator loss: 0.6942185759544373, Discriminator loss: 0.6926136016845703\n",
      "Epoch 235, Generator loss: 0.6948872804641724, Discriminator loss: 0.6922053694725037\n",
      "Epoch 236, Generator loss: 0.6941813826560974, Discriminator loss: 0.6924644112586975\n",
      "Epoch 237, Generator loss: 0.6949750185012817, Discriminator loss: 0.6923302412033081\n",
      "Epoch 238, Generator loss: 0.6951193809509277, Discriminator loss: 0.6920404434204102\n",
      "Epoch 239, Generator loss: 0.6943358182907104, Discriminator loss: 0.6923485994338989\n",
      "Epoch 240, Generator loss: 0.6941449046134949, Discriminator loss: 0.692572832107544\n",
      "Epoch 241, Generator loss: 0.6944571137428284, Discriminator loss: 0.692264199256897\n",
      "Epoch 242, Generator loss: 0.6944295167922974, Discriminator loss: 0.6922200322151184\n",
      "Epoch 243, Generator loss: 0.6941132545471191, Discriminator loss: 0.6922922134399414\n",
      "Epoch 244, Generator loss: 0.6947312951087952, Discriminator loss: 0.6921279430389404\n",
      "Epoch 245, Generator loss: 0.6948742270469666, Discriminator loss: 0.692206859588623\n",
      "Epoch 246, Generator loss: 0.694616436958313, Discriminator loss: 0.6919682025909424\n",
      "Epoch 247, Generator loss: 0.6961201429367065, Discriminator loss: 0.6914281249046326\n",
      "Epoch 248, Generator loss: 0.6950007081031799, Discriminator loss: 0.692063570022583\n",
      "Epoch 249, Generator loss: 0.69636470079422, Discriminator loss: 0.691260576248169\n",
      "Epoch 250, Generator loss: 0.6952078938484192, Discriminator loss: 0.691714346408844\n",
      "Epoch 251, Generator loss: 0.6951048374176025, Discriminator loss: 0.6917300224304199\n",
      "Epoch 252, Generator loss: 0.6953266859054565, Discriminator loss: 0.691840648651123\n",
      "Epoch 253, Generator loss: 0.6945364475250244, Discriminator loss: 0.691956102848053\n",
      "Epoch 254, Generator loss: 0.6953182816505432, Discriminator loss: 0.6915146112442017\n",
      "Epoch 255, Generator loss: 0.6953955292701721, Discriminator loss: 0.6913473606109619\n",
      "Epoch 256, Generator loss: 0.6959440112113953, Discriminator loss: 0.691234290599823\n",
      "Epoch 257, Generator loss: 0.6957078576087952, Discriminator loss: 0.691252589225769\n",
      "Epoch 258, Generator loss: 0.6949848532676697, Discriminator loss: 0.6916314959526062\n",
      "Epoch 259, Generator loss: 0.697196364402771, Discriminator loss: 0.6907384991645813\n",
      "Epoch 260, Generator loss: 0.6959656476974487, Discriminator loss: 0.6908347606658936\n",
      "Epoch 261, Generator loss: 0.6941343545913696, Discriminator loss: 0.6904734373092651\n",
      "Epoch 262, Generator loss: 0.6955097317695618, Discriminator loss: 0.6915098428726196\n",
      "Epoch 263, Generator loss: 0.6976414918899536, Discriminator loss: 0.6905554533004761\n",
      "Epoch 264, Generator loss: 0.6944792866706848, Discriminator loss: 0.6910758018493652\n",
      "Epoch 265, Generator loss: 0.6982954144477844, Discriminator loss: 0.6904332041740417\n",
      "Epoch 266, Generator loss: 0.6972643136978149, Discriminator loss: 0.6898534297943115\n",
      "Epoch 267, Generator loss: 0.6993740797042847, Discriminator loss: 0.6893510818481445\n",
      "Epoch 268, Generator loss: 0.6973803639411926, Discriminator loss: 0.6896255612373352\n",
      "Epoch 269, Generator loss: 0.6961256861686707, Discriminator loss: 0.6903388500213623\n",
      "Epoch 270, Generator loss: 0.6963238716125488, Discriminator loss: 0.6905255317687988\n",
      "Epoch 271, Generator loss: 0.6992066502571106, Discriminator loss: 0.6888014674186707\n",
      "Epoch 272, Generator loss: 0.6994316577911377, Discriminator loss: 0.6884571313858032\n",
      "Epoch 273, Generator loss: 0.697015643119812, Discriminator loss: 0.6899118423461914\n",
      "Epoch 274, Generator loss: 0.7006500363349915, Discriminator loss: 0.6883575320243835\n",
      "Epoch 275, Generator loss: 0.7045849561691284, Discriminator loss: 0.6861257553100586\n",
      "Epoch 276, Generator loss: 0.7031577229499817, Discriminator loss: 0.686972439289093\n",
      "Epoch 277, Generator loss: 0.6977980136871338, Discriminator loss: 0.6879556179046631\n",
      "Epoch 278, Generator loss: 0.7051038146018982, Discriminator loss: 0.6864200234413147\n",
      "Epoch 279, Generator loss: 0.7097924947738647, Discriminator loss: 0.6831154823303223\n",
      "Epoch 280, Generator loss: 0.701331377029419, Discriminator loss: 0.6830457448959351\n",
      "Epoch 281, Generator loss: 0.7055359482765198, Discriminator loss: 0.6875834465026855\n",
      "Epoch 282, Generator loss: 0.7108429074287415, Discriminator loss: 0.6881881952285767\n",
      "Epoch 283, Generator loss: 0.6684916019439697, Discriminator loss: 0.7033230662345886\n",
      "Epoch 284, Generator loss: 0.6593045592308044, Discriminator loss: 0.712922990322113\n",
      "Epoch 285, Generator loss: 0.6624224781990051, Discriminator loss: 0.7122262716293335\n",
      "Epoch 286, Generator loss: 0.6678470969200134, Discriminator loss: 0.7092950344085693\n",
      "Epoch 287, Generator loss: 0.6746177673339844, Discriminator loss: 0.7035374641418457\n",
      "Epoch 288, Generator loss: 0.681260347366333, Discriminator loss: 0.6992591619491577\n",
      "Epoch 289, Generator loss: 0.6837249398231506, Discriminator loss: 0.6983046531677246\n",
      "Epoch 290, Generator loss: 0.683527410030365, Discriminator loss: 0.6978566646575928\n",
      "Epoch 291, Generator loss: 0.6857876181602478, Discriminator loss: 0.6961991190910339\n",
      "Epoch 292, Generator loss: 0.6866673827171326, Discriminator loss: 0.6955195665359497\n",
      "Epoch 293, Generator loss: 0.6876603960990906, Discriminator loss: 0.694843053817749\n",
      "Epoch 294, Generator loss: 0.6886395812034607, Discriminator loss: 0.694000244140625\n",
      "Epoch 295, Generator loss: 0.6899380683898926, Discriminator loss: 0.6936212778091431\n",
      "Epoch 296, Generator loss: 0.6898850798606873, Discriminator loss: 0.6932209134101868\n",
      "Epoch 297, Generator loss: 0.6907550096511841, Discriminator loss: 0.6927337646484375\n",
      "Epoch 298, Generator loss: 0.6913148164749146, Discriminator loss: 0.6923604607582092\n",
      "Epoch 299, Generator loss: 0.6908018589019775, Discriminator loss: 0.6922186613082886\n",
      "Epoch 300, Generator loss: 0.6936365365982056, Discriminator loss: 0.6911343336105347\n",
      "Epoch 301, Generator loss: 0.6913917064666748, Discriminator loss: 0.6915667057037354\n",
      "Epoch 302, Generator loss: 0.6933145523071289, Discriminator loss: 0.6907204389572144\n",
      "Epoch 303, Generator loss: 0.6942200064659119, Discriminator loss: 0.6901569366455078\n",
      "Epoch 304, Generator loss: 0.6978898644447327, Discriminator loss: 0.6877814531326294\n",
      "Epoch 305, Generator loss: 0.6949591040611267, Discriminator loss: 0.6890852451324463\n",
      "Epoch 306, Generator loss: 0.7011200189590454, Discriminator loss: 0.6859943270683289\n",
      "Epoch 307, Generator loss: 0.6970097422599792, Discriminator loss: 0.6875109672546387\n",
      "Epoch 308, Generator loss: 0.7041148543357849, Discriminator loss: 0.6838133335113525\n",
      "Epoch 309, Generator loss: 0.7015884518623352, Discriminator loss: 0.6847504377365112\n",
      "Epoch 310, Generator loss: 0.7010173201560974, Discriminator loss: 0.6847472190856934\n",
      "Epoch 311, Generator loss: 0.7013728022575378, Discriminator loss: 0.6840336322784424\n",
      "Epoch 312, Generator loss: 0.6981170773506165, Discriminator loss: 0.684578537940979\n",
      "Epoch 313, Generator loss: 0.7064656615257263, Discriminator loss: 0.6814045906066895\n",
      "Epoch 314, Generator loss: 0.7059849500656128, Discriminator loss: 0.6808667182922363\n",
      "Epoch 315, Generator loss: 0.7130803465843201, Discriminator loss: 0.6827974319458008\n",
      "Epoch 316, Generator loss: 0.6858403086662292, Discriminator loss: 0.697291374206543\n",
      "Epoch 317, Generator loss: 0.6707712411880493, Discriminator loss: 0.7162251472473145\n",
      "Epoch 318, Generator loss: 0.6706122159957886, Discriminator loss: 0.7033542394638062\n",
      "Epoch 319, Generator loss: 0.7020407319068909, Discriminator loss: 0.7010364532470703\n",
      "Epoch 320, Generator loss: 0.677884578704834, Discriminator loss: 0.7024188041687012\n",
      "Epoch 321, Generator loss: 0.6968483328819275, Discriminator loss: 0.6968352794647217\n",
      "Epoch 322, Generator loss: 0.6963880062103271, Discriminator loss: 0.6941862106323242\n",
      "Epoch 323, Generator loss: 0.702485203742981, Discriminator loss: 0.6919947862625122\n",
      "Epoch 324, Generator loss: 0.7055742144584656, Discriminator loss: 0.6886319518089294\n",
      "Epoch 325, Generator loss: 0.7064752578735352, Discriminator loss: 0.687096118927002\n",
      "Epoch 326, Generator loss: 0.7127765417098999, Discriminator loss: 0.6833586692810059\n",
      "Epoch 327, Generator loss: 0.7334061861038208, Discriminator loss: 0.6749720573425293\n",
      "Epoch 328, Generator loss: 0.7465507388114929, Discriminator loss: 0.6627975106239319\n",
      "Epoch 329, Generator loss: 0.7426373362541199, Discriminator loss: 0.6777578592300415\n",
      "Epoch 330, Generator loss: 0.6182369589805603, Discriminator loss: 0.7586926817893982\n",
      "Epoch 331, Generator loss: 0.6554309129714966, Discriminator loss: 0.7319440245628357\n",
      "Epoch 332, Generator loss: 0.6656826734542847, Discriminator loss: 0.7102835178375244\n",
      "Epoch 333, Generator loss: 0.6751633286476135, Discriminator loss: 0.7054531574249268\n",
      "Epoch 334, Generator loss: 0.681244432926178, Discriminator loss: 0.7021060585975647\n",
      "Epoch 335, Generator loss: 0.6797030568122864, Discriminator loss: 0.7003467082977295\n",
      "Epoch 336, Generator loss: 0.6860990524291992, Discriminator loss: 0.6983438730239868\n",
      "Epoch 337, Generator loss: 0.6863613724708557, Discriminator loss: 0.6968860626220703\n",
      "Epoch 338, Generator loss: 0.6863287091255188, Discriminator loss: 0.6970803737640381\n",
      "Epoch 339, Generator loss: 0.6871466040611267, Discriminator loss: 0.6965805292129517\n",
      "Epoch 340, Generator loss: 0.6867627501487732, Discriminator loss: 0.6967437267303467\n",
      "Epoch 341, Generator loss: 0.6897864937782288, Discriminator loss: 0.6952071189880371\n",
      "Epoch 342, Generator loss: 0.688310444355011, Discriminator loss: 0.6955320835113525\n",
      "Epoch 343, Generator loss: 0.6893201470375061, Discriminator loss: 0.6952694058418274\n",
      "Epoch 344, Generator loss: 0.6890347003936768, Discriminator loss: 0.6953849792480469\n",
      "Epoch 345, Generator loss: 0.6892611384391785, Discriminator loss: 0.6948127746582031\n",
      "Epoch 346, Generator loss: 0.6903576254844666, Discriminator loss: 0.6946161389350891\n",
      "Epoch 347, Generator loss: 0.6897987127304077, Discriminator loss: 0.6944272518157959\n",
      "Epoch 348, Generator loss: 0.6903878450393677, Discriminator loss: 0.6943209171295166\n",
      "Epoch 349, Generator loss: 0.6907057762145996, Discriminator loss: 0.6939684152603149\n",
      "Epoch 350, Generator loss: 0.6907861828804016, Discriminator loss: 0.6939557194709778\n",
      "Epoch 351, Generator loss: 0.691408097743988, Discriminator loss: 0.6938014626502991\n",
      "Epoch 352, Generator loss: 0.6910507082939148, Discriminator loss: 0.6936641931533813\n",
      "Epoch 353, Generator loss: 0.6911895275115967, Discriminator loss: 0.6935120820999146\n",
      "Epoch 354, Generator loss: 0.6917263865470886, Discriminator loss: 0.6933532953262329\n",
      "Epoch 355, Generator loss: 0.6917114853858948, Discriminator loss: 0.6932666897773743\n",
      "Epoch 356, Generator loss: 0.6922126412391663, Discriminator loss: 0.6931290030479431\n",
      "Epoch 357, Generator loss: 0.6916936635971069, Discriminator loss: 0.693018913269043\n",
      "Epoch 358, Generator loss: 0.6924421787261963, Discriminator loss: 0.6929197907447815\n",
      "Epoch 359, Generator loss: 0.692101776599884, Discriminator loss: 0.6927825212478638\n",
      "Epoch 360, Generator loss: 0.69266676902771, Discriminator loss: 0.6926441192626953\n",
      "Epoch 361, Generator loss: 0.6922699213027954, Discriminator loss: 0.6925523281097412\n",
      "Epoch 362, Generator loss: 0.6925889849662781, Discriminator loss: 0.6926419138908386\n",
      "Epoch 363, Generator loss: 0.6924256086349487, Discriminator loss: 0.6923848390579224\n",
      "Epoch 364, Generator loss: 0.6937354207038879, Discriminator loss: 0.6920092701911926\n",
      "Epoch 365, Generator loss: 0.6934027671813965, Discriminator loss: 0.6921099424362183\n",
      "Epoch 366, Generator loss: 0.6928741335868835, Discriminator loss: 0.6919592618942261\n",
      "Epoch 367, Generator loss: 0.6938540935516357, Discriminator loss: 0.6917550563812256\n",
      "Epoch 368, Generator loss: 0.6927401423454285, Discriminator loss: 0.691961407661438\n",
      "Epoch 369, Generator loss: 0.6936694979667664, Discriminator loss: 0.6915616393089294\n",
      "Epoch 370, Generator loss: 0.6936834454536438, Discriminator loss: 0.6914166212081909\n",
      "Epoch 371, Generator loss: 0.6941462159156799, Discriminator loss: 0.6912130117416382\n",
      "Epoch 372, Generator loss: 0.6935926079750061, Discriminator loss: 0.6912223100662231\n",
      "Epoch 373, Generator loss: 0.6935955882072449, Discriminator loss: 0.6913068294525146\n",
      "Epoch 374, Generator loss: 0.6944882273674011, Discriminator loss: 0.6906759738922119\n",
      "Epoch 375, Generator loss: 0.6937789916992188, Discriminator loss: 0.690963625907898\n",
      "Epoch 376, Generator loss: 0.6953712701797485, Discriminator loss: 0.6903170347213745\n",
      "Epoch 377, Generator loss: 0.6923346519470215, Discriminator loss: 0.689872145652771\n",
      "Epoch 378, Generator loss: 0.6972243785858154, Discriminator loss: 0.6898319125175476\n",
      "Epoch 379, Generator loss: 0.6954828500747681, Discriminator loss: 0.6890907287597656\n",
      "Epoch 380, Generator loss: 0.6953375339508057, Discriminator loss: 0.689591109752655\n",
      "Epoch 381, Generator loss: 0.6940902471542358, Discriminator loss: 0.689570426940918\n",
      "Epoch 382, Generator loss: 0.696757435798645, Discriminator loss: 0.6882114410400391\n",
      "Epoch 383, Generator loss: 0.6990664601325989, Discriminator loss: 0.6870559453964233\n",
      "Epoch 384, Generator loss: 0.7024394869804382, Discriminator loss: 0.6852722764015198\n",
      "Epoch 385, Generator loss: 0.6996303200721741, Discriminator loss: 0.6858001947402954\n",
      "Epoch 386, Generator loss: 0.7010262608528137, Discriminator loss: 0.6847631335258484\n",
      "Epoch 387, Generator loss: 0.7172016501426697, Discriminator loss: 0.6782724261283875\n",
      "Epoch 388, Generator loss: 0.702250063419342, Discriminator loss: 0.68178391456604\n",
      "Epoch 389, Generator loss: 0.7057719826698303, Discriminator loss: 0.680942177772522\n",
      "Epoch 390, Generator loss: 0.6956400871276855, Discriminator loss: 0.682496190071106\n",
      "Epoch 391, Generator loss: 0.6911988258361816, Discriminator loss: 0.686435341835022\n",
      "Epoch 392, Generator loss: 0.6834020018577576, Discriminator loss: 0.7100661993026733\n",
      "Epoch 393, Generator loss: 0.655981183052063, Discriminator loss: 0.7211729288101196\n",
      "Epoch 394, Generator loss: 0.6474044919013977, Discriminator loss: 0.7254466414451599\n",
      "Epoch 395, Generator loss: 0.6582565307617188, Discriminator loss: 0.7149441242218018\n",
      "Epoch 396, Generator loss: 0.6810093522071838, Discriminator loss: 0.7047482132911682\n",
      "Epoch 397, Generator loss: 0.6807194352149963, Discriminator loss: 0.7017676830291748\n",
      "Epoch 398, Generator loss: 0.6872485876083374, Discriminator loss: 0.6989962458610535\n",
      "Epoch 399, Generator loss: 0.6821315288543701, Discriminator loss: 0.7001438140869141\n",
      "Epoch 400, Generator loss: 0.6887850761413574, Discriminator loss: 0.6969221830368042\n",
      "Epoch 401, Generator loss: 0.6920142769813538, Discriminator loss: 0.6952629685401917\n",
      "Epoch 402, Generator loss: 0.6896584033966064, Discriminator loss: 0.6961812376976013\n",
      "Epoch 403, Generator loss: 0.6915774941444397, Discriminator loss: 0.6950159072875977\n",
      "Epoch 404, Generator loss: 0.6914728879928589, Discriminator loss: 0.6948144435882568\n",
      "Epoch 405, Generator loss: 0.6917572021484375, Discriminator loss: 0.6944547891616821\n",
      "Epoch 406, Generator loss: 0.6926889419555664, Discriminator loss: 0.6941784024238586\n",
      "Epoch 407, Generator loss: 0.6928837895393372, Discriminator loss: 0.6936359405517578\n",
      "Epoch 408, Generator loss: 0.6935484409332275, Discriminator loss: 0.6935092210769653\n",
      "Epoch 409, Generator loss: 0.6940292119979858, Discriminator loss: 0.6931273937225342\n",
      "Epoch 410, Generator loss: 0.6938698887825012, Discriminator loss: 0.6928825974464417\n",
      "Epoch 411, Generator loss: 0.6949679851531982, Discriminator loss: 0.6926250457763672\n",
      "Epoch 412, Generator loss: 0.6946420669555664, Discriminator loss: 0.6924533843994141\n",
      "Epoch 413, Generator loss: 0.6949332356452942, Discriminator loss: 0.6920878887176514\n",
      "Epoch 414, Generator loss: 0.695092499256134, Discriminator loss: 0.692162275314331\n",
      "Epoch 415, Generator loss: 0.6958921551704407, Discriminator loss: 0.6914821863174438\n",
      "Epoch 416, Generator loss: 0.6967182755470276, Discriminator loss: 0.691152811050415\n",
      "Epoch 417, Generator loss: 0.696753203868866, Discriminator loss: 0.6912084817886353\n",
      "Epoch 418, Generator loss: 0.696837306022644, Discriminator loss: 0.6906797885894775\n",
      "Epoch 419, Generator loss: 0.6980615258216858, Discriminator loss: 0.690401554107666\n",
      "Epoch 420, Generator loss: 0.6967849135398865, Discriminator loss: 0.6909292340278625\n",
      "Epoch 421, Generator loss: 0.7006018757820129, Discriminator loss: 0.6892963647842407\n",
      "Epoch 422, Generator loss: 0.6975271105766296, Discriminator loss: 0.6904789805412292\n",
      "Epoch 423, Generator loss: 0.6986203193664551, Discriminator loss: 0.6898938417434692\n",
      "Epoch 424, Generator loss: 0.7013818621635437, Discriminator loss: 0.6890506744384766\n",
      "Epoch 425, Generator loss: 0.7014381885528564, Discriminator loss: 0.6886085271835327\n",
      "Epoch 426, Generator loss: 0.7045031785964966, Discriminator loss: 0.6878215074539185\n",
      "Epoch 427, Generator loss: 0.7071561813354492, Discriminator loss: 0.6862128973007202\n",
      "Epoch 428, Generator loss: 0.7022872567176819, Discriminator loss: 0.6889790296554565\n",
      "Epoch 429, Generator loss: 0.7012068629264832, Discriminator loss: 0.6899951696395874\n",
      "Epoch 430, Generator loss: 0.7001037001609802, Discriminator loss: 0.6922419667243958\n",
      "Epoch 431, Generator loss: 0.6958288550376892, Discriminator loss: 0.6948816776275635\n",
      "Epoch 432, Generator loss: 0.6918677091598511, Discriminator loss: 0.6985684633255005\n",
      "Epoch 433, Generator loss: 0.6854327917098999, Discriminator loss: 0.7026767730712891\n",
      "Epoch 434, Generator loss: 0.6839190125465393, Discriminator loss: 0.7040614485740662\n",
      "Epoch 435, Generator loss: 0.6875878572463989, Discriminator loss: 0.702014148235321\n",
      "Epoch 436, Generator loss: 0.6883512139320374, Discriminator loss: 0.7001997232437134\n",
      "Epoch 437, Generator loss: 0.6848444938659668, Discriminator loss: 0.7012978792190552\n",
      "Epoch 438, Generator loss: 0.6856719255447388, Discriminator loss: 0.6999579668045044\n",
      "Epoch 439, Generator loss: 0.6882526278495789, Discriminator loss: 0.6984834671020508\n",
      "Epoch 440, Generator loss: 0.6880931258201599, Discriminator loss: 0.6980407238006592\n",
      "Epoch 441, Generator loss: 0.6886022090911865, Discriminator loss: 0.6972109079360962\n",
      "Epoch 442, Generator loss: 0.6896551847457886, Discriminator loss: 0.696788489818573\n",
      "Epoch 443, Generator loss: 0.6898069381713867, Discriminator loss: 0.6960153579711914\n",
      "Epoch 444, Generator loss: 0.6913244724273682, Discriminator loss: 0.6954911351203918\n",
      "Epoch 445, Generator loss: 0.691787600517273, Discriminator loss: 0.6950538158416748\n",
      "Epoch 446, Generator loss: 0.6922239661216736, Discriminator loss: 0.6944336295127869\n",
      "Epoch 447, Generator loss: 0.6919047236442566, Discriminator loss: 0.6943168044090271\n",
      "Epoch 448, Generator loss: 0.6927360892295837, Discriminator loss: 0.6940094232559204\n",
      "Epoch 449, Generator loss: 0.6929150819778442, Discriminator loss: 0.6936556696891785\n",
      "Epoch 450, Generator loss: 0.6933582425117493, Discriminator loss: 0.6932984590530396\n",
      "Epoch 451, Generator loss: 0.6935139298439026, Discriminator loss: 0.6930296421051025\n",
      "Epoch 452, Generator loss: 0.6938648819923401, Discriminator loss: 0.6927407383918762\n",
      "Epoch 453, Generator loss: 0.6941758990287781, Discriminator loss: 0.6924574375152588\n",
      "Epoch 454, Generator loss: 0.6952780485153198, Discriminator loss: 0.6918870210647583\n",
      "Epoch 455, Generator loss: 0.694831371307373, Discriminator loss: 0.6917857527732849\n",
      "Epoch 456, Generator loss: 0.6972873210906982, Discriminator loss: 0.6905805468559265\n",
      "Epoch 457, Generator loss: 0.6958430409431458, Discriminator loss: 0.6911401748657227\n",
      "Epoch 458, Generator loss: 0.6947933435440063, Discriminator loss: 0.6914756298065186\n",
      "Epoch 459, Generator loss: 0.695325493812561, Discriminator loss: 0.6909528970718384\n",
      "Epoch 460, Generator loss: 0.7003185153007507, Discriminator loss: 0.6884747743606567\n",
      "Epoch 461, Generator loss: 0.6976145505905151, Discriminator loss: 0.6896977424621582\n",
      "Epoch 462, Generator loss: 0.6984791159629822, Discriminator loss: 0.6890855431556702\n",
      "Epoch 463, Generator loss: 0.6992631554603577, Discriminator loss: 0.6886303424835205\n",
      "Epoch 464, Generator loss: 0.7009366750717163, Discriminator loss: 0.6875928044319153\n",
      "Epoch 465, Generator loss: 0.6961635947227478, Discriminator loss: 0.6893930435180664\n",
      "Epoch 466, Generator loss: 0.6976631283760071, Discriminator loss: 0.6884860396385193\n",
      "Epoch 467, Generator loss: 0.6975147724151611, Discriminator loss: 0.6881293058395386\n",
      "Epoch 468, Generator loss: 0.7014308571815491, Discriminator loss: 0.6867053508758545\n",
      "Epoch 469, Generator loss: 0.7037796974182129, Discriminator loss: 0.6852490901947021\n",
      "Epoch 470, Generator loss: 0.7006186246871948, Discriminator loss: 0.6866084933280945\n",
      "Epoch 471, Generator loss: 0.7009477615356445, Discriminator loss: 0.6866734027862549\n",
      "Epoch 472, Generator loss: 0.6973692178726196, Discriminator loss: 0.6883084774017334\n",
      "Epoch 473, Generator loss: 0.6968730092048645, Discriminator loss: 0.6884089708328247\n",
      "Epoch 474, Generator loss: 0.6865593791007996, Discriminator loss: 0.6932010054588318\n",
      "Epoch 475, Generator loss: 0.6786003112792969, Discriminator loss: 0.6986656785011292\n",
      "Epoch 476, Generator loss: 0.6788644194602966, Discriminator loss: 0.6993718147277832\n",
      "Epoch 477, Generator loss: 0.6705413460731506, Discriminator loss: 0.7046365141868591\n",
      "Epoch 478, Generator loss: 0.6696053743362427, Discriminator loss: 0.706247091293335\n",
      "Epoch 479, Generator loss: 0.6750302314758301, Discriminator loss: 0.7030223608016968\n",
      "Epoch 480, Generator loss: 0.6777437329292297, Discriminator loss: 0.70174241065979\n",
      "Epoch 481, Generator loss: 0.6812346577644348, Discriminator loss: 0.6996828317642212\n",
      "Epoch 482, Generator loss: 0.6831669807434082, Discriminator loss: 0.6986038684844971\n",
      "Epoch 483, Generator loss: 0.6844730973243713, Discriminator loss: 0.6980630159378052\n",
      "Epoch 484, Generator loss: 0.685027539730072, Discriminator loss: 0.6973772644996643\n",
      "Epoch 485, Generator loss: 0.6868347525596619, Discriminator loss: 0.6964199542999268\n",
      "Epoch 486, Generator loss: 0.6876957416534424, Discriminator loss: 0.6960927248001099\n",
      "Epoch 487, Generator loss: 0.6873885989189148, Discriminator loss: 0.6961065530776978\n",
      "Epoch 488, Generator loss: 0.6895288228988647, Discriminator loss: 0.6948248147964478\n",
      "Epoch 489, Generator loss: 0.6876059770584106, Discriminator loss: 0.6956813931465149\n",
      "Epoch 490, Generator loss: 0.6897507309913635, Discriminator loss: 0.6946700811386108\n",
      "Epoch 491, Generator loss: 0.6903761625289917, Discriminator loss: 0.6939919590950012\n",
      "Epoch 492, Generator loss: 0.6911101937294006, Discriminator loss: 0.693778395652771\n",
      "Epoch 493, Generator loss: 0.6912736296653748, Discriminator loss: 0.6934211254119873\n",
      "Epoch 494, Generator loss: 0.6917381882667542, Discriminator loss: 0.6930977702140808\n",
      "Epoch 495, Generator loss: 0.6921780109405518, Discriminator loss: 0.6928038597106934\n",
      "Epoch 496, Generator loss: 0.6935391426086426, Discriminator loss: 0.6921682953834534\n",
      "Epoch 497, Generator loss: 0.6927186250686646, Discriminator loss: 0.6922205090522766\n",
      "Epoch 498, Generator loss: 0.6942039728164673, Discriminator loss: 0.6917290687561035\n",
      "Epoch 499, Generator loss: 0.6934545636177063, Discriminator loss: 0.6917413473129272\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    for _, (signal_tensor, params_tensor) in enumerate(train_loader):\n",
    "        #z = torch.randn(1, num_latent_variables, 1).to(device)\n",
    "        g_loss = gan.train_generator(signal_tensor, z)\n",
    "        d_loss = gan.train_discriminator(signal_tensor, params_tensor, z)\n",
    "        g_loss_list.append(g_loss)\n",
    "        d_loss_list.append(d_loss)\n",
    "    print(f\"Epoch {epoch}, Generator loss: {g_loss}, Discriminator loss: {d_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = gan.generator\n",
    "generator.eval()\n",
    "\n",
    "TS = Signal_Generator(num_sources=1, noise_amplitude=1)\n",
    "test_data = TS.generating_signal()\n",
    "params = TS.printing_parameters()\n",
    "\n",
    "input_signal = test_data['Signal'].values\n",
    "input_signal_tensor = torch.tensor(input_signal, dtype=torch.float).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_params = generator(input_signal_tensor, z).squeeze().cpu().numpy()\n",
    "\n",
    "print(params)\n",
    "print(generated_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN():\n",
    "    def __init__(self, dataset, num_latent_variables, lr, weight_clip):\n",
    "        self.dataset = dataset\n",
    "        self.num_latent_variables = num_latent_variables\n",
    "        self.lr = lr\n",
    "        self.weight_clip = weight_clip\n",
    "\n",
    "        # Networks\n",
    "        self.generator = Generator(in_channels=1, num_latent_variables=num_latent_variables, length=len(signal), num_parameters=len(params)).to(device)\n",
    "        self.discriminator = Discriminator(input_channels=1, length=len(signal), num_parameters=len(params)).to(device)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_g = optim.Adam(self.generator.parameters(), lr=self.lr)\n",
    "        self.optimizer_d = optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "\n",
    "    def wasserstein_loss(self, output_d, y):\n",
    "        return torch.mean(output_d * y)\n",
    "    \n",
    "    def train_generator(self, signal_tensor, z):\n",
    "        generated_params = self.generator(signal_tensor, z)\n",
    "        fake_output = self.discriminator(signal_tensor, generated_params)\n",
    "        g_loss = -torch.mean(fake_output)\n",
    "\n",
    "        self.optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_g.step()\n",
    "\n",
    "        return g_loss.item()\n",
    "    \n",
    "    def train_discriminator(self, signal_tensor, params_tensor, z):\n",
    "        fake_params = self.generator(signal_tensor, z).detach()\n",
    "        real_output = self.discriminator(signal_tensor, params_tensor)\n",
    "        fake_output = self.discriminator(signal_tensor, fake_params)\n",
    "\n",
    "        d_loss = -(torch.mean(real_output) - torch.mean(fake_output))\n",
    "\n",
    "        self.optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_d.step()\n",
    "\n",
    "        # Weight clipping\n",
    "        for p in self.discriminator.parameters():\n",
    "            p.data.clamp_(-self.weight_clip, self.weight_clip)\n",
    "\n",
    "\n",
    "        return d_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Generator loss: -0.5003039240837097, Discriminator loss: -0.002644360065460205\n",
      "Epoch 2/500, Generator loss: -0.5002155303955078, Discriminator loss: -0.0032842159271240234\n",
      "Epoch 3/500, Generator loss: -0.5036647319793701, Discriminator loss: 0.0011756420135498047\n",
      "Epoch 4/500, Generator loss: -0.5031977891921997, Discriminator loss: 0.0004311203956604004\n",
      "Epoch 5/500, Generator loss: -0.5038115382194519, Discriminator loss: -0.0013865232467651367\n",
      "Epoch 6/500, Generator loss: -0.5044497847557068, Discriminator loss: -0.00036263465881347656\n",
      "Epoch 7/500, Generator loss: -0.5046750903129578, Discriminator loss: -0.00028449296951293945\n",
      "Epoch 8/500, Generator loss: -0.5039489269256592, Discriminator loss: -0.0009512901306152344\n",
      "Epoch 9/500, Generator loss: -0.5034221410751343, Discriminator loss: -0.0016151666641235352\n",
      "Epoch 10/500, Generator loss: -0.503147542476654, Discriminator loss: -0.002013683319091797\n",
      "Epoch 11/500, Generator loss: -0.5039631128311157, Discriminator loss: -0.0012792348861694336\n",
      "Epoch 12/500, Generator loss: -0.5045477151870728, Discriminator loss: -0.0012413263320922852\n",
      "Epoch 13/500, Generator loss: -0.5019620656967163, Discriminator loss: -0.002822279930114746\n",
      "Epoch 14/500, Generator loss: -0.5029987096786499, Discriminator loss: -0.004094421863555908\n",
      "Epoch 15/500, Generator loss: -0.5010237693786621, Discriminator loss: -0.0010069012641906738\n",
      "Epoch 16/500, Generator loss: -0.5043057799339294, Discriminator loss: -0.0011658668518066406\n",
      "Epoch 17/500, Generator loss: -0.503946840763092, Discriminator loss: -0.0010889768600463867\n",
      "Epoch 18/500, Generator loss: -0.5048885345458984, Discriminator loss: -0.0003039240837097168\n",
      "Epoch 19/500, Generator loss: -0.5033766627311707, Discriminator loss: -0.0012623071670532227\n",
      "Epoch 20/500, Generator loss: -0.501977264881134, Discriminator loss: -0.0029616951942443848\n",
      "Epoch 21/500, Generator loss: -0.5008289813995361, Discriminator loss: -0.0042372941970825195\n",
      "Epoch 22/500, Generator loss: -0.500525176525116, Discriminator loss: -0.0037055015563964844\n",
      "Epoch 23/500, Generator loss: -0.5017099380493164, Discriminator loss: -0.0035053491592407227\n",
      "Epoch 24/500, Generator loss: -0.5042332410812378, Discriminator loss: -0.00038510560989379883\n",
      "Epoch 25/500, Generator loss: -0.5030454397201538, Discriminator loss: -0.00010383129119873047\n",
      "Epoch 26/500, Generator loss: -0.5021480321884155, Discriminator loss: -0.0023779869079589844\n",
      "Epoch 27/500, Generator loss: -0.5018312335014343, Discriminator loss: -0.003060638904571533\n",
      "Epoch 28/500, Generator loss: -0.5008358359336853, Discriminator loss: -0.002983570098876953\n",
      "Epoch 29/500, Generator loss: -0.5029385089874268, Discriminator loss: -0.002064526081085205\n",
      "Epoch 30/500, Generator loss: -0.5025976300239563, Discriminator loss: -0.0020183324813842773\n",
      "Epoch 31/500, Generator loss: -0.5012896656990051, Discriminator loss: -0.0016176104545593262\n",
      "Epoch 32/500, Generator loss: -0.500853419303894, Discriminator loss: -0.003879725933074951\n",
      "Epoch 33/500, Generator loss: -0.5008253455162048, Discriminator loss: -0.004119873046875\n",
      "Epoch 34/500, Generator loss: -0.5028460025787354, Discriminator loss: -0.0005649924278259277\n",
      "Epoch 35/500, Generator loss: -0.501560389995575, Discriminator loss: -0.002550184726715088\n",
      "Epoch 36/500, Generator loss: -0.5029445886611938, Discriminator loss: -0.003769218921661377\n",
      "Epoch 37/500, Generator loss: -0.5003155469894409, Discriminator loss: -0.0003165602684020996\n",
      "Epoch 38/500, Generator loss: -0.5005829334259033, Discriminator loss: -0.004169762134552002\n",
      "Epoch 39/500, Generator loss: -0.5021498203277588, Discriminator loss: -0.002137482166290283\n",
      "Epoch 40/500, Generator loss: -0.5044292211532593, Discriminator loss: -0.0003427267074584961\n",
      "Epoch 41/500, Generator loss: -0.5041136145591736, Discriminator loss: -0.003356337547302246\n",
      "Epoch 42/500, Generator loss: -0.5004748702049255, Discriminator loss: -0.0024768710136413574\n",
      "Epoch 43/500, Generator loss: -0.5007921457290649, Discriminator loss: -0.004003345966339111\n",
      "Epoch 44/500, Generator loss: -0.5001179575920105, Discriminator loss: 0.0002726316452026367\n",
      "Epoch 45/500, Generator loss: -0.5020240545272827, Discriminator loss: 6.0439109802246094e-05\n",
      "Epoch 46/500, Generator loss: -0.5040084719657898, Discriminator loss: -0.0027371644973754883\n",
      "Epoch 47/500, Generator loss: -0.5034682154655457, Discriminator loss: -0.0028955936431884766\n",
      "Epoch 48/500, Generator loss: -0.5023027062416077, Discriminator loss: -0.0020188093185424805\n",
      "Epoch 49/500, Generator loss: -0.5001966953277588, Discriminator loss: -0.0021072030067443848\n",
      "Epoch 50/500, Generator loss: -0.5016327500343323, Discriminator loss: -0.004048824310302734\n",
      "Epoch 51/500, Generator loss: -0.5030245184898376, Discriminator loss: -0.0020220279693603516\n",
      "Epoch 52/500, Generator loss: -0.5042400360107422, Discriminator loss: -0.00023359060287475586\n",
      "Epoch 53/500, Generator loss: -0.5040990710258484, Discriminator loss: -0.00018513202667236328\n",
      "Epoch 54/500, Generator loss: -0.5040696263313293, Discriminator loss: 0.00014644861221313477\n",
      "Epoch 55/500, Generator loss: -0.5038812756538391, Discriminator loss: 0.0012460947036743164\n",
      "Epoch 56/500, Generator loss: -0.5034120082855225, Discriminator loss: -0.00022470951080322266\n",
      "Epoch 57/500, Generator loss: -0.5032283663749695, Discriminator loss: -0.0008310675621032715\n",
      "Epoch 58/500, Generator loss: -0.5021203756332397, Discriminator loss: -0.0013653039932250977\n",
      "Epoch 59/500, Generator loss: -0.5012108683586121, Discriminator loss: -0.0023279786109924316\n",
      "Epoch 60/500, Generator loss: -0.5012736916542053, Discriminator loss: -0.0021971464157104492\n",
      "Epoch 61/500, Generator loss: -0.500113308429718, Discriminator loss: -0.0026552677154541016\n",
      "Epoch 62/500, Generator loss: -0.5033495426177979, Discriminator loss: -0.003047764301300049\n",
      "Epoch 63/500, Generator loss: -0.5028582215309143, Discriminator loss: -0.00011986494064331055\n",
      "Epoch 64/500, Generator loss: -0.5023552179336548, Discriminator loss: -0.0012617707252502441\n",
      "Epoch 65/500, Generator loss: -0.5008941888809204, Discriminator loss: -0.001692354679107666\n",
      "Epoch 66/500, Generator loss: -0.5033345818519592, Discriminator loss: -0.001312255859375\n",
      "Epoch 67/500, Generator loss: -0.5030072331428528, Discriminator loss: 9.560585021972656e-05\n",
      "Epoch 68/500, Generator loss: -0.5028347969055176, Discriminator loss: -0.00043314695358276367\n",
      "Epoch 69/500, Generator loss: -0.5025264620780945, Discriminator loss: 0.00033551454544067383\n",
      "Epoch 70/500, Generator loss: -0.5026159286499023, Discriminator loss: -0.0009496212005615234\n",
      "Epoch 71/500, Generator loss: -0.5032727718353271, Discriminator loss: -0.0002484917640686035\n",
      "Epoch 72/500, Generator loss: -0.501870334148407, Discriminator loss: 6.568431854248047e-05\n",
      "Epoch 73/500, Generator loss: -0.5024569034576416, Discriminator loss: 0.0003025531768798828\n",
      "Epoch 74/500, Generator loss: -0.5029970407485962, Discriminator loss: -0.00061798095703125\n",
      "Epoch 75/500, Generator loss: -0.5030775666236877, Discriminator loss: -6.973743438720703e-06\n",
      "Epoch 76/500, Generator loss: -0.5027216672897339, Discriminator loss: -0.0003345608711242676\n",
      "Epoch 77/500, Generator loss: -0.5025906562805176, Discriminator loss: -0.0007814764976501465\n",
      "Epoch 78/500, Generator loss: -0.502640962600708, Discriminator loss: -0.0009898543357849121\n",
      "Epoch 79/500, Generator loss: -0.5017903447151184, Discriminator loss: -0.0016454458236694336\n",
      "Epoch 80/500, Generator loss: -0.501846432685852, Discriminator loss: -9.077787399291992e-05\n",
      "Epoch 81/500, Generator loss: -0.5018877983093262, Discriminator loss: -0.0003383159637451172\n",
      "Epoch 82/500, Generator loss: -0.5006523728370667, Discriminator loss: -0.0014233589172363281\n",
      "Epoch 83/500, Generator loss: -0.5017575025558472, Discriminator loss: 4.0531158447265625e-05\n",
      "Epoch 84/500, Generator loss: -0.5016202926635742, Discriminator loss: 0.0004898905754089355\n",
      "Epoch 85/500, Generator loss: -0.5030372142791748, Discriminator loss: -0.0001233220100402832\n",
      "Epoch 86/500, Generator loss: -0.5027165412902832, Discriminator loss: -0.0003539919853210449\n",
      "Epoch 87/500, Generator loss: -0.5025704503059387, Discriminator loss: -0.001027822494506836\n",
      "Epoch 88/500, Generator loss: -0.5014562010765076, Discriminator loss: -0.000699460506439209\n",
      "Epoch 89/500, Generator loss: -0.5008352994918823, Discriminator loss: 0.001038968563079834\n",
      "Epoch 90/500, Generator loss: -0.49994540214538574, Discriminator loss: -0.0001366734504699707\n",
      "Epoch 91/500, Generator loss: -0.49983370304107666, Discriminator loss: -0.00034290552139282227\n",
      "Epoch 92/500, Generator loss: -0.4995003640651703, Discriminator loss: -0.0004407167434692383\n",
      "Epoch 93/500, Generator loss: -0.5004295110702515, Discriminator loss: 8.559226989746094e-05\n",
      "Epoch 94/500, Generator loss: -0.5006864070892334, Discriminator loss: -0.00029289722442626953\n",
      "Epoch 95/500, Generator loss: -0.5006672143936157, Discriminator loss: 2.288818359375e-05\n",
      "Epoch 96/500, Generator loss: -0.5008295178413391, Discriminator loss: -3.5643577575683594e-05\n",
      "Epoch 97/500, Generator loss: -0.5005709528923035, Discriminator loss: -0.00020772218704223633\n",
      "Epoch 98/500, Generator loss: -0.5015337467193604, Discriminator loss: 7.11679458618164e-05\n",
      "Epoch 99/500, Generator loss: -0.5013397932052612, Discriminator loss: 0.0011569857597351074\n",
      "Epoch 100/500, Generator loss: -0.5001627206802368, Discriminator loss: -1.0967254638671875e-05\n",
      "Epoch 101/500, Generator loss: -0.4999377727508545, Discriminator loss: -0.0002200603485107422\n",
      "Epoch 102/500, Generator loss: -0.4996972680091858, Discriminator loss: -0.0002516508102416992\n",
      "Epoch 103/500, Generator loss: -0.499073326587677, Discriminator loss: -0.0006525516510009766\n",
      "Epoch 104/500, Generator loss: -0.49941709637641907, Discriminator loss: 2.8014183044433594e-05\n",
      "Epoch 105/500, Generator loss: -0.5013439655303955, Discriminator loss: 0.0005254149436950684\n",
      "Epoch 106/500, Generator loss: -0.5022574663162231, Discriminator loss: -0.0009611248970031738\n",
      "Epoch 107/500, Generator loss: -0.5016911029815674, Discriminator loss: -0.00023931264877319336\n",
      "Epoch 108/500, Generator loss: -0.5023758411407471, Discriminator loss: -0.0007495284080505371\n",
      "Epoch 109/500, Generator loss: -0.5013072490692139, Discriminator loss: -8.279085159301758e-05\n",
      "Epoch 110/500, Generator loss: -0.5006878972053528, Discriminator loss: -0.002028226852416992\n",
      "Epoch 111/500, Generator loss: -0.5024904012680054, Discriminator loss: 0.0011677742004394531\n",
      "Epoch 112/500, Generator loss: -0.5023037791252136, Discriminator loss: 0.00013488531112670898\n",
      "Epoch 113/500, Generator loss: -0.5016965270042419, Discriminator loss: 0.00014770030975341797\n",
      "Epoch 114/500, Generator loss: -0.5016420483589172, Discriminator loss: -0.00014579296112060547\n",
      "Epoch 115/500, Generator loss: -0.5015969276428223, Discriminator loss: -0.00044465065002441406\n",
      "Epoch 116/500, Generator loss: -0.5016636252403259, Discriminator loss: -0.000870048999786377\n",
      "Epoch 117/500, Generator loss: -0.5009187459945679, Discriminator loss: -0.0009362697601318359\n",
      "Epoch 118/500, Generator loss: -0.501026451587677, Discriminator loss: -0.001445770263671875\n",
      "Epoch 119/500, Generator loss: -0.501557469367981, Discriminator loss: 1.3530254364013672e-05\n",
      "Epoch 120/500, Generator loss: -0.5011729598045349, Discriminator loss: 5.3048133850097656e-05\n",
      "Epoch 121/500, Generator loss: -0.5009734034538269, Discriminator loss: -0.00045490264892578125\n",
      "Epoch 122/500, Generator loss: -0.501287579536438, Discriminator loss: -0.0006545782089233398\n",
      "Epoch 123/500, Generator loss: -0.5017347931861877, Discriminator loss: 0.0001360177993774414\n",
      "Epoch 124/500, Generator loss: -0.5017627477645874, Discriminator loss: 0.00023490190505981445\n",
      "Epoch 125/500, Generator loss: -0.5017788410186768, Discriminator loss: -0.000630795955657959\n",
      "Epoch 126/500, Generator loss: -0.5002099275588989, Discriminator loss: 3.445148468017578e-05\n",
      "Epoch 127/500, Generator loss: -0.5018059015274048, Discriminator loss: -0.0002530217170715332\n",
      "Epoch 128/500, Generator loss: -0.5019069314002991, Discriminator loss: -1.0848045349121094e-05\n",
      "Epoch 129/500, Generator loss: -0.5016610026359558, Discriminator loss: 0.00040227174758911133\n",
      "Epoch 130/500, Generator loss: -0.5014395117759705, Discriminator loss: -0.0007706880569458008\n",
      "Epoch 131/500, Generator loss: -0.5013298988342285, Discriminator loss: -0.0004379749298095703\n",
      "Epoch 132/500, Generator loss: -0.5017956495285034, Discriminator loss: -8.511543273925781e-05\n",
      "Epoch 133/500, Generator loss: -0.5008288025856018, Discriminator loss: 0.0005456507205963135\n",
      "Epoch 134/500, Generator loss: -0.5010433793067932, Discriminator loss: -0.00040608644485473633\n",
      "Epoch 135/500, Generator loss: -0.5011037588119507, Discriminator loss: -5.644559860229492e-05\n",
      "Epoch 136/500, Generator loss: -0.49940264225006104, Discriminator loss: -0.0009803175926208496\n",
      "Epoch 137/500, Generator loss: -0.5000755190849304, Discriminator loss: 0.00044268369674682617\n",
      "Epoch 138/500, Generator loss: -0.5005022287368774, Discriminator loss: 3.5762786865234375e-06\n",
      "Epoch 139/500, Generator loss: -0.500458836555481, Discriminator loss: -0.00022280216217041016\n",
      "Epoch 140/500, Generator loss: -0.5014780759811401, Discriminator loss: 0.0003428459167480469\n",
      "Epoch 141/500, Generator loss: -0.5016724467277527, Discriminator loss: -0.00040012598037719727\n",
      "Epoch 142/500, Generator loss: -0.5014145970344543, Discriminator loss: -0.00020104646682739258\n",
      "Epoch 143/500, Generator loss: -0.5008265972137451, Discriminator loss: -0.00020372867584228516\n",
      "Epoch 144/500, Generator loss: -0.5010282397270203, Discriminator loss: -0.00023096799850463867\n",
      "Epoch 145/500, Generator loss: -0.5012727379798889, Discriminator loss: 6.598234176635742e-05\n",
      "Epoch 146/500, Generator loss: -0.5013488531112671, Discriminator loss: -1.0251998901367188e-05\n",
      "Epoch 147/500, Generator loss: -0.501340389251709, Discriminator loss: 0.00019246339797973633\n",
      "Epoch 148/500, Generator loss: -0.5014899969100952, Discriminator loss: -1.3053417205810547e-05\n",
      "Epoch 149/500, Generator loss: -0.5011184215545654, Discriminator loss: -7.2479248046875e-05\n",
      "Epoch 150/500, Generator loss: -0.5008871555328369, Discriminator loss: 0.00012546777725219727\n",
      "Epoch 151/500, Generator loss: -0.5010822415351868, Discriminator loss: -0.0003285408020019531\n",
      "Epoch 152/500, Generator loss: -0.5011864900588989, Discriminator loss: -6.16312026977539e-05\n",
      "Epoch 153/500, Generator loss: -0.5011520981788635, Discriminator loss: -4.684925079345703e-05\n",
      "Epoch 154/500, Generator loss: -0.5010187029838562, Discriminator loss: -0.00012868642807006836\n",
      "Epoch 155/500, Generator loss: -0.5011782050132751, Discriminator loss: 3.808736801147461e-05\n",
      "Epoch 156/500, Generator loss: -0.5017934441566467, Discriminator loss: -0.00045168399810791016\n",
      "Epoch 157/500, Generator loss: -0.5013420581817627, Discriminator loss: -3.844499588012695e-05\n",
      "Epoch 158/500, Generator loss: -0.501247227191925, Discriminator loss: -9.965896606445312e-05\n",
      "Epoch 159/500, Generator loss: -0.5010160207748413, Discriminator loss: -0.0005958676338195801\n",
      "Epoch 160/500, Generator loss: -0.5013766288757324, Discriminator loss: -0.000392913818359375\n",
      "Epoch 161/500, Generator loss: -0.49995505809783936, Discriminator loss: -0.00011837482452392578\n",
      "Epoch 162/500, Generator loss: -0.5000255107879639, Discriminator loss: 0.00029218196868896484\n",
      "Epoch 163/500, Generator loss: -0.5005606412887573, Discriminator loss: 6.669759750366211e-05\n",
      "Epoch 164/500, Generator loss: -0.5008249878883362, Discriminator loss: -1.055002212524414e-05\n",
      "Epoch 165/500, Generator loss: -0.5008050799369812, Discriminator loss: -0.00016897916793823242\n",
      "Epoch 166/500, Generator loss: -0.4999616742134094, Discriminator loss: 3.847479820251465e-05\n",
      "Epoch 167/500, Generator loss: -0.5004156231880188, Discriminator loss: -0.0003962516784667969\n",
      "Epoch 168/500, Generator loss: -0.49987515807151794, Discriminator loss: -0.0006620287895202637\n",
      "Epoch 169/500, Generator loss: -0.5011218786239624, Discriminator loss: 0.000186920166015625\n",
      "Epoch 170/500, Generator loss: -0.4995557963848114, Discriminator loss: -0.0001786351203918457\n",
      "Epoch 171/500, Generator loss: -0.500691294670105, Discriminator loss: 0.0008260011672973633\n",
      "Epoch 172/500, Generator loss: -0.5007584691047668, Discriminator loss: 0.00035125017166137695\n",
      "Epoch 173/500, Generator loss: -0.49966031312942505, Discriminator loss: -0.0008074641227722168\n",
      "Epoch 174/500, Generator loss: -0.5005528330802917, Discriminator loss: -0.0008817911148071289\n",
      "Epoch 175/500, Generator loss: -0.49900901317596436, Discriminator loss: -0.00012001395225524902\n",
      "Epoch 176/500, Generator loss: -0.4990619719028473, Discriminator loss: -3.835558891296387e-05\n",
      "Epoch 177/500, Generator loss: -0.4991397261619568, Discriminator loss: 2.816319465637207e-05\n",
      "Epoch 178/500, Generator loss: -0.4990738034248352, Discriminator loss: 1.6748905181884766e-05\n",
      "Epoch 179/500, Generator loss: -0.49903935194015503, Discriminator loss: 9.864568710327148e-06\n",
      "Epoch 180/500, Generator loss: -0.4989624619483948, Discriminator loss: -7.796287536621094e-05\n",
      "Epoch 181/500, Generator loss: -0.49854275584220886, Discriminator loss: -0.0006050765514373779\n",
      "Epoch 182/500, Generator loss: -0.49960288405418396, Discriminator loss: -0.0010166764259338379\n",
      "Epoch 183/500, Generator loss: -0.49928018450737, Discriminator loss: -5.602836608886719e-06\n",
      "Epoch 184/500, Generator loss: -0.4999898672103882, Discriminator loss: 0.0002308487892150879\n",
      "Epoch 185/500, Generator loss: -0.5014957785606384, Discriminator loss: -0.0009753704071044922\n",
      "Epoch 186/500, Generator loss: -0.5000147223472595, Discriminator loss: -0.0016453266143798828\n",
      "Epoch 187/500, Generator loss: -0.5014613270759583, Discriminator loss: 0.0011608600616455078\n",
      "Epoch 188/500, Generator loss: -0.4992292523384094, Discriminator loss: -0.0012935101985931396\n",
      "Epoch 189/500, Generator loss: -0.49904897809028625, Discriminator loss: -0.0016572773456573486\n",
      "Epoch 190/500, Generator loss: -0.5003635287284851, Discriminator loss: 0.00013554096221923828\n",
      "Epoch 191/500, Generator loss: -0.5006211400032043, Discriminator loss: -0.0005648136138916016\n",
      "Epoch 192/500, Generator loss: -0.5011977553367615, Discriminator loss: -0.0006684660911560059\n",
      "Epoch 193/500, Generator loss: -0.5012414455413818, Discriminator loss: 0.0014492273330688477\n",
      "Epoch 194/500, Generator loss: -0.5013256072998047, Discriminator loss: -0.00024890899658203125\n",
      "Epoch 195/500, Generator loss: -0.4996272623538971, Discriminator loss: -0.0012078583240509033\n",
      "Epoch 196/500, Generator loss: -0.5003712177276611, Discriminator loss: -0.0019057393074035645\n",
      "Epoch 197/500, Generator loss: -0.501230776309967, Discriminator loss: 0.00013208389282226562\n",
      "Epoch 198/500, Generator loss: -0.5003729462623596, Discriminator loss: -0.00028246641159057617\n",
      "Epoch 199/500, Generator loss: -0.5011511445045471, Discriminator loss: -0.0017869770526885986\n",
      "Epoch 200/500, Generator loss: -0.5006292462348938, Discriminator loss: 8.273124694824219e-05\n",
      "Epoch 201/500, Generator loss: -0.501365065574646, Discriminator loss: -0.0011338591575622559\n",
      "Epoch 202/500, Generator loss: -0.5007820725440979, Discriminator loss: 4.5299530029296875e-05\n",
      "Epoch 203/500, Generator loss: -0.5003242492675781, Discriminator loss: -0.0001214742660522461\n",
      "Epoch 204/500, Generator loss: -0.4987148344516754, Discriminator loss: -0.0021141767501831055\n",
      "Epoch 205/500, Generator loss: -0.5005829930305481, Discriminator loss: 0.00030934810638427734\n",
      "Epoch 206/500, Generator loss: -0.4979877769947052, Discriminator loss: -8.231401443481445e-05\n",
      "Epoch 207/500, Generator loss: -0.4983724355697632, Discriminator loss: -0.0006710588932037354\n",
      "Epoch 208/500, Generator loss: -0.4992079734802246, Discriminator loss: -0.0002136826515197754\n",
      "Epoch 209/500, Generator loss: -0.49900397658348083, Discriminator loss: -0.0003643631935119629\n",
      "Epoch 210/500, Generator loss: -0.4984368085861206, Discriminator loss: 0.00036978721618652344\n",
      "Epoch 211/500, Generator loss: -0.5014702677726746, Discriminator loss: -0.00030416250228881836\n",
      "Epoch 212/500, Generator loss: -0.5012133717536926, Discriminator loss: 9.310245513916016e-05\n",
      "Epoch 213/500, Generator loss: -0.5008306503295898, Discriminator loss: -0.0012184977531433105\n",
      "Epoch 214/500, Generator loss: -0.4999936819076538, Discriminator loss: -0.0011147260665893555\n",
      "Epoch 215/500, Generator loss: -0.5013318061828613, Discriminator loss: 0.0004768967628479004\n",
      "Epoch 216/500, Generator loss: -0.5012857913970947, Discriminator loss: -0.00041413307189941406\n",
      "Epoch 217/500, Generator loss: -0.5012682676315308, Discriminator loss: -7.098913192749023e-05\n",
      "Epoch 218/500, Generator loss: -0.5010218620300293, Discriminator loss: -4.0590763092041016e-05\n",
      "Epoch 219/500, Generator loss: -0.5005613565444946, Discriminator loss: -0.0011592209339141846\n",
      "Epoch 220/500, Generator loss: -0.49768510460853577, Discriminator loss: 0.0008094608783721924\n",
      "Epoch 221/500, Generator loss: -0.4990691542625427, Discriminator loss: 0.0005221068859100342\n",
      "Epoch 222/500, Generator loss: -0.5001856088638306, Discriminator loss: 5.036592483520508e-05\n",
      "Epoch 223/500, Generator loss: -0.5000218152999878, Discriminator loss: -0.0002918243408203125\n",
      "Epoch 224/500, Generator loss: -0.4995790719985962, Discriminator loss: -0.0002313852310180664\n",
      "Epoch 225/500, Generator loss: -0.5010497570037842, Discriminator loss: -0.00021398067474365234\n",
      "Epoch 226/500, Generator loss: -0.5008180141448975, Discriminator loss: -0.0001831650733947754\n",
      "Epoch 227/500, Generator loss: -0.5008379817008972, Discriminator loss: -0.0005686283111572266\n",
      "Epoch 228/500, Generator loss: -0.5011389255523682, Discriminator loss: -3.236532211303711e-05\n",
      "Epoch 229/500, Generator loss: -0.500694215297699, Discriminator loss: -0.0006124377250671387\n",
      "Epoch 230/500, Generator loss: -0.5005981922149658, Discriminator loss: -0.0011929571628570557\n",
      "Epoch 231/500, Generator loss: -0.500009298324585, Discriminator loss: 0.0005033612251281738\n",
      "Epoch 232/500, Generator loss: -0.5001208186149597, Discriminator loss: 0.00016099214553833008\n",
      "Epoch 233/500, Generator loss: -0.5004986524581909, Discriminator loss: -0.0011471807956695557\n",
      "Epoch 234/500, Generator loss: -0.5007435083389282, Discriminator loss: -8.231401443481445e-05\n",
      "Epoch 235/500, Generator loss: -0.5006480813026428, Discriminator loss: -0.00026422739028930664\n",
      "Epoch 236/500, Generator loss: -0.499752402305603, Discriminator loss: -0.0010136663913726807\n",
      "Epoch 237/500, Generator loss: -0.4993375837802887, Discriminator loss: 4.57763671875e-05\n",
      "Epoch 238/500, Generator loss: -0.49950939416885376, Discriminator loss: -0.00016260147094726562\n",
      "Epoch 239/500, Generator loss: -0.4999134838581085, Discriminator loss: -0.0007474422454833984\n",
      "Epoch 240/500, Generator loss: -0.49935734272003174, Discriminator loss: 0.0010806620121002197\n",
      "Epoch 241/500, Generator loss: -0.49849557876586914, Discriminator loss: -0.0003117024898529053\n",
      "Epoch 242/500, Generator loss: -0.4989587366580963, Discriminator loss: -0.0003393590450286865\n",
      "Epoch 243/500, Generator loss: -0.4995957314968109, Discriminator loss: -0.00015121698379516602\n",
      "Epoch 244/500, Generator loss: -0.4998074769973755, Discriminator loss: 0.0002319812774658203\n",
      "Epoch 245/500, Generator loss: -0.4990527033805847, Discriminator loss: -0.0016011595726013184\n",
      "Epoch 246/500, Generator loss: -0.5004997253417969, Discriminator loss: -0.00036078691482543945\n",
      "Epoch 247/500, Generator loss: -0.5000814199447632, Discriminator loss: -0.000594794750213623\n",
      "Epoch 248/500, Generator loss: -0.4998856484889984, Discriminator loss: -0.0005921125411987305\n",
      "Epoch 249/500, Generator loss: -0.4990376830101013, Discriminator loss: -0.0008034110069274902\n",
      "Epoch 250/500, Generator loss: -0.49888846278190613, Discriminator loss: -0.002005636692047119\n",
      "Epoch 251/500, Generator loss: -0.49922093749046326, Discriminator loss: 0.00017130374908447266\n",
      "Epoch 252/500, Generator loss: -0.4982602596282959, Discriminator loss: 0.000658571720123291\n",
      "Epoch 253/500, Generator loss: -0.49955716729164124, Discriminator loss: -0.00017815828323364258\n",
      "Epoch 254/500, Generator loss: -0.49984103441238403, Discriminator loss: -0.0005556643009185791\n",
      "Epoch 255/500, Generator loss: -0.4989430010318756, Discriminator loss: -0.0003896355628967285\n",
      "Epoch 256/500, Generator loss: -0.5010771751403809, Discriminator loss: 9.673833847045898e-05\n",
      "Epoch 257/500, Generator loss: -0.5010684132575989, Discriminator loss: 0.0002586245536804199\n",
      "Epoch 258/500, Generator loss: -0.49925318360328674, Discriminator loss: 0.0006379485130310059\n",
      "Epoch 259/500, Generator loss: -0.49891397356987, Discriminator loss: -0.0004881620407104492\n",
      "Epoch 260/500, Generator loss: -0.49942976236343384, Discriminator loss: 0.00023633241653442383\n",
      "Epoch 261/500, Generator loss: -0.49912551045417786, Discriminator loss: -0.00010725855827331543\n",
      "Epoch 262/500, Generator loss: -0.4983193278312683, Discriminator loss: -0.0001735985279083252\n",
      "Epoch 263/500, Generator loss: -0.498793363571167, Discriminator loss: 0.00046366453170776367\n",
      "Epoch 264/500, Generator loss: -0.4985421597957611, Discriminator loss: 0.0001322329044342041\n",
      "Epoch 265/500, Generator loss: -0.49827203154563904, Discriminator loss: -7.414817810058594e-05\n",
      "Epoch 266/500, Generator loss: -0.49815183877944946, Discriminator loss: 2.0891427993774414e-05\n",
      "Epoch 267/500, Generator loss: -0.49822497367858887, Discriminator loss: -5.4210424423217773e-05\n",
      "Epoch 268/500, Generator loss: -0.49817076325416565, Discriminator loss: 4.6819448471069336e-05\n",
      "Epoch 269/500, Generator loss: -0.4983142912387848, Discriminator loss: -0.00025835633277893066\n",
      "Epoch 270/500, Generator loss: -0.4991670548915863, Discriminator loss: -0.0004107952117919922\n",
      "Epoch 271/500, Generator loss: -0.499186635017395, Discriminator loss: 0.0004938244819641113\n",
      "Epoch 272/500, Generator loss: -0.4991048574447632, Discriminator loss: -1.5676021575927734e-05\n",
      "Epoch 273/500, Generator loss: -0.4992034137248993, Discriminator loss: 7.152557373046875e-07\n",
      "Epoch 274/500, Generator loss: -0.4993783235549927, Discriminator loss: -0.00023922324180603027\n",
      "Epoch 275/500, Generator loss: -0.49890175461769104, Discriminator loss: -4.011392593383789e-05\n",
      "Epoch 276/500, Generator loss: -0.49916085600852966, Discriminator loss: 0.00023731589317321777\n",
      "Epoch 277/500, Generator loss: -0.4991950988769531, Discriminator loss: -0.0002097785472869873\n",
      "Epoch 278/500, Generator loss: -0.49865737557411194, Discriminator loss: 3.3676624298095703e-06\n",
      "Epoch 279/500, Generator loss: -0.49974149465560913, Discriminator loss: 0.0007106363773345947\n",
      "Epoch 280/500, Generator loss: -0.49932676553726196, Discriminator loss: -0.000699162483215332\n",
      "Epoch 281/500, Generator loss: -0.49980729818344116, Discriminator loss: -0.0002536177635192871\n",
      "Epoch 282/500, Generator loss: -0.5003077387809753, Discriminator loss: -0.0006507635116577148\n",
      "Epoch 283/500, Generator loss: -0.5005306005477905, Discriminator loss: 0.00014901161193847656\n",
      "Epoch 284/500, Generator loss: -0.5001871585845947, Discriminator loss: 0.00016313791275024414\n",
      "Epoch 285/500, Generator loss: -0.5012133121490479, Discriminator loss: -5.644559860229492e-05\n",
      "Epoch 286/500, Generator loss: -0.5012803673744202, Discriminator loss: -0.0003439784049987793\n",
      "Epoch 287/500, Generator loss: -0.5014188885688782, Discriminator loss: -0.00042492151260375977\n",
      "Epoch 288/500, Generator loss: -0.500971794128418, Discriminator loss: -0.0005419254302978516\n",
      "Epoch 289/500, Generator loss: -0.5001550316810608, Discriminator loss: 0.0006195008754730225\n",
      "Epoch 290/500, Generator loss: -0.5007268786430359, Discriminator loss: -0.0001399517059326172\n",
      "Epoch 291/500, Generator loss: -0.5001875162124634, Discriminator loss: -0.0018585026264190674\n",
      "Epoch 292/500, Generator loss: -0.4998724162578583, Discriminator loss: 0.0005021989345550537\n",
      "Epoch 293/500, Generator loss: -0.4988122880458832, Discriminator loss: -0.0007020235061645508\n",
      "Epoch 294/500, Generator loss: -0.49990418553352356, Discriminator loss: 0.00016829371452331543\n",
      "Epoch 295/500, Generator loss: -0.4997245967388153, Discriminator loss: -0.00010180473327636719\n",
      "Epoch 296/500, Generator loss: -0.49970048666000366, Discriminator loss: -0.0004940927028656006\n",
      "Epoch 297/500, Generator loss: -0.49932047724723816, Discriminator loss: 0.00038230419158935547\n",
      "Epoch 298/500, Generator loss: -0.5011182427406311, Discriminator loss: 0.0005416274070739746\n",
      "Epoch 299/500, Generator loss: -0.4995052218437195, Discriminator loss: 1.6301870346069336e-05\n",
      "Epoch 300/500, Generator loss: -0.4995568096637726, Discriminator loss: 0.00019097328186035156\n",
      "Epoch 301/500, Generator loss: -0.5003756284713745, Discriminator loss: -0.00025659799575805664\n",
      "Epoch 302/500, Generator loss: -0.5007773041725159, Discriminator loss: -0.0001602768898010254\n",
      "Epoch 303/500, Generator loss: -0.5009147524833679, Discriminator loss: 0.00031501054763793945\n",
      "Epoch 304/500, Generator loss: -0.5008814930915833, Discriminator loss: 6.693601608276367e-05\n",
      "Epoch 305/500, Generator loss: -0.5004969239234924, Discriminator loss: 6.61015510559082e-05\n",
      "Epoch 306/500, Generator loss: -0.4998950660228729, Discriminator loss: -0.0007773339748382568\n",
      "Epoch 307/500, Generator loss: -0.5009931325912476, Discriminator loss: 0.0002636909484863281\n",
      "Epoch 308/500, Generator loss: -0.5008527040481567, Discriminator loss: 9.304285049438477e-05\n",
      "Epoch 309/500, Generator loss: -0.49800363183021545, Discriminator loss: -0.0002103745937347412\n",
      "Epoch 310/500, Generator loss: -0.5001213550567627, Discriminator loss: -0.0010110437870025635\n",
      "Epoch 311/500, Generator loss: -0.5000739097595215, Discriminator loss: 0.00046756863594055176\n",
      "Epoch 312/500, Generator loss: -0.5010660886764526, Discriminator loss: 1.5616416931152344e-05\n",
      "Epoch 313/500, Generator loss: -0.5009612441062927, Discriminator loss: -3.5762786865234375e-06\n",
      "Epoch 314/500, Generator loss: -0.4990583658218384, Discriminator loss: -0.001758664846420288\n",
      "Epoch 315/500, Generator loss: -0.49915629625320435, Discriminator loss: -0.0015311837196350098\n",
      "Epoch 316/500, Generator loss: -0.5011614561080933, Discriminator loss: -3.0338764190673828e-05\n",
      "Epoch 317/500, Generator loss: -0.5006251335144043, Discriminator loss: -0.0001055002212524414\n",
      "Epoch 318/500, Generator loss: -0.5001949071884155, Discriminator loss: -0.001035153865814209\n",
      "Epoch 319/500, Generator loss: -0.501171350479126, Discriminator loss: -0.0003237128257751465\n",
      "Epoch 320/500, Generator loss: -0.5008813142776489, Discriminator loss: 8.690357208251953e-05\n",
      "Epoch 321/500, Generator loss: -0.49947589635849, Discriminator loss: 0.0004730820655822754\n",
      "Epoch 322/500, Generator loss: -0.5007684826850891, Discriminator loss: -0.0002924799919128418\n",
      "Epoch 323/500, Generator loss: -0.49956122040748596, Discriminator loss: -0.0010856389999389648\n",
      "Epoch 324/500, Generator loss: -0.5013584494590759, Discriminator loss: -5.459785461425781e-05\n",
      "Epoch 325/500, Generator loss: -0.49976271390914917, Discriminator loss: -0.00041726231575012207\n",
      "Epoch 326/500, Generator loss: -0.5005884170532227, Discriminator loss: -4.5239925384521484e-05\n",
      "Epoch 327/500, Generator loss: -0.4993427097797394, Discriminator loss: -0.00011748075485229492\n",
      "Epoch 328/500, Generator loss: -0.4984757900238037, Discriminator loss: -0.00019407272338867188\n",
      "Epoch 329/500, Generator loss: -0.4983244836330414, Discriminator loss: -0.00011369585990905762\n",
      "Epoch 330/500, Generator loss: -0.4992840886116028, Discriminator loss: -0.0002561211585998535\n",
      "Epoch 331/500, Generator loss: -0.49892762303352356, Discriminator loss: 0.00033357739448547363\n",
      "Epoch 332/500, Generator loss: -0.4995371699333191, Discriminator loss: 0.0008329153060913086\n",
      "Epoch 333/500, Generator loss: -0.5001357197761536, Discriminator loss: 0.00028699636459350586\n",
      "Epoch 334/500, Generator loss: -0.5002694129943848, Discriminator loss: -0.00020420551300048828\n",
      "Epoch 335/500, Generator loss: -0.49951744079589844, Discriminator loss: -5.728006362915039e-05\n",
      "Epoch 336/500, Generator loss: -0.5004770159721375, Discriminator loss: -0.0003133416175842285\n",
      "Epoch 337/500, Generator loss: -0.500688910484314, Discriminator loss: -0.0013549327850341797\n",
      "Epoch 338/500, Generator loss: -0.5009434819221497, Discriminator loss: -0.000988781452178955\n",
      "Epoch 339/500, Generator loss: -0.49985751509666443, Discriminator loss: 0.00022038817405700684\n",
      "Epoch 340/500, Generator loss: -0.4988881051540375, Discriminator loss: -0.001310974359512329\n",
      "Epoch 341/500, Generator loss: -0.4998507797718048, Discriminator loss: -0.0011178553104400635\n",
      "Epoch 342/500, Generator loss: -0.49966999888420105, Discriminator loss: -6.35385513305664e-05\n",
      "Epoch 343/500, Generator loss: -0.49988552927970886, Discriminator loss: -0.00027042627334594727\n",
      "Epoch 344/500, Generator loss: -0.5011039972305298, Discriminator loss: -0.002599775791168213\n",
      "Epoch 345/500, Generator loss: -0.5012058615684509, Discriminator loss: -0.0015446841716766357\n",
      "Epoch 346/500, Generator loss: -0.500094473361969, Discriminator loss: -0.000382840633392334\n",
      "Epoch 347/500, Generator loss: -0.4986461102962494, Discriminator loss: -9.816884994506836e-05\n",
      "Epoch 348/500, Generator loss: -0.5005092620849609, Discriminator loss: -0.00027316808700561523\n",
      "Epoch 349/500, Generator loss: -0.5006841421127319, Discriminator loss: -0.0017990469932556152\n",
      "Epoch 350/500, Generator loss: -0.4986993670463562, Discriminator loss: -0.0021657943725585938\n",
      "Epoch 351/500, Generator loss: -0.5018852949142456, Discriminator loss: 2.872943878173828e-05\n",
      "Epoch 352/500, Generator loss: -0.5014635324478149, Discriminator loss: -6.0498714447021484e-05\n",
      "Epoch 353/500, Generator loss: -0.498999685049057, Discriminator loss: -0.0017608702182769775\n",
      "Epoch 354/500, Generator loss: -0.5002521872520447, Discriminator loss: -5.066394805908203e-05\n",
      "Epoch 355/500, Generator loss: -0.49997079372406006, Discriminator loss: -0.00023025274276733398\n",
      "Epoch 356/500, Generator loss: -0.4997106194496155, Discriminator loss: 0.0001061558723449707\n",
      "Epoch 357/500, Generator loss: -0.49805647134780884, Discriminator loss: 0.0004614889621734619\n",
      "Epoch 358/500, Generator loss: -0.4993217885494232, Discriminator loss: -0.0011777281761169434\n",
      "Epoch 359/500, Generator loss: -0.500779926776886, Discriminator loss: -0.0016998052597045898\n",
      "Epoch 360/500, Generator loss: -0.5011702179908752, Discriminator loss: -0.0012645125389099121\n",
      "Epoch 361/500, Generator loss: -0.5000752806663513, Discriminator loss: 0.00018298625946044922\n",
      "Epoch 362/500, Generator loss: -0.5010324120521545, Discriminator loss: -0.0017857849597930908\n",
      "Epoch 363/500, Generator loss: -0.49941322207450867, Discriminator loss: -0.0013548731803894043\n",
      "Epoch 364/500, Generator loss: -0.49926328659057617, Discriminator loss: 0.0001469254493713379\n",
      "Epoch 365/500, Generator loss: -0.49971529841423035, Discriminator loss: -0.0005145370960235596\n",
      "Epoch 366/500, Generator loss: -0.4999980926513672, Discriminator loss: -0.0015850067138671875\n",
      "Epoch 367/500, Generator loss: -0.5009289383888245, Discriminator loss: -0.0004096031188964844\n",
      "Epoch 368/500, Generator loss: -0.5016676783561707, Discriminator loss: -3.427267074584961e-05\n",
      "Epoch 369/500, Generator loss: -0.5006453990936279, Discriminator loss: -0.0006436705589294434\n",
      "Epoch 370/500, Generator loss: -0.49930810928344727, Discriminator loss: -0.0012309849262237549\n",
      "Epoch 371/500, Generator loss: -0.500656008720398, Discriminator loss: -0.00079306960105896\n",
      "Epoch 372/500, Generator loss: -0.49815136194229126, Discriminator loss: -0.0010136961936950684\n",
      "Epoch 373/500, Generator loss: -0.5009632110595703, Discriminator loss: -0.00021767616271972656\n",
      "Epoch 374/500, Generator loss: -0.49764567613601685, Discriminator loss: 0.0005014240741729736\n",
      "Epoch 375/500, Generator loss: -0.5003849864006042, Discriminator loss: -0.0014896690845489502\n",
      "Epoch 376/500, Generator loss: -0.5008869767189026, Discriminator loss: -0.0027222037315368652\n",
      "Epoch 377/500, Generator loss: -0.501575767993927, Discriminator loss: 0.00012880563735961914\n",
      "Epoch 378/500, Generator loss: -0.5011478662490845, Discriminator loss: -0.00045877695083618164\n",
      "Epoch 379/500, Generator loss: -0.500367283821106, Discriminator loss: -0.001336425542831421\n",
      "Epoch 380/500, Generator loss: -0.49862170219421387, Discriminator loss: -0.0021639466285705566\n",
      "Epoch 381/500, Generator loss: -0.5011110305786133, Discriminator loss: -0.0015718936920166016\n",
      "Epoch 382/500, Generator loss: -0.49795112013816833, Discriminator loss: -0.00035113096237182617\n",
      "Epoch 383/500, Generator loss: -0.4995584785938263, Discriminator loss: 0.0002868473529815674\n",
      "Epoch 384/500, Generator loss: -0.5012956857681274, Discriminator loss: -1.8835067749023438e-05\n",
      "Epoch 385/500, Generator loss: -0.4992715120315552, Discriminator loss: 0.00020432472229003906\n",
      "Epoch 386/500, Generator loss: -0.5008677840232849, Discriminator loss: -3.343820571899414e-05\n",
      "Epoch 387/500, Generator loss: -0.500957190990448, Discriminator loss: -3.153085708618164e-05\n",
      "Epoch 388/500, Generator loss: -0.5011388659477234, Discriminator loss: -0.0002626776695251465\n",
      "Epoch 389/500, Generator loss: -0.5001497268676758, Discriminator loss: -0.0009513199329376221\n",
      "Epoch 390/500, Generator loss: -0.4999314546585083, Discriminator loss: -0.0004838705062866211\n",
      "Epoch 391/500, Generator loss: -0.49972429871559143, Discriminator loss: 0.00020551681518554688\n",
      "Epoch 392/500, Generator loss: -0.5003976821899414, Discriminator loss: 8.7738037109375e-05\n",
      "Epoch 393/500, Generator loss: -0.4978030025959015, Discriminator loss: -0.00027698278427124023\n",
      "Epoch 394/500, Generator loss: -0.5001172423362732, Discriminator loss: -0.0005721151828765869\n",
      "Epoch 395/500, Generator loss: -0.49995553493499756, Discriminator loss: -3.546476364135742e-05\n",
      "Epoch 396/500, Generator loss: -0.5011128783226013, Discriminator loss: -0.000278472900390625\n",
      "Epoch 397/500, Generator loss: -0.5015605092048645, Discriminator loss: -0.00032407045364379883\n",
      "Epoch 398/500, Generator loss: -0.5001493096351624, Discriminator loss: -6.961822509765625e-05\n",
      "Epoch 399/500, Generator loss: -0.49958691000938416, Discriminator loss: -0.0009480118751525879\n",
      "Epoch 400/500, Generator loss: -0.5003571510314941, Discriminator loss: 7.063150405883789e-05\n",
      "Epoch 401/500, Generator loss: -0.4993824362754822, Discriminator loss: -1.481175422668457e-05\n",
      "Epoch 402/500, Generator loss: -0.4996204972267151, Discriminator loss: 7.385015487670898e-05\n",
      "Epoch 403/500, Generator loss: -0.49966520071029663, Discriminator loss: -1.2665987014770508e-05\n",
      "Epoch 404/500, Generator loss: -0.49975597858428955, Discriminator loss: 7.420778274536133e-06\n",
      "Epoch 405/500, Generator loss: -0.4997584819793701, Discriminator loss: 1.2755393981933594e-05\n",
      "Epoch 406/500, Generator loss: -0.4998033940792084, Discriminator loss: 1.055002212524414e-05\n",
      "Epoch 407/500, Generator loss: -0.49977004528045654, Discriminator loss: -1.1801719665527344e-05\n",
      "Epoch 408/500, Generator loss: -0.49978628754615784, Discriminator loss: 4.5299530029296875e-06\n",
      "Epoch 409/500, Generator loss: -0.49980077147483826, Discriminator loss: -2.086162567138672e-06\n",
      "Epoch 410/500, Generator loss: -0.49982762336730957, Discriminator loss: -6.258487701416016e-06\n",
      "Epoch 411/500, Generator loss: -0.4998202919960022, Discriminator loss: 5.781650543212891e-06\n",
      "Epoch 412/500, Generator loss: -0.4998322129249573, Discriminator loss: -5.066394805908203e-06\n",
      "Epoch 413/500, Generator loss: -0.4998195767402649, Discriminator loss: 3.5762786865234375e-06\n",
      "Epoch 414/500, Generator loss: -0.4998062551021576, Discriminator loss: -6.735324859619141e-06\n",
      "Epoch 415/500, Generator loss: -0.49988219141960144, Discriminator loss: -1.0251998901367188e-05\n",
      "Epoch 416/500, Generator loss: -0.4999274015426636, Discriminator loss: -1.3768672943115234e-05\n",
      "Epoch 417/500, Generator loss: -0.49993497133255005, Discriminator loss: -3.5762786865234375e-06\n",
      "Epoch 418/500, Generator loss: -0.4999741315841675, Discriminator loss: -6.9141387939453125e-06\n",
      "Epoch 419/500, Generator loss: -0.49999499320983887, Discriminator loss: 2.0802021026611328e-05\n",
      "Epoch 420/500, Generator loss: -0.5000823736190796, Discriminator loss: -0.00014156103134155273\n",
      "Epoch 421/500, Generator loss: -0.49922633171081543, Discriminator loss: -0.0005809366703033447\n",
      "Epoch 422/500, Generator loss: -0.5007530450820923, Discriminator loss: -0.0002263188362121582\n",
      "Epoch 423/500, Generator loss: -0.5000959038734436, Discriminator loss: 6.41942024230957e-05\n",
      "Epoch 424/500, Generator loss: -0.49960097670555115, Discriminator loss: 7.191300392150879e-05\n",
      "Epoch 425/500, Generator loss: -0.5000507831573486, Discriminator loss: -5.412101745605469e-05\n",
      "Epoch 426/500, Generator loss: -0.5001088976860046, Discriminator loss: 1.806020736694336e-05\n",
      "Epoch 427/500, Generator loss: -0.5001132488250732, Discriminator loss: -7.033348083496094e-06\n",
      "Epoch 428/500, Generator loss: -0.5001453161239624, Discriminator loss: 5.185604095458984e-06\n",
      "Epoch 429/500, Generator loss: -0.5001493692398071, Discriminator loss: 4.112720489501953e-06\n",
      "Epoch 430/500, Generator loss: -0.5001744627952576, Discriminator loss: 1.3947486877441406e-05\n",
      "Epoch 431/500, Generator loss: -0.5001952648162842, Discriminator loss: -7.152557373046875e-07\n",
      "Epoch 432/500, Generator loss: -0.5001952648162842, Discriminator loss: 5.781650543212891e-06\n",
      "Epoch 433/500, Generator loss: -0.5002042055130005, Discriminator loss: 6.198883056640625e-06\n",
      "Epoch 434/500, Generator loss: -0.500211238861084, Discriminator loss: -5.662441253662109e-06\n",
      "Epoch 435/500, Generator loss: -0.5002276301383972, Discriminator loss: 5.066394805908203e-06\n",
      "Epoch 436/500, Generator loss: -0.5002159476280212, Discriminator loss: -9.5367431640625e-07\n",
      "Epoch 437/500, Generator loss: -0.5002120733261108, Discriminator loss: -1.6093254089355469e-06\n",
      "Epoch 438/500, Generator loss: -0.5002159476280212, Discriminator loss: 1.430511474609375e-06\n",
      "Epoch 439/500, Generator loss: -0.5002138018608093, Discriminator loss: -7.152557373046875e-07\n",
      "Epoch 440/500, Generator loss: -0.5002120733261108, Discriminator loss: -2.6226043701171875e-06\n",
      "Epoch 441/500, Generator loss: -0.5002028942108154, Discriminator loss: -5.364418029785156e-06\n",
      "Epoch 442/500, Generator loss: -0.5002037882804871, Discriminator loss: -5.900859832763672e-06\n",
      "Epoch 443/500, Generator loss: -0.5002081990242004, Discriminator loss: -7.748603820800781e-07\n",
      "Epoch 444/500, Generator loss: -0.5002129673957825, Discriminator loss: -2.1457672119140625e-05\n",
      "Epoch 445/500, Generator loss: -0.5001979470252991, Discriminator loss: -6.079673767089844e-06\n",
      "Epoch 446/500, Generator loss: -0.5001567006111145, Discriminator loss: -3.719329833984375e-05\n",
      "Epoch 447/500, Generator loss: -0.5000644326210022, Discriminator loss: -9.483098983764648e-05\n",
      "Epoch 448/500, Generator loss: -0.5016462802886963, Discriminator loss: -1.5139579772949219e-05\n",
      "Epoch 449/500, Generator loss: -0.499494343996048, Discriminator loss: 0.00024300813674926758\n",
      "Epoch 450/500, Generator loss: -0.5014553666114807, Discriminator loss: 0.00046759843826293945\n",
      "Epoch 451/500, Generator loss: -0.5021453499794006, Discriminator loss: -0.00010085105895996094\n",
      "Epoch 452/500, Generator loss: -0.4996556043624878, Discriminator loss: -0.000865638256072998\n",
      "Epoch 453/500, Generator loss: -0.4989136755466461, Discriminator loss: -0.002427607774734497\n",
      "Epoch 454/500, Generator loss: -0.4999285936355591, Discriminator loss: -0.0011993646621704102\n",
      "Epoch 455/500, Generator loss: -0.5008494853973389, Discriminator loss: -0.00018012523651123047\n",
      "Epoch 456/500, Generator loss: -0.501688539981842, Discriminator loss: -9.649991989135742e-05\n",
      "Epoch 457/500, Generator loss: -0.5012691617012024, Discriminator loss: -0.00015413761138916016\n",
      "Epoch 458/500, Generator loss: -0.49931660294532776, Discriminator loss: 7.539987564086914e-05\n",
      "Epoch 459/500, Generator loss: -0.49922478199005127, Discriminator loss: -0.0005567371845245361\n",
      "Epoch 460/500, Generator loss: -0.4996972680091858, Discriminator loss: -0.0011585652828216553\n",
      "Epoch 461/500, Generator loss: -0.5002426505088806, Discriminator loss: -0.0014640390872955322\n",
      "Epoch 462/500, Generator loss: -0.4986879825592041, Discriminator loss: 5.441904067993164e-05\n",
      "Epoch 463/500, Generator loss: -0.49730297923088074, Discriminator loss: 0.00011432170867919922\n",
      "Epoch 464/500, Generator loss: -0.498140424489975, Discriminator loss: -0.0007181763648986816\n",
      "Epoch 465/500, Generator loss: -0.5007554888725281, Discriminator loss: -0.0008968710899353027\n",
      "Epoch 466/500, Generator loss: -0.5008921027183533, Discriminator loss: -0.0019841790199279785\n",
      "Epoch 467/500, Generator loss: -0.49899375438690186, Discriminator loss: -0.000584721565246582\n",
      "Epoch 468/500, Generator loss: -0.5000427961349487, Discriminator loss: 1.8596649169921875e-05\n",
      "Epoch 469/500, Generator loss: -0.501400351524353, Discriminator loss: -0.0005022287368774414\n",
      "Epoch 470/500, Generator loss: -0.4997413158416748, Discriminator loss: 3.516674041748047e-06\n",
      "Epoch 471/500, Generator loss: -0.500322163105011, Discriminator loss: 1.3649463653564453e-05\n",
      "Epoch 472/500, Generator loss: -0.49984198808670044, Discriminator loss: -0.0015019774436950684\n",
      "Epoch 473/500, Generator loss: -0.5008848309516907, Discriminator loss: -0.0003253817558288574\n",
      "Epoch 474/500, Generator loss: -0.5012177228927612, Discriminator loss: -0.00012379884719848633\n",
      "Epoch 475/500, Generator loss: -0.4994812607765198, Discriminator loss: -0.0003140568733215332\n",
      "Epoch 476/500, Generator loss: -0.5008559226989746, Discriminator loss: -0.0015880763530731201\n",
      "Epoch 477/500, Generator loss: -0.49848315119743347, Discriminator loss: -0.0007557272911071777\n",
      "Epoch 478/500, Generator loss: -0.4972783327102661, Discriminator loss: -0.00012388825416564941\n",
      "Epoch 479/500, Generator loss: -0.4989820122718811, Discriminator loss: 1.3947486877441406e-05\n",
      "Epoch 480/500, Generator loss: -0.5009618401527405, Discriminator loss: 0.00030225515365600586\n",
      "Epoch 481/500, Generator loss: -0.5032054781913757, Discriminator loss: -7.236003875732422e-05\n",
      "Epoch 482/500, Generator loss: -0.5023462772369385, Discriminator loss: -0.0007823705673217773\n",
      "Epoch 483/500, Generator loss: -0.4980296492576599, Discriminator loss: -0.0008202791213989258\n",
      "Epoch 484/500, Generator loss: -0.4993560314178467, Discriminator loss: -0.00031122565269470215\n",
      "Epoch 485/500, Generator loss: -0.49736547470092773, Discriminator loss: -0.0005040764808654785\n",
      "Epoch 486/500, Generator loss: -0.4986002445220947, Discriminator loss: -0.0008185207843780518\n",
      "Epoch 487/500, Generator loss: -0.5014972686767578, Discriminator loss: -0.00016772747039794922\n",
      "Epoch 488/500, Generator loss: -0.49902284145355225, Discriminator loss: 0.0001589059829711914\n",
      "Epoch 489/500, Generator loss: -0.5018557906150818, Discriminator loss: -2.6226043701171875e-05\n",
      "Epoch 490/500, Generator loss: -0.49924546480178833, Discriminator loss: -0.0009139180183410645\n",
      "Epoch 491/500, Generator loss: -0.4990202486515045, Discriminator loss: -0.002119779586791992\n",
      "Epoch 492/500, Generator loss: -0.49724259972572327, Discriminator loss: -8.14199447631836e-05\n",
      "Epoch 493/500, Generator loss: -0.49738848209381104, Discriminator loss: -0.0007452666759490967\n",
      "Epoch 494/500, Generator loss: -0.5001853704452515, Discriminator loss: -1.5020370483398438e-05\n",
      "Epoch 495/500, Generator loss: -0.5005964636802673, Discriminator loss: -0.0019326210021972656\n",
      "Epoch 496/500, Generator loss: -0.5012063384056091, Discriminator loss: 7.82012939453125e-05\n",
      "Epoch 497/500, Generator loss: -0.5016865134239197, Discriminator loss: 3.790855407714844e-05\n",
      "Epoch 498/500, Generator loss: -0.5013576745986938, Discriminator loss: 6.604194641113281e-05\n",
      "Epoch 499/500, Generator loss: -0.5017412304878235, Discriminator loss: 2.5093555450439453e-05\n",
      "Epoch 500/500, Generator loss: -0.5015436410903931, Discriminator loss: 0.0003229379653930664\n"
     ]
    }
   ],
   "source": [
    "wgan = WGAN(dataset, num_latent_variables=num_latent_variables, lr=learning_rate, weight_clip=0.01)\n",
    "\n",
    "\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "num_epochs = 500\n",
    "num_critic = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for _, (signal_tensor, params_tensor) in enumerate(train_loader):\n",
    "        signal_tensor = signal_tensor.to(device)\n",
    "        params_tensor = params_tensor.to(device)\n",
    "        \n",
    "        for _ in range(num_critic):\n",
    "            z = torch.randn(signal_tensor.size(0), wgan.num_latent_variables, 1).to(device)\n",
    "            d_loss = wgan.train_discriminator(signal_tensor, params_tensor, z)\n",
    "        \n",
    "        z = torch.randn(signal_tensor.size(0), wgan.num_latent_variables, 1).to(device)\n",
    "        g_loss = wgan.train_generator(signal_tensor, z)\n",
    "        \n",
    "        g_loss_list.append(g_loss)\n",
    "        d_loss_list.append(d_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Generator loss: {g_loss}, Discriminator loss: {d_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.684454710500766, 0.2785319288485978, 0.9111084160949837]\n",
      "[ 8.713801   -0.14443322  3.5694919 ]\n",
      "[10.936496259803521, 0.2981078136250689, 0.03876672843272887]\n",
      "[ 7.270414   -0.79827875  3.768388  ]\n",
      "[10.215128402431985, 0.3925196061055872, 0.9322377266462796]\n",
      "[ 8.614659   -0.47095633  3.8795588 ]\n",
      "[10.791707429527968, 0.33887817237630685, 0.27712219355009865]\n",
      "[ 8.608505   -0.18909442  3.886261  ]\n",
      "[7.7862689190518175, 0.4057925059966937, 5.031714069836047]\n",
      "[ 9.326688   -0.22190507  3.570914  ]\n",
      "[10.306911124248657, 0.3847102181822575, 3.209721330044897]\n",
      "[ 9.125465   -0.33485416  4.3714294 ]\n",
      "[9.763407088603955, 0.4262137120658779, 0.909128988908456]\n",
      "[ 7.8996453  -0.35099566  3.423662  ]\n",
      "[10.333264513621586, 0.4328789376296332, 6.214777862046191]\n",
      "[9.233693   0.30373397 4.7812138 ]\n",
      "[10.665680283177151, 0.4382246718617583, 5.7737907865639]\n",
      "[8.732942   0.24169317 3.5212786 ]\n",
      "[8.859682325831159, 0.33792174466427066, 4.5196306327616265]\n",
      "[ 7.5417337  -0.32620895  3.1741993 ]\n"
     ]
    }
   ],
   "source": [
    "generator = wgan.generator\n",
    "generator.eval()\n",
    "\n",
    "for i in range(10):\n",
    "    TS = Signal_Generator(num_sources=1, noise_amplitude=1)\n",
    "    test_data = TS.generating_signal()\n",
    "    params = TS.printing_parameters()\n",
    "\n",
    "    input_signal = test_data['Signal'].values\n",
    "    input_signal_tensor = torch.tensor(input_signal, dtype=torch.float).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_params = generator(input_signal_tensor, z).squeeze().cpu().numpy()\n",
    "\n",
    "    print(params)\n",
    "    print(generated_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasLISA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
