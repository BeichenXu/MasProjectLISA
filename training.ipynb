{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import lightning as L\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Network import Generator, Discriminator\n",
    "from Signal_Generator import *\n",
    "from Signal_Analyzer import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(10):\n",
    "    SG = Signal_Generator(num_sources=1, noise_amplitude=1)\n",
    "    signals = SG.generating_signal()\n",
    "    params = SG.printing_parameters()\n",
    "    signal = signals['Signal'].values\n",
    "\n",
    "    signal_tensor = torch.tensor(signal, dtype=torch.float).unsqueeze(0).to(device)\n",
    "    params_tensor = torch.tensor(params, dtype=torch.float).to(device)\n",
    "\n",
    "    dataset.append((signal_tensor, params_tensor))\n",
    "\n",
    "train_loader = utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "num_latent_variables = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, dataset, num_latent_variables, lr):\n",
    "        self.dataset = dataset\n",
    "        self.num_latent_variables = num_latent_variables\n",
    "        self.lr = lr\n",
    "\n",
    "        # Networks\n",
    "        self.generator = Generator(in_channels=1, num_latent_variables=num_latent_variables, length=len(signal), num_parameters=len(params)).to(device)\n",
    "        self.discriminator = Discriminator(input_channels=1, length=len(signal), num_parameters=len(params)).to(device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_g = optim.Adam(self.generator.parameters(), lr=self.lr)\n",
    "        self.optimizer_d = optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "\n",
    "    def adversarial_loss(self, output_d, y):\n",
    "        return self.criterion(output_d, y)\n",
    "    \n",
    "    def train_generator(self, signal_tensor, z):\n",
    "        generated_params = self.generator(signal_tensor, z)\n",
    "        fake_output = self.discriminator(signal_tensor, generated_params)\n",
    "        g_loss = self.adversarial_loss(fake_output, torch.ones_like(fake_output))\n",
    "\n",
    "        self.optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_g.step()\n",
    "\n",
    "        return g_loss.item()\n",
    "    \n",
    "    def train_discriminator(self, signal_tensor, params_tensor, z):\n",
    "        fake_params = self.generator(signal_tensor, z).detach()\n",
    "        real_output = self.discriminator(signal_tensor, params_tensor)\n",
    "        fake_output = self.discriminator(signal_tensor, fake_params)\n",
    "\n",
    "        real_loss = self.adversarial_loss(real_output, torch.ones_like(real_output))\n",
    "        fake_loss = self.adversarial_loss(fake_output, torch.zeros_like(fake_output))\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        self.optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_d.step()\n",
    "\n",
    "        return d_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Discriminator loss: 0.6103736162185669\n",
      "Epoch 1, Discriminator loss: 0.4586426913738251\n",
      "Epoch 2, Discriminator loss: 0.08554631471633911\n",
      "Epoch 3, Discriminator loss: 0.04793523997068405\n",
      "Epoch 4, Discriminator loss: 0.029579130932688713\n",
      "Epoch 5, Discriminator loss: 0.02718707174062729\n",
      "Epoch 6, Discriminator loss: 0.01659131608903408\n",
      "Epoch 7, Discriminator loss: 0.017424246296286583\n",
      "Epoch 8, Discriminator loss: 0.011122768744826317\n",
      "Epoch 9, Discriminator loss: 0.01295013539493084\n",
      "Epoch 10, Discriminator loss: 0.00799653586000204\n",
      "Epoch 11, Discriminator loss: 0.008804685436189175\n",
      "Epoch 12, Discriminator loss: 0.007441840134561062\n",
      "Epoch 13, Discriminator loss: 0.006603815592825413\n",
      "Epoch 14, Discriminator loss: 0.005590219050645828\n",
      "Epoch 15, Discriminator loss: 0.005783941596746445\n",
      "Epoch 16, Discriminator loss: 0.006288745440542698\n",
      "Epoch 17, Discriminator loss: 0.004813989624381065\n",
      "Epoch 18, Discriminator loss: 0.004871400538831949\n",
      "Epoch 19, Discriminator loss: 0.004946405068039894\n",
      "Epoch 20, Discriminator loss: 0.008216137997806072\n",
      "Epoch 21, Discriminator loss: 0.004008008167147636\n",
      "Epoch 22, Discriminator loss: 0.003282085992395878\n",
      "Epoch 23, Discriminator loss: 0.0035576748196035624\n",
      "Epoch 24, Discriminator loss: 0.0034011630341410637\n",
      "Epoch 25, Discriminator loss: 0.004252200946211815\n",
      "Epoch 26, Discriminator loss: 0.003070756094530225\n",
      "Epoch 27, Discriminator loss: 0.004270633682608604\n",
      "Epoch 28, Discriminator loss: 0.002770198043435812\n",
      "Epoch 29, Discriminator loss: 0.0025370954535901546\n",
      "Epoch 30, Discriminator loss: 0.002545581664890051\n",
      "Epoch 31, Discriminator loss: 0.0029900679364800453\n",
      "Epoch 32, Discriminator loss: 0.00257178395986557\n",
      "Epoch 33, Discriminator loss: 0.003251755377277732\n",
      "Epoch 34, Discriminator loss: 0.0027880575507879257\n",
      "Epoch 35, Discriminator loss: 0.002760519739240408\n",
      "Epoch 36, Discriminator loss: 0.002427787985652685\n",
      "Epoch 37, Discriminator loss: 0.0019574083853513002\n",
      "Epoch 38, Discriminator loss: 0.0025879242457449436\n",
      "Epoch 39, Discriminator loss: 0.0018526059575378895\n",
      "Epoch 40, Discriminator loss: 0.0018108501099050045\n",
      "Epoch 41, Discriminator loss: 0.0024128337390720844\n",
      "Epoch 42, Discriminator loss: 0.0017771138809621334\n",
      "Epoch 43, Discriminator loss: 0.001994137419387698\n",
      "Epoch 44, Discriminator loss: 0.0017426802078261971\n",
      "Epoch 45, Discriminator loss: 0.0016305951867252588\n",
      "Epoch 46, Discriminator loss: 0.0015922209713608027\n",
      "Epoch 47, Discriminator loss: 0.001653185929171741\n",
      "Epoch 48, Discriminator loss: 0.0018755935598164797\n",
      "Epoch 49, Discriminator loss: 0.0018769379239529371\n",
      "Epoch 50, Discriminator loss: 0.0020908894948661327\n",
      "Epoch 51, Discriminator loss: 0.0016176789067685604\n",
      "Epoch 52, Discriminator loss: 0.0016040136106312275\n",
      "Epoch 53, Discriminator loss: 0.0014934814535081387\n",
      "Epoch 54, Discriminator loss: 0.0015084741171449423\n",
      "Epoch 55, Discriminator loss: 0.0015380234690383077\n",
      "Epoch 56, Discriminator loss: 0.0014682821929454803\n",
      "Epoch 57, Discriminator loss: 0.0013189518358558416\n",
      "Epoch 58, Discriminator loss: 0.0013485916424542665\n",
      "Epoch 59, Discriminator loss: 0.0012754186755046248\n",
      "Epoch 60, Discriminator loss: 0.001447815913707018\n",
      "Epoch 61, Discriminator loss: 0.0014982132706791162\n",
      "Epoch 62, Discriminator loss: 0.0012775679351761937\n",
      "Epoch 63, Discriminator loss: 0.0013853590935468674\n",
      "Epoch 64, Discriminator loss: 0.001275744871236384\n",
      "Epoch 65, Discriminator loss: 0.0011585860047489405\n",
      "Epoch 66, Discriminator loss: 0.0012299374211579561\n",
      "Epoch 67, Discriminator loss: 0.0015393114881590009\n",
      "Epoch 68, Discriminator loss: 0.001303940312936902\n",
      "Epoch 69, Discriminator loss: 0.0013477199245244265\n",
      "Epoch 70, Discriminator loss: 0.0011582886800169945\n",
      "Epoch 71, Discriminator loss: 0.0010785472113639116\n",
      "Epoch 72, Discriminator loss: 0.001111069694161415\n",
      "Epoch 73, Discriminator loss: 0.0011171684600412846\n",
      "Epoch 74, Discriminator loss: 0.0010886620730161667\n",
      "Epoch 75, Discriminator loss: 0.0010761967860162258\n",
      "Epoch 76, Discriminator loss: 0.0010875659063458443\n",
      "Epoch 77, Discriminator loss: 0.0010511676082387567\n",
      "Epoch 78, Discriminator loss: 0.0011510355398058891\n",
      "Epoch 79, Discriminator loss: 0.001041045063175261\n",
      "Epoch 80, Discriminator loss: 0.0010224939323961735\n",
      "Epoch 81, Discriminator loss: 0.0012045411858707666\n",
      "Epoch 82, Discriminator loss: 0.0010299497516825795\n",
      "Epoch 83, Discriminator loss: 0.0010925098322331905\n",
      "Epoch 84, Discriminator loss: 0.000962322112172842\n",
      "Epoch 85, Discriminator loss: 0.0011570232454687357\n",
      "Epoch 86, Discriminator loss: 0.0011453304905444384\n",
      "Epoch 87, Discriminator loss: 0.0009378959657624364\n",
      "Epoch 88, Discriminator loss: 0.000940575497224927\n",
      "Epoch 89, Discriminator loss: 0.0009255118202418089\n",
      "Epoch 90, Discriminator loss: 0.0009961033938452601\n",
      "Epoch 91, Discriminator loss: 0.0008721931371837854\n",
      "Epoch 92, Discriminator loss: 0.0009737591026350856\n",
      "Epoch 93, Discriminator loss: 0.000871905533131212\n",
      "Epoch 94, Discriminator loss: 0.0008828784339129925\n",
      "Epoch 95, Discriminator loss: 0.0008376659825444221\n",
      "Epoch 96, Discriminator loss: 0.0008680648170411587\n",
      "Epoch 97, Discriminator loss: 0.0010314017999917269\n",
      "Epoch 98, Discriminator loss: 0.0009666380938142538\n",
      "Epoch 99, Discriminator loss: 0.0008258484303951263\n",
      "Epoch 100, Discriminator loss: 0.0008407427230849862\n",
      "Epoch 101, Discriminator loss: 0.000813573831692338\n",
      "Epoch 102, Discriminator loss: 0.000917155179195106\n",
      "Epoch 103, Discriminator loss: 0.000790439429692924\n",
      "Epoch 104, Discriminator loss: 0.0008126204484142363\n",
      "Epoch 105, Discriminator loss: 0.0008554458618164062\n",
      "Epoch 106, Discriminator loss: 0.0008458286756649613\n",
      "Epoch 107, Discriminator loss: 0.0009222590597346425\n",
      "Epoch 108, Discriminator loss: 0.0007629977189935744\n",
      "Epoch 109, Discriminator loss: 0.000740597490221262\n",
      "Epoch 110, Discriminator loss: 0.0007866331143304706\n",
      "Epoch 111, Discriminator loss: 0.000737731228582561\n",
      "Epoch 112, Discriminator loss: 0.0009128833189606667\n",
      "Epoch 113, Discriminator loss: 0.0007419979083351791\n",
      "Epoch 114, Discriminator loss: 0.0008108734618872404\n",
      "Epoch 115, Discriminator loss: 0.0007825293578207493\n",
      "Epoch 116, Discriminator loss: 0.000791975820902735\n",
      "Epoch 117, Discriminator loss: 0.0008437575306743383\n",
      "Epoch 118, Discriminator loss: 0.000808287353720516\n",
      "Epoch 119, Discriminator loss: 0.0006972691044211388\n",
      "Epoch 120, Discriminator loss: 0.0007459033513441682\n",
      "Epoch 121, Discriminator loss: 0.0006752402405254543\n",
      "Epoch 122, Discriminator loss: 0.0007506905240006745\n",
      "Epoch 123, Discriminator loss: 0.0007251302013173699\n",
      "Epoch 124, Discriminator loss: 0.0006760756950825453\n",
      "Epoch 125, Discriminator loss: 0.0006599003681913018\n",
      "Epoch 126, Discriminator loss: 0.0006511362153105438\n",
      "Epoch 127, Discriminator loss: 0.0006426188629120588\n",
      "Epoch 128, Discriminator loss: 0.0006422095466405153\n",
      "Epoch 129, Discriminator loss: 0.0006750482716597617\n",
      "Epoch 130, Discriminator loss: 0.0007375634741038084\n",
      "Epoch 131, Discriminator loss: 0.0006345651345327497\n",
      "Epoch 132, Discriminator loss: 0.0006200233474373817\n",
      "Epoch 133, Discriminator loss: 0.0006141711492091417\n",
      "Epoch 134, Discriminator loss: 0.0006183059886097908\n",
      "Epoch 135, Discriminator loss: 0.0006046351627446711\n",
      "Epoch 136, Discriminator loss: 0.0006243137177079916\n",
      "Epoch 137, Discriminator loss: 0.0007761629531159997\n",
      "Epoch 138, Discriminator loss: 0.0006921763997524977\n",
      "Epoch 139, Discriminator loss: 0.0005859959055669606\n",
      "Epoch 140, Discriminator loss: 0.0005782990483567119\n",
      "Epoch 141, Discriminator loss: 0.0006119463359937072\n",
      "Epoch 142, Discriminator loss: 0.0006118317833170295\n",
      "Epoch 143, Discriminator loss: 0.0005780236097052693\n",
      "Epoch 144, Discriminator loss: 0.0005882989498786628\n",
      "Epoch 145, Discriminator loss: 0.0005849177832715213\n",
      "Epoch 146, Discriminator loss: 0.0005762755172327161\n",
      "Epoch 147, Discriminator loss: 0.0005566837498918176\n",
      "Epoch 148, Discriminator loss: 0.0005461655673570931\n",
      "Epoch 149, Discriminator loss: 0.0005457847146317363\n",
      "Epoch 150, Discriminator loss: 0.0005366289988160133\n",
      "Epoch 151, Discriminator loss: 0.0005639231530949473\n",
      "Epoch 152, Discriminator loss: 0.00052639190107584\n",
      "Epoch 153, Discriminator loss: 0.0005298059550113976\n",
      "Epoch 154, Discriminator loss: 0.0005518195684999228\n",
      "Epoch 155, Discriminator loss: 0.0005539290141314268\n",
      "Epoch 156, Discriminator loss: 0.0006047853385098279\n",
      "Epoch 157, Discriminator loss: 0.0005109627963975072\n",
      "Epoch 158, Discriminator loss: 0.0005113189108669758\n",
      "Epoch 159, Discriminator loss: 0.0004999982193112373\n",
      "Epoch 160, Discriminator loss: 0.0005014659836888313\n",
      "Epoch 161, Discriminator loss: 0.0005030668107792735\n",
      "Epoch 162, Discriminator loss: 0.0004874155856668949\n",
      "Epoch 163, Discriminator loss: 0.0004955572076141834\n",
      "Epoch 164, Discriminator loss: 0.0005105045274831355\n",
      "Epoch 165, Discriminator loss: 0.0005923816934227943\n",
      "Epoch 166, Discriminator loss: 0.0005128327757120132\n",
      "Epoch 167, Discriminator loss: 0.00047519116196781397\n",
      "Epoch 168, Discriminator loss: 0.0004879501066170633\n",
      "Epoch 169, Discriminator loss: 0.0004715853137895465\n",
      "Epoch 170, Discriminator loss: 0.00047178089153021574\n",
      "Epoch 171, Discriminator loss: 0.00046034439583308995\n",
      "Epoch 172, Discriminator loss: 0.00045711768325418234\n",
      "Epoch 173, Discriminator loss: 0.0004911818541586399\n",
      "Epoch 174, Discriminator loss: 0.00045887508895248175\n",
      "Epoch 175, Discriminator loss: 0.00044776126742362976\n",
      "Epoch 176, Discriminator loss: 0.0004888976109214127\n",
      "Epoch 177, Discriminator loss: 0.0004547883872874081\n",
      "Epoch 178, Discriminator loss: 0.0004370801034383476\n",
      "Epoch 179, Discriminator loss: 0.0004393912968225777\n",
      "Epoch 180, Discriminator loss: 0.00044677400728687644\n",
      "Epoch 181, Discriminator loss: 0.0005667024524882436\n",
      "Epoch 182, Discriminator loss: 0.00047019723569974303\n",
      "Epoch 183, Discriminator loss: 0.00045501941349357367\n",
      "Epoch 184, Discriminator loss: 0.00046741479309275746\n",
      "Epoch 185, Discriminator loss: 0.0004611714684870094\n",
      "Epoch 186, Discriminator loss: 0.0004291425575502217\n",
      "Epoch 187, Discriminator loss: 0.0004108385182917118\n",
      "Epoch 188, Discriminator loss: 0.00040828128112480044\n",
      "Epoch 189, Discriminator loss: 0.00040883844485506415\n",
      "Epoch 190, Discriminator loss: 0.0004789124068338424\n",
      "Epoch 191, Discriminator loss: 0.0004034444864373654\n",
      "Epoch 192, Discriminator loss: 0.00041443618829362094\n",
      "Epoch 193, Discriminator loss: 0.0003902265161741525\n",
      "Epoch 194, Discriminator loss: 0.0003872973029501736\n",
      "Epoch 195, Discriminator loss: 0.00039439357351511717\n",
      "Epoch 196, Discriminator loss: 0.00039326277328655124\n",
      "Epoch 197, Discriminator loss: 0.00042368590948171914\n",
      "Epoch 198, Discriminator loss: 0.00037942006019875407\n",
      "Epoch 199, Discriminator loss: 0.00041302916361019015\n",
      "Epoch 200, Discriminator loss: 0.00041928995051421225\n",
      "Epoch 201, Discriminator loss: 0.0003736947546713054\n",
      "Epoch 202, Discriminator loss: 0.0003889105864800513\n",
      "Epoch 203, Discriminator loss: 0.0003694053739309311\n",
      "Epoch 204, Discriminator loss: 0.0003697852953337133\n",
      "Epoch 205, Discriminator loss: 0.00038925191620364785\n",
      "Epoch 206, Discriminator loss: 0.00036210560938343406\n",
      "Epoch 207, Discriminator loss: 0.0003592571010813117\n",
      "Epoch 208, Discriminator loss: 0.0003779386170208454\n",
      "Epoch 209, Discriminator loss: 0.000357714481651783\n",
      "Epoch 210, Discriminator loss: 0.000386505329515785\n",
      "Epoch 211, Discriminator loss: 0.00038282645982690156\n",
      "Epoch 212, Discriminator loss: 0.00034052462433464825\n",
      "Epoch 213, Discriminator loss: 0.00038230555946938694\n",
      "Epoch 214, Discriminator loss: 0.0003533842973411083\n",
      "Epoch 215, Discriminator loss: 0.00033812224864959717\n",
      "Epoch 216, Discriminator loss: 0.00037647082353942096\n",
      "Epoch 217, Discriminator loss: 0.00033337841159664094\n",
      "Epoch 218, Discriminator loss: 0.00033528212225064635\n",
      "Epoch 219, Discriminator loss: 0.000327482441207394\n",
      "Epoch 220, Discriminator loss: 0.0003449677606113255\n",
      "Epoch 221, Discriminator loss: 0.0003382986760698259\n",
      "Epoch 222, Discriminator loss: 0.00032261706655845046\n",
      "Epoch 223, Discriminator loss: 0.0003164552617818117\n",
      "Epoch 224, Discriminator loss: 0.00034896156284958124\n",
      "Epoch 225, Discriminator loss: 0.00031816906994208694\n",
      "Epoch 226, Discriminator loss: 0.00031339802080765367\n",
      "Epoch 227, Discriminator loss: 0.00032421862124465406\n",
      "Epoch 228, Discriminator loss: 0.0003288287844043225\n",
      "Epoch 229, Discriminator loss: 0.00033710076240822673\n",
      "Epoch 230, Discriminator loss: 0.00030155328568071127\n",
      "Epoch 231, Discriminator loss: 0.00030304992105811834\n",
      "Epoch 232, Discriminator loss: 0.00030165634234435856\n",
      "Epoch 233, Discriminator loss: 0.00031281134579330683\n",
      "Epoch 234, Discriminator loss: 0.0003094356507062912\n",
      "Epoch 235, Discriminator loss: 0.0003079035668633878\n",
      "Epoch 236, Discriminator loss: 0.00028995538013987243\n",
      "Epoch 237, Discriminator loss: 0.0002905742730945349\n",
      "Epoch 238, Discriminator loss: 0.0002988710766658187\n",
      "Epoch 239, Discriminator loss: 0.0002988952037412673\n",
      "Epoch 240, Discriminator loss: 0.0003288154839538038\n",
      "Epoch 241, Discriminator loss: 0.0002852763282135129\n",
      "Epoch 242, Discriminator loss: 0.0003079515299759805\n",
      "Epoch 243, Discriminator loss: 0.0002806472184602171\n",
      "Epoch 244, Discriminator loss: 0.0002788991550914943\n",
      "Epoch 245, Discriminator loss: 0.0002850508317351341\n",
      "Epoch 246, Discriminator loss: 0.0002843517577275634\n",
      "Epoch 247, Discriminator loss: 0.00027066917391493917\n",
      "Epoch 248, Discriminator loss: 0.0002706899540498853\n",
      "Epoch 249, Discriminator loss: 0.0002680401084944606\n",
      "Epoch 250, Discriminator loss: 0.00026886764680966735\n",
      "Epoch 251, Discriminator loss: 0.00026181997964158654\n",
      "Epoch 252, Discriminator loss: 0.00026042648823931813\n",
      "Epoch 253, Discriminator loss: 0.00026404630625620484\n",
      "Epoch 254, Discriminator loss: 0.00026905816048383713\n",
      "Epoch 255, Discriminator loss: 0.000313084659865126\n",
      "Epoch 256, Discriminator loss: 0.00027694430900737643\n",
      "Epoch 257, Discriminator loss: 0.0002686048101168126\n",
      "Epoch 258, Discriminator loss: 0.0002636074204929173\n",
      "Epoch 259, Discriminator loss: 0.0002497220993973315\n",
      "Epoch 260, Discriminator loss: 0.00025481131160631776\n",
      "Epoch 261, Discriminator loss: 0.0002513965009711683\n",
      "Epoch 262, Discriminator loss: 0.00024739332729950547\n",
      "Epoch 263, Discriminator loss: 0.00024371518520638347\n",
      "Epoch 264, Discriminator loss: 0.000246322073508054\n",
      "Epoch 265, Discriminator loss: 0.0002660343307070434\n",
      "Epoch 266, Discriminator loss: 0.00024078867863863707\n",
      "Epoch 267, Discriminator loss: 0.00025319759151898324\n",
      "Epoch 268, Discriminator loss: 0.00023859983775764704\n",
      "Epoch 269, Discriminator loss: 0.0002540010609664023\n",
      "Epoch 270, Discriminator loss: 0.00024253586889244616\n",
      "Epoch 271, Discriminator loss: 0.00023276690626516938\n",
      "Epoch 272, Discriminator loss: 0.00024676613975316286\n",
      "Epoch 273, Discriminator loss: 0.00024455206585116684\n",
      "Epoch 274, Discriminator loss: 0.0002277806052006781\n",
      "Epoch 275, Discriminator loss: 0.0002484794531483203\n",
      "Epoch 276, Discriminator loss: 0.00023313009296543896\n",
      "Epoch 277, Discriminator loss: 0.0002236862783320248\n",
      "Epoch 278, Discriminator loss: 0.00022225320572033525\n",
      "Epoch 279, Discriminator loss: 0.0002412533649476245\n",
      "Epoch 280, Discriminator loss: 0.00023731525288894773\n",
      "Epoch 281, Discriminator loss: 0.00021849526092410088\n",
      "Epoch 282, Discriminator loss: 0.0002423353143967688\n",
      "Epoch 283, Discriminator loss: 0.0002292099961778149\n",
      "Epoch 284, Discriminator loss: 0.00021710882720071822\n",
      "Epoch 285, Discriminator loss: 0.00022013968555256724\n",
      "Epoch 286, Discriminator loss: 0.0002281445194967091\n",
      "Epoch 287, Discriminator loss: 0.00021942617604508996\n",
      "Epoch 288, Discriminator loss: 0.00020809314446523786\n",
      "Epoch 289, Discriminator loss: 0.00021825666772201657\n",
      "Epoch 290, Discriminator loss: 0.00020948676683474332\n",
      "Epoch 291, Discriminator loss: 0.00020309278625063598\n",
      "Epoch 292, Discriminator loss: 0.00021177108283154666\n",
      "Epoch 293, Discriminator loss: 0.00021252398437354714\n",
      "Epoch 294, Discriminator loss: 0.0002028590824920684\n",
      "Epoch 295, Discriminator loss: 0.00020927676814608276\n",
      "Epoch 296, Discriminator loss: 0.00019751128274947405\n",
      "Epoch 297, Discriminator loss: 0.0002123111189575866\n",
      "Epoch 298, Discriminator loss: 0.00019758715643547475\n",
      "Epoch 299, Discriminator loss: 0.00019594354671426117\n",
      "Epoch 300, Discriminator loss: 0.00019455351866781712\n",
      "Epoch 301, Discriminator loss: 0.00019478442845866084\n",
      "Epoch 302, Discriminator loss: 0.000192563165910542\n",
      "Epoch 303, Discriminator loss: 0.00020055408822372556\n",
      "Epoch 304, Discriminator loss: 0.00019042938947677612\n",
      "Epoch 305, Discriminator loss: 0.00019594328477978706\n",
      "Epoch 306, Discriminator loss: 0.0001869582338258624\n",
      "Epoch 307, Discriminator loss: 0.00019254343351349235\n",
      "Epoch 308, Discriminator loss: 0.0001843481295509264\n",
      "Epoch 309, Discriminator loss: 0.00018165475921705365\n",
      "Epoch 310, Discriminator loss: 0.00018876336980611086\n",
      "Epoch 311, Discriminator loss: 0.000203876435989514\n",
      "Epoch 312, Discriminator loss: 0.0001798124867491424\n",
      "Epoch 313, Discriminator loss: 0.0001854645524872467\n",
      "Epoch 314, Discriminator loss: 0.00018801340775098652\n",
      "Epoch 315, Discriminator loss: 0.0001778044388629496\n",
      "Epoch 316, Discriminator loss: 0.00017545331502333283\n",
      "Epoch 317, Discriminator loss: 0.00019001614418812096\n",
      "Epoch 318, Discriminator loss: 0.00018480353173799813\n",
      "Epoch 319, Discriminator loss: 0.00019175553461536765\n",
      "Epoch 320, Discriminator loss: 0.00018401994020678103\n",
      "Epoch 321, Discriminator loss: 0.0001817476295400411\n",
      "Epoch 322, Discriminator loss: 0.0001705458853393793\n",
      "Epoch 323, Discriminator loss: 0.00018060964066535234\n",
      "Epoch 324, Discriminator loss: 0.0001669130870141089\n",
      "Epoch 325, Discriminator loss: 0.00017535210645291954\n",
      "Epoch 326, Discriminator loss: 0.00017125964222941548\n",
      "Epoch 327, Discriminator loss: 0.00016362624592147768\n",
      "Epoch 328, Discriminator loss: 0.00016275806410703808\n",
      "Epoch 329, Discriminator loss: 0.00016201118705794215\n",
      "Epoch 330, Discriminator loss: 0.00018248643027618527\n",
      "Epoch 331, Discriminator loss: 0.00016993095050565898\n",
      "Epoch 332, Discriminator loss: 0.000157371730892919\n",
      "Epoch 333, Discriminator loss: 0.00016550729924347252\n",
      "Epoch 334, Discriminator loss: 0.00015668311971239746\n",
      "Epoch 335, Discriminator loss: 0.0001594780187588185\n",
      "Epoch 336, Discriminator loss: 0.00017558940453454852\n",
      "Epoch 337, Discriminator loss: 0.00015469608479179442\n",
      "Epoch 338, Discriminator loss: 0.0001543583202874288\n",
      "Epoch 339, Discriminator loss: 0.00015777235967107117\n",
      "Epoch 340, Discriminator loss: 0.00015643663937225938\n",
      "Epoch 341, Discriminator loss: 0.0001879222400020808\n",
      "Epoch 342, Discriminator loss: 0.0001522263919468969\n",
      "Epoch 343, Discriminator loss: 0.00015758135123178363\n",
      "Epoch 344, Discriminator loss: 0.00014785470557399094\n",
      "Epoch 345, Discriminator loss: 0.00015457859262824059\n",
      "Epoch 346, Discriminator loss: 0.00015975080896168947\n",
      "Epoch 347, Discriminator loss: 0.0001500812650192529\n",
      "Epoch 348, Discriminator loss: 0.0001452517171856016\n",
      "Epoch 349, Discriminator loss: 0.00014243062469176948\n",
      "Epoch 350, Discriminator loss: 0.0001572293695062399\n",
      "Epoch 351, Discriminator loss: 0.00014264570199884474\n",
      "Epoch 352, Discriminator loss: 0.00014211767120286822\n",
      "Epoch 353, Discriminator loss: 0.0001393389975419268\n",
      "Epoch 354, Discriminator loss: 0.00014591631770599633\n",
      "Epoch 355, Discriminator loss: 0.0001385982322972268\n",
      "Epoch 356, Discriminator loss: 0.00014414383622352034\n",
      "Epoch 357, Discriminator loss: 0.00014298170572146773\n",
      "Epoch 358, Discriminator loss: 0.0001491388538852334\n",
      "Epoch 359, Discriminator loss: 0.00014495759387500584\n",
      "Epoch 360, Discriminator loss: 0.00013832458353135735\n",
      "Epoch 361, Discriminator loss: 0.0001325041230302304\n",
      "Epoch 362, Discriminator loss: 0.00013218994718044996\n",
      "Epoch 363, Discriminator loss: 0.0001395270082866773\n",
      "Epoch 364, Discriminator loss: 0.00013112275337334722\n",
      "Epoch 365, Discriminator loss: 0.0001302527089137584\n",
      "Epoch 366, Discriminator loss: 0.0001297284907195717\n",
      "Epoch 367, Discriminator loss: 0.00014856031339149922\n",
      "Epoch 368, Discriminator loss: 0.00012786916340701282\n",
      "Epoch 369, Discriminator loss: 0.0001322872849414125\n",
      "Epoch 370, Discriminator loss: 0.00013807474169880152\n",
      "Epoch 371, Discriminator loss: 0.0001267074840143323\n",
      "Epoch 372, Discriminator loss: 0.00013475457672029734\n",
      "Epoch 373, Discriminator loss: 0.00012436771066859365\n",
      "Epoch 374, Discriminator loss: 0.00012686203990597278\n",
      "Epoch 375, Discriminator loss: 0.00012161211634520441\n",
      "Epoch 376, Discriminator loss: 0.00013010084512643516\n",
      "Epoch 377, Discriminator loss: 0.00012749363668262959\n",
      "Epoch 378, Discriminator loss: 0.00012053071259288117\n",
      "Epoch 379, Discriminator loss: 0.0001241447462234646\n",
      "Epoch 380, Discriminator loss: 0.0001234225055668503\n",
      "Epoch 381, Discriminator loss: 0.00011784447269747034\n",
      "Epoch 382, Discriminator loss: 0.00011766615352826193\n",
      "Epoch 383, Discriminator loss: 0.0001226422464242205\n",
      "Epoch 384, Discriminator loss: 0.00011620893201325089\n",
      "Epoch 385, Discriminator loss: 0.0001164652785519138\n",
      "Epoch 386, Discriminator loss: 0.00011897632793989033\n",
      "Epoch 387, Discriminator loss: 0.00011869384616147727\n",
      "Epoch 388, Discriminator loss: 0.00011411173909436911\n",
      "Epoch 389, Discriminator loss: 0.00011222812463529408\n",
      "Epoch 390, Discriminator loss: 0.00011556221579667181\n",
      "Epoch 391, Discriminator loss: 0.00012009440979454666\n",
      "Epoch 392, Discriminator loss: 0.00011931951303267851\n",
      "Epoch 393, Discriminator loss: 0.00011307956447126344\n",
      "Epoch 394, Discriminator loss: 0.00011006074055330828\n",
      "Epoch 395, Discriminator loss: 0.00011714090942405164\n",
      "Epoch 396, Discriminator loss: 0.00011186968913534656\n",
      "Epoch 397, Discriminator loss: 0.00011879463272634894\n",
      "Epoch 398, Discriminator loss: 0.00010701073915697634\n",
      "Epoch 399, Discriminator loss: 0.0001099568180507049\n",
      "Epoch 400, Discriminator loss: 0.00011291214468656108\n",
      "Epoch 401, Discriminator loss: 0.00010489887790754437\n",
      "Epoch 402, Discriminator loss: 0.00010645100701367483\n",
      "Epoch 403, Discriminator loss: 0.00011483347770990804\n",
      "Epoch 404, Discriminator loss: 0.00010862279305001721\n",
      "Epoch 405, Discriminator loss: 0.00010593405022518709\n",
      "Epoch 406, Discriminator loss: 0.00010688812471926212\n",
      "Epoch 407, Discriminator loss: 0.00011102601274615154\n",
      "Epoch 408, Discriminator loss: 0.00010570144513621926\n",
      "Epoch 409, Discriminator loss: 0.00010525267134653404\n",
      "Epoch 410, Discriminator loss: 9.88668471109122e-05\n",
      "Epoch 411, Discriminator loss: 0.00010129043948836625\n",
      "Epoch 412, Discriminator loss: 0.0001071109072654508\n",
      "Epoch 413, Discriminator loss: 0.00010339150321669877\n",
      "Epoch 414, Discriminator loss: 9.744871931616217e-05\n",
      "Epoch 415, Discriminator loss: 9.622209472581744e-05\n",
      "Epoch 416, Discriminator loss: 0.00010047950490843505\n",
      "Epoch 417, Discriminator loss: 0.00010132751776836812\n",
      "Epoch 418, Discriminator loss: 9.570874681230634e-05\n",
      "Epoch 419, Discriminator loss: 9.749893069965765e-05\n",
      "Epoch 420, Discriminator loss: 0.00010049781121779233\n",
      "Epoch 421, Discriminator loss: 9.44958592299372e-05\n",
      "Epoch 422, Discriminator loss: 0.0001025170786306262\n",
      "Epoch 423, Discriminator loss: 9.694242908153683e-05\n",
      "Epoch 424, Discriminator loss: 9.135951404459774e-05\n",
      "Epoch 425, Discriminator loss: 0.00010422050399938598\n",
      "Epoch 426, Discriminator loss: 9.603280341252685e-05\n",
      "Epoch 427, Discriminator loss: 8.990691276267171e-05\n",
      "Epoch 428, Discriminator loss: 9.0112051111646e-05\n",
      "Epoch 429, Discriminator loss: 9.399627742823213e-05\n",
      "Epoch 430, Discriminator loss: 9.323378617409617e-05\n",
      "Epoch 431, Discriminator loss: 9.130535181611776e-05\n",
      "Epoch 432, Discriminator loss: 9.340470569441095e-05\n",
      "Epoch 433, Discriminator loss: 9.210816642735153e-05\n",
      "Epoch 434, Discriminator loss: 9.224496898241341e-05\n",
      "Epoch 435, Discriminator loss: 8.579835412092507e-05\n",
      "Epoch 436, Discriminator loss: 8.554194937460124e-05\n",
      "Epoch 437, Discriminator loss: 8.5435327491723e-05\n",
      "Epoch 438, Discriminator loss: 9.181885252473876e-05\n",
      "Epoch 439, Discriminator loss: 8.977565448731184e-05\n",
      "Epoch 440, Discriminator loss: 8.381705265492201e-05\n",
      "Epoch 441, Discriminator loss: 8.2960250438191e-05\n",
      "Epoch 442, Discriminator loss: 8.231389801949263e-05\n",
      "Epoch 443, Discriminator loss: 8.664002234581858e-05\n",
      "Epoch 444, Discriminator loss: 8.598686690675095e-05\n",
      "Epoch 445, Discriminator loss: 8.439844532404095e-05\n",
      "Epoch 446, Discriminator loss: 8.056762453634292e-05\n",
      "Epoch 447, Discriminator loss: 8.066813461482525e-05\n",
      "Epoch 448, Discriminator loss: 8.496994996676221e-05\n",
      "Epoch 449, Discriminator loss: 8.094764780253172e-05\n",
      "Epoch 450, Discriminator loss: 8.301383059006184e-05\n",
      "Epoch 451, Discriminator loss: 7.824049680493772e-05\n",
      "Epoch 452, Discriminator loss: 7.803316111676395e-05\n",
      "Epoch 453, Discriminator loss: 7.726689364062622e-05\n",
      "Epoch 454, Discriminator loss: 7.74206273490563e-05\n",
      "Epoch 455, Discriminator loss: 7.999341323738918e-05\n",
      "Epoch 456, Discriminator loss: 7.835499127395451e-05\n",
      "Epoch 457, Discriminator loss: 8.051444456214085e-05\n",
      "Epoch 458, Discriminator loss: 7.540911610703915e-05\n",
      "Epoch 459, Discriminator loss: 8.011595491552725e-05\n",
      "Epoch 460, Discriminator loss: 7.996211934369057e-05\n",
      "Epoch 461, Discriminator loss: 7.984801777638495e-05\n",
      "Epoch 462, Discriminator loss: 7.788106449879706e-05\n",
      "Epoch 463, Discriminator loss: 7.314968388527632e-05\n",
      "Epoch 464, Discriminator loss: 7.387838559225202e-05\n",
      "Epoch 465, Discriminator loss: 7.416275911964476e-05\n",
      "Epoch 466, Discriminator loss: 7.669439946766943e-05\n",
      "Epoch 467, Discriminator loss: 7.151524914661422e-05\n",
      "Epoch 468, Discriminator loss: 7.156369974836707e-05\n",
      "Epoch 469, Discriminator loss: 7.149142038542777e-05\n",
      "Epoch 470, Discriminator loss: 7.013418507995084e-05\n",
      "Epoch 471, Discriminator loss: 7.365726924035698e-05\n",
      "Epoch 472, Discriminator loss: 6.980171019677073e-05\n",
      "Epoch 473, Discriminator loss: 7.206678128568456e-05\n",
      "Epoch 474, Discriminator loss: 7.684054435230792e-05\n",
      "Epoch 475, Discriminator loss: 7.131683378247544e-05\n",
      "Epoch 476, Discriminator loss: 7.024829392321408e-05\n",
      "Epoch 477, Discriminator loss: 7.068582635838538e-05\n",
      "Epoch 478, Discriminator loss: 7.064070086926222e-05\n",
      "Epoch 479, Discriminator loss: 6.994986324571073e-05\n",
      "Epoch 480, Discriminator loss: 6.754614878445864e-05\n",
      "Epoch 481, Discriminator loss: 7.012106652837247e-05\n",
      "Epoch 482, Discriminator loss: 6.607035174965858e-05\n",
      "Epoch 483, Discriminator loss: 6.826956814620644e-05\n",
      "Epoch 484, Discriminator loss: 6.478650902863592e-05\n",
      "Epoch 485, Discriminator loss: 6.460469012381509e-05\n",
      "Epoch 486, Discriminator loss: 6.95704948157072e-05\n",
      "Epoch 487, Discriminator loss: 6.865085742902011e-05\n",
      "Epoch 488, Discriminator loss: 6.642009248025715e-05\n",
      "Epoch 489, Discriminator loss: 6.680801016045734e-05\n",
      "Epoch 490, Discriminator loss: 6.272980681387708e-05\n",
      "Epoch 491, Discriminator loss: 6.269392179092392e-05\n",
      "Epoch 492, Discriminator loss: 6.208405829966068e-05\n",
      "Epoch 493, Discriminator loss: 6.253032188396901e-05\n",
      "Epoch 494, Discriminator loss: 6.601709901588038e-05\n",
      "Epoch 495, Discriminator loss: 6.152197602204978e-05\n",
      "Epoch 496, Discriminator loss: 6.31458024145104e-05\n",
      "Epoch 497, Discriminator loss: 6.345103611238301e-05\n",
      "Epoch 498, Discriminator loss: 5.9918194892816246e-05\n",
      "Epoch 499, Discriminator loss: 6.0015303461113945e-05\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(dataset, num_latent_variables=num_latent_variables, lr=learning_rate)\n",
    "\n",
    "d_loss_list = []\n",
    "# Train the GAN\n",
    "for i in range(500):\n",
    "    for _, (signal_tensor, params_tensor) in enumerate(train_loader):\n",
    "        z = torch.randn(1, num_latent_variables, 1).to(device)\n",
    "        d_loss = gan.train_discriminator(signal_tensor, params_tensor, z)\n",
    "        d_loss_list.append(d_loss)\n",
    "    print(f\"Epoch {i}, Discriminator loss: {d_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(d_loss_list)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrater\\.conda\\envs\\MasLISA\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Generator loss: 0.9598954916000366, Discriminator loss: 0.4602241814136505\n",
      "Epoch 1, Generator loss: 0.5524179339408875, Discriminator loss: 0.628926694393158\n",
      "Epoch 2, Generator loss: 0.8993642926216125, Discriminator loss: 0.6622500419616699\n",
      "Epoch 3, Generator loss: 0.6609662175178528, Discriminator loss: 0.6545752286911011\n",
      "Epoch 4, Generator loss: 0.8864288330078125, Discriminator loss: 0.6450120210647583\n",
      "Epoch 5, Generator loss: 0.6938309669494629, Discriminator loss: 0.6697636246681213\n",
      "Epoch 6, Generator loss: 0.7026347517967224, Discriminator loss: 0.7234135866165161\n",
      "Epoch 7, Generator loss: 0.765996515750885, Discriminator loss: 0.72740638256073\n",
      "Epoch 8, Generator loss: 0.7653149366378784, Discriminator loss: 0.8561668992042542\n",
      "Epoch 9, Generator loss: 0.6270281076431274, Discriminator loss: 0.7690722942352295\n",
      "Epoch 10, Generator loss: 0.6094039678573608, Discriminator loss: 0.7759862542152405\n",
      "Epoch 11, Generator loss: 0.6626381874084473, Discriminator loss: 0.7501171827316284\n",
      "Epoch 12, Generator loss: 0.6941866278648376, Discriminator loss: 0.7093397378921509\n",
      "Epoch 13, Generator loss: 0.6374912858009338, Discriminator loss: 0.7289569973945618\n",
      "Epoch 14, Generator loss: 0.6675301790237427, Discriminator loss: 0.7196645736694336\n",
      "Epoch 15, Generator loss: 0.6839364767074585, Discriminator loss: 0.7095158696174622\n",
      "Epoch 16, Generator loss: 0.6629821062088013, Discriminator loss: 0.7090046405792236\n",
      "Epoch 17, Generator loss: 0.6868882775306702, Discriminator loss: 0.6957917213439941\n",
      "Epoch 18, Generator loss: 0.6872416138648987, Discriminator loss: 0.696675181388855\n",
      "Epoch 19, Generator loss: 0.6872181296348572, Discriminator loss: 0.6991108655929565\n",
      "Epoch 20, Generator loss: 0.6868813037872314, Discriminator loss: 0.6941251754760742\n",
      "Epoch 21, Generator loss: 0.6893278956413269, Discriminator loss: 0.6957764625549316\n",
      "Epoch 22, Generator loss: 0.6830145120620728, Discriminator loss: 0.6942135095596313\n",
      "Epoch 23, Generator loss: 0.6876972913742065, Discriminator loss: 0.6935972571372986\n",
      "Epoch 24, Generator loss: 0.7008105516433716, Discriminator loss: 0.6913585662841797\n",
      "Epoch 25, Generator loss: 0.6854209899902344, Discriminator loss: 0.6907235383987427\n",
      "Epoch 26, Generator loss: 0.6930278539657593, Discriminator loss: 0.6898250579833984\n",
      "Epoch 27, Generator loss: 0.6937111616134644, Discriminator loss: 0.6899805068969727\n",
      "Epoch 28, Generator loss: 0.6873735189437866, Discriminator loss: 0.6886449456214905\n",
      "Epoch 29, Generator loss: 0.6877712607383728, Discriminator loss: 0.6892396211624146\n",
      "Epoch 30, Generator loss: 0.694931149482727, Discriminator loss: 0.6858704686164856\n",
      "Epoch 31, Generator loss: 0.7387601137161255, Discriminator loss: 0.6686978340148926\n",
      "Epoch 32, Generator loss: 0.6922385096549988, Discriminator loss: 0.6828517913818359\n",
      "Epoch 33, Generator loss: 0.7034767866134644, Discriminator loss: 0.6773676872253418\n",
      "Epoch 34, Generator loss: 0.6764910221099854, Discriminator loss: 0.6847923994064331\n",
      "Epoch 35, Generator loss: 0.7024874687194824, Discriminator loss: 0.6720662713050842\n",
      "Epoch 36, Generator loss: 0.6895521283149719, Discriminator loss: 0.6796212196350098\n",
      "Epoch 37, Generator loss: 0.7415014505386353, Discriminator loss: 0.6635768413543701\n",
      "Epoch 38, Generator loss: 0.7598275542259216, Discriminator loss: 0.6434729099273682\n",
      "Epoch 39, Generator loss: 0.7141201496124268, Discriminator loss: 0.6555538177490234\n",
      "Epoch 40, Generator loss: 0.69820636510849, Discriminator loss: 0.6715967655181885\n",
      "Epoch 41, Generator loss: 0.721099853515625, Discriminator loss: 0.6618944406509399\n",
      "Epoch 42, Generator loss: 0.6678962707519531, Discriminator loss: 0.6882073283195496\n",
      "Epoch 43, Generator loss: 0.7762786746025085, Discriminator loss: 0.6475477814674377\n",
      "Epoch 44, Generator loss: 0.8089059591293335, Discriminator loss: 0.6323471069335938\n",
      "Epoch 45, Generator loss: 0.6704506874084473, Discriminator loss: 0.6879010200500488\n",
      "Epoch 46, Generator loss: 0.6792877316474915, Discriminator loss: 0.6902696490287781\n",
      "Epoch 47, Generator loss: 0.6755098700523376, Discriminator loss: 0.7093491554260254\n",
      "Epoch 48, Generator loss: 0.7310382723808289, Discriminator loss: 0.6927611231803894\n",
      "Epoch 49, Generator loss: 0.7073368430137634, Discriminator loss: 0.713455319404602\n",
      "Epoch 50, Generator loss: 0.6695241928100586, Discriminator loss: 0.709750235080719\n",
      "Epoch 51, Generator loss: 0.6991268992424011, Discriminator loss: 0.7007904052734375\n",
      "Epoch 52, Generator loss: 0.658228874206543, Discriminator loss: 0.7111501693725586\n",
      "Epoch 53, Generator loss: 0.6750333905220032, Discriminator loss: 0.7118063569068909\n",
      "Epoch 54, Generator loss: 0.6825047135353088, Discriminator loss: 0.7078641057014465\n",
      "Epoch 55, Generator loss: 0.7032188177108765, Discriminator loss: 0.7067798376083374\n",
      "Epoch 56, Generator loss: 0.6692096590995789, Discriminator loss: 0.7060209512710571\n",
      "Epoch 57, Generator loss: 0.6895493865013123, Discriminator loss: 0.7057089805603027\n",
      "Epoch 58, Generator loss: 0.6868085861206055, Discriminator loss: 0.7007613778114319\n",
      "Epoch 59, Generator loss: 0.6787742972373962, Discriminator loss: 0.7145202159881592\n",
      "Epoch 60, Generator loss: 0.6869456171989441, Discriminator loss: 0.7060220241546631\n",
      "Epoch 61, Generator loss: 0.6722992062568665, Discriminator loss: 0.7062355279922485\n",
      "Epoch 62, Generator loss: 0.6774019598960876, Discriminator loss: 0.7141799926757812\n",
      "Epoch 63, Generator loss: 0.6895074248313904, Discriminator loss: 0.7095121145248413\n",
      "Epoch 64, Generator loss: 0.6874249577522278, Discriminator loss: 0.7032140493392944\n",
      "Epoch 65, Generator loss: 0.6859925985336304, Discriminator loss: 0.7055886387825012\n",
      "Epoch 66, Generator loss: 0.6761895418167114, Discriminator loss: 0.7086735367774963\n",
      "Epoch 67, Generator loss: 0.6780967712402344, Discriminator loss: 0.7093174457550049\n",
      "Epoch 68, Generator loss: 0.6896653175354004, Discriminator loss: 0.7046467065811157\n",
      "Epoch 69, Generator loss: 0.6881862282752991, Discriminator loss: 0.7089747190475464\n",
      "Epoch 70, Generator loss: 0.6799123287200928, Discriminator loss: 0.7079658508300781\n",
      "Epoch 71, Generator loss: 0.6833513975143433, Discriminator loss: 0.7048941850662231\n",
      "Epoch 72, Generator loss: 0.6824538111686707, Discriminator loss: 0.706235945224762\n",
      "Epoch 73, Generator loss: 0.6849421262741089, Discriminator loss: 0.7044070959091187\n",
      "Epoch 74, Generator loss: 0.6902546882629395, Discriminator loss: 0.7001442909240723\n",
      "Epoch 75, Generator loss: 0.6894444823265076, Discriminator loss: 0.7005887627601624\n",
      "Epoch 76, Generator loss: 0.6833683848381042, Discriminator loss: 0.7033818960189819\n",
      "Epoch 77, Generator loss: 0.6737071871757507, Discriminator loss: 0.7077692747116089\n",
      "Epoch 78, Generator loss: 0.6900048851966858, Discriminator loss: 0.6995030045509338\n",
      "Epoch 79, Generator loss: 0.6887068748474121, Discriminator loss: 0.6996225118637085\n",
      "Epoch 80, Generator loss: 0.681904673576355, Discriminator loss: 0.7024388909339905\n",
      "Epoch 81, Generator loss: 0.6869514584541321, Discriminator loss: 0.699063777923584\n",
      "Epoch 82, Generator loss: 0.6873882412910461, Discriminator loss: 0.6989890336990356\n",
      "Epoch 83, Generator loss: 0.6914064288139343, Discriminator loss: 0.6956527233123779\n",
      "Epoch 84, Generator loss: 0.6915898323059082, Discriminator loss: 0.6954171061515808\n",
      "Epoch 85, Generator loss: 0.6966710090637207, Discriminator loss: 0.693700909614563\n",
      "Epoch 86, Generator loss: 0.6946415901184082, Discriminator loss: 0.6926833391189575\n",
      "Epoch 87, Generator loss: 0.6972265243530273, Discriminator loss: 0.6910268664360046\n",
      "Epoch 88, Generator loss: 0.6943440437316895, Discriminator loss: 0.6909525394439697\n",
      "Epoch 89, Generator loss: 0.6973088979721069, Discriminator loss: 0.6890277862548828\n",
      "Epoch 90, Generator loss: 0.704883337020874, Discriminator loss: 0.6872403621673584\n",
      "Epoch 91, Generator loss: 0.6988422870635986, Discriminator loss: 0.6878724694252014\n",
      "Epoch 92, Generator loss: 0.7048069834709167, Discriminator loss: 0.6869657039642334\n",
      "Epoch 93, Generator loss: 0.6982586979866028, Discriminator loss: 0.6872798204421997\n",
      "Epoch 94, Generator loss: 0.7056987881660461, Discriminator loss: 0.6854854226112366\n",
      "Epoch 95, Generator loss: 0.6935697793960571, Discriminator loss: 0.6890114545822144\n",
      "Epoch 96, Generator loss: 0.712337076663971, Discriminator loss: 0.6811122298240662\n",
      "Epoch 97, Generator loss: 0.7028896808624268, Discriminator loss: 0.6832787990570068\n",
      "Epoch 98, Generator loss: 0.7122995853424072, Discriminator loss: 0.6831375360488892\n",
      "Epoch 99, Generator loss: 0.6936600208282471, Discriminator loss: 0.6882528066635132\n",
      "Epoch 100, Generator loss: 0.7038328647613525, Discriminator loss: 0.6854809522628784\n",
      "Epoch 101, Generator loss: 0.6996302008628845, Discriminator loss: 0.6867885589599609\n",
      "Epoch 102, Generator loss: 0.6999250650405884, Discriminator loss: 0.6874080896377563\n",
      "Epoch 103, Generator loss: 0.6981450319290161, Discriminator loss: 0.6876120567321777\n",
      "Epoch 104, Generator loss: 0.6928896307945251, Discriminator loss: 0.6880850195884705\n",
      "Epoch 105, Generator loss: 0.7071822881698608, Discriminator loss: 0.6889951229095459\n",
      "Epoch 106, Generator loss: 0.6933228373527527, Discriminator loss: 0.6902934312820435\n",
      "Epoch 107, Generator loss: 0.700274646282196, Discriminator loss: 0.6891939640045166\n",
      "Epoch 108, Generator loss: 0.7012143135070801, Discriminator loss: 0.6899465918540955\n",
      "Epoch 109, Generator loss: 0.6985250115394592, Discriminator loss: 0.6913286447525024\n",
      "Epoch 110, Generator loss: 0.6917743682861328, Discriminator loss: 0.692609965801239\n",
      "Epoch 111, Generator loss: 0.6905328631401062, Discriminator loss: 0.6932647824287415\n",
      "Epoch 112, Generator loss: 0.6973954439163208, Discriminator loss: 0.6933244466781616\n",
      "Epoch 113, Generator loss: 0.6934499144554138, Discriminator loss: 0.6983537673950195\n",
      "Epoch 114, Generator loss: 0.6946018934249878, Discriminator loss: 0.7003177404403687\n",
      "Epoch 115, Generator loss: 0.6897715330123901, Discriminator loss: 0.6955829858779907\n",
      "Epoch 116, Generator loss: 0.6890872716903687, Discriminator loss: 0.6967771649360657\n",
      "Epoch 117, Generator loss: 0.696201503276825, Discriminator loss: 0.6994091272354126\n",
      "Epoch 118, Generator loss: 0.6923367977142334, Discriminator loss: 0.6972037553787231\n",
      "Epoch 119, Generator loss: 0.696520209312439, Discriminator loss: 0.6988428831100464\n",
      "Epoch 120, Generator loss: 0.6960942149162292, Discriminator loss: 0.6956319212913513\n",
      "Epoch 121, Generator loss: 0.6952630877494812, Discriminator loss: 0.6928083896636963\n",
      "Epoch 122, Generator loss: 0.6997459530830383, Discriminator loss: 0.6926356554031372\n",
      "Epoch 123, Generator loss: 0.6991769075393677, Discriminator loss: 0.6909576654434204\n",
      "Epoch 124, Generator loss: 0.7046968340873718, Discriminator loss: 0.6881714463233948\n",
      "Epoch 125, Generator loss: 0.7026380300521851, Discriminator loss: 0.6872040033340454\n",
      "Epoch 126, Generator loss: 0.7091352939605713, Discriminator loss: 0.682643711566925\n",
      "Epoch 127, Generator loss: 0.7084958553314209, Discriminator loss: 0.6802724599838257\n",
      "Epoch 128, Generator loss: 0.7152712345123291, Discriminator loss: 0.6774265766143799\n",
      "Epoch 129, Generator loss: 0.7039968371391296, Discriminator loss: 0.6794415712356567\n",
      "Epoch 130, Generator loss: 0.7117565274238586, Discriminator loss: 0.6734446287155151\n",
      "Epoch 131, Generator loss: 0.7260216474533081, Discriminator loss: 0.6704796552658081\n",
      "Epoch 132, Generator loss: 0.7169718146324158, Discriminator loss: 0.6732325553894043\n",
      "Epoch 133, Generator loss: 0.7261164784431458, Discriminator loss: 0.6710495948791504\n",
      "Epoch 134, Generator loss: 0.7066237926483154, Discriminator loss: 0.6835805177688599\n",
      "Epoch 135, Generator loss: 0.700163722038269, Discriminator loss: 0.6861870288848877\n",
      "Epoch 136, Generator loss: 0.688247561454773, Discriminator loss: 0.6894527673721313\n",
      "Epoch 137, Generator loss: 0.6931995153427124, Discriminator loss: 0.7078129649162292\n",
      "Epoch 138, Generator loss: 0.6562216281890869, Discriminator loss: 0.7390632629394531\n",
      "Epoch 139, Generator loss: 0.6238566637039185, Discriminator loss: 0.7721642255783081\n",
      "Epoch 140, Generator loss: 0.666758120059967, Discriminator loss: 0.7318466901779175\n",
      "Epoch 141, Generator loss: 0.6288493871688843, Discriminator loss: 0.7637014389038086\n",
      "Epoch 142, Generator loss: 0.6541760563850403, Discriminator loss: 0.7317017316818237\n",
      "Epoch 143, Generator loss: 0.6487300992012024, Discriminator loss: 0.7295752167701721\n",
      "Epoch 144, Generator loss: 0.6528874039649963, Discriminator loss: 0.7263285517692566\n",
      "Epoch 145, Generator loss: 0.6624563336372375, Discriminator loss: 0.7135661840438843\n",
      "Epoch 146, Generator loss: 0.6724063158035278, Discriminator loss: 0.7041622400283813\n",
      "Epoch 147, Generator loss: 0.6868400573730469, Discriminator loss: 0.694753885269165\n",
      "Epoch 148, Generator loss: 0.6870833039283752, Discriminator loss: 0.6918720006942749\n",
      "Epoch 149, Generator loss: 0.6888350248336792, Discriminator loss: 0.6892057061195374\n",
      "Epoch 150, Generator loss: 0.7028000354766846, Discriminator loss: 0.682356059551239\n",
      "Epoch 151, Generator loss: 0.6866809129714966, Discriminator loss: 0.6864575147628784\n",
      "Epoch 152, Generator loss: 0.6821888089179993, Discriminator loss: 0.6840397715568542\n",
      "Epoch 153, Generator loss: 0.7461186051368713, Discriminator loss: 0.6568900346755981\n",
      "Epoch 154, Generator loss: 0.7110235095024109, Discriminator loss: 0.6684286594390869\n",
      "Epoch 155, Generator loss: 0.7005634903907776, Discriminator loss: 0.6639717817306519\n",
      "Epoch 156, Generator loss: 0.8007224798202515, Discriminator loss: 0.6291507482528687\n",
      "Epoch 157, Generator loss: 0.7084682583808899, Discriminator loss: 0.657937228679657\n",
      "Epoch 158, Generator loss: 0.7276836633682251, Discriminator loss: 0.6618005633354187\n",
      "Epoch 159, Generator loss: 0.7264847755432129, Discriminator loss: 0.6582882404327393\n",
      "Epoch 160, Generator loss: 0.6669557690620422, Discriminator loss: 0.6919446587562561\n",
      "Epoch 161, Generator loss: 0.6893295049667358, Discriminator loss: 0.6957156658172607\n",
      "Epoch 162, Generator loss: 0.6672205924987793, Discriminator loss: 0.7150231599807739\n",
      "Epoch 163, Generator loss: 0.688544750213623, Discriminator loss: 0.7160851955413818\n",
      "Epoch 164, Generator loss: 0.6847565174102783, Discriminator loss: 0.7208942174911499\n",
      "Epoch 165, Generator loss: 0.6843511462211609, Discriminator loss: 0.7131096124649048\n",
      "Epoch 166, Generator loss: 0.6842839121818542, Discriminator loss: 0.7072808742523193\n",
      "Epoch 167, Generator loss: 0.7026865482330322, Discriminator loss: 0.6966910362243652\n",
      "Epoch 168, Generator loss: 0.6933290362358093, Discriminator loss: 0.6917091608047485\n",
      "Epoch 169, Generator loss: 0.7154717445373535, Discriminator loss: 0.6818747520446777\n",
      "Epoch 170, Generator loss: 0.7215155363082886, Discriminator loss: 0.6729444265365601\n",
      "Epoch 171, Generator loss: 0.7194507718086243, Discriminator loss: 0.6756985187530518\n",
      "Epoch 172, Generator loss: 0.6935504674911499, Discriminator loss: 0.6845226287841797\n",
      "Epoch 173, Generator loss: 0.7291005849838257, Discriminator loss: 0.6748011112213135\n",
      "Epoch 174, Generator loss: 0.700082004070282, Discriminator loss: 0.687435507774353\n",
      "Epoch 175, Generator loss: 0.719858705997467, Discriminator loss: 0.6881833672523499\n",
      "Epoch 176, Generator loss: 0.7000261545181274, Discriminator loss: 0.7020649909973145\n",
      "Epoch 177, Generator loss: 0.6906973123550415, Discriminator loss: 0.7059286236763\n",
      "Epoch 178, Generator loss: 0.7128546833992004, Discriminator loss: 0.7071510553359985\n",
      "Epoch 179, Generator loss: 0.6962690353393555, Discriminator loss: 0.7135227918624878\n",
      "Epoch 180, Generator loss: 0.6177698373794556, Discriminator loss: 0.7637969851493835\n",
      "Epoch 181, Generator loss: 0.7332949638366699, Discriminator loss: 0.7040417194366455\n",
      "Epoch 182, Generator loss: 0.6497882008552551, Discriminator loss: 0.7401312589645386\n",
      "Epoch 183, Generator loss: 0.6921026110649109, Discriminator loss: 0.7183158993721008\n",
      "Epoch 184, Generator loss: 0.6241282224655151, Discriminator loss: 0.7448335289955139\n",
      "Epoch 185, Generator loss: 0.6910008788108826, Discriminator loss: 0.709574818611145\n",
      "Epoch 186, Generator loss: 0.6585836410522461, Discriminator loss: 0.7252349257469177\n",
      "Epoch 187, Generator loss: 0.6806862354278564, Discriminator loss: 0.7101062536239624\n",
      "Epoch 188, Generator loss: 0.6754632592201233, Discriminator loss: 0.7104853391647339\n",
      "Epoch 189, Generator loss: 0.683542013168335, Discriminator loss: 0.7035824060440063\n",
      "Epoch 190, Generator loss: 0.694427490234375, Discriminator loss: 0.7001594305038452\n",
      "Epoch 191, Generator loss: 0.7015780210494995, Discriminator loss: 0.696110725402832\n",
      "Epoch 192, Generator loss: 0.6965687274932861, Discriminator loss: 0.6962521076202393\n",
      "Epoch 193, Generator loss: 0.6936903595924377, Discriminator loss: 0.6985244750976562\n",
      "Epoch 194, Generator loss: 0.693929135799408, Discriminator loss: 0.6956214904785156\n",
      "Epoch 195, Generator loss: 0.6955599188804626, Discriminator loss: 0.6950700879096985\n",
      "Epoch 196, Generator loss: 0.6900132894515991, Discriminator loss: 0.6976369023323059\n",
      "Epoch 197, Generator loss: 0.6987552046775818, Discriminator loss: 0.693637490272522\n",
      "Epoch 198, Generator loss: 0.6912510395050049, Discriminator loss: 0.6955289840698242\n",
      "Epoch 199, Generator loss: 0.6914902329444885, Discriminator loss: 0.6966007351875305\n",
      "Epoch 200, Generator loss: 0.6918200850486755, Discriminator loss: 0.6964247226715088\n",
      "Epoch 201, Generator loss: 0.6935237050056458, Discriminator loss: 0.6944345235824585\n",
      "Epoch 202, Generator loss: 0.6908563375473022, Discriminator loss: 0.6955848932266235\n",
      "Epoch 203, Generator loss: 0.6911375522613525, Discriminator loss: 0.6952619552612305\n",
      "Epoch 204, Generator loss: 0.6952497363090515, Discriminator loss: 0.6930586099624634\n",
      "Epoch 205, Generator loss: 0.6920059323310852, Discriminator loss: 0.6937498450279236\n",
      "Epoch 206, Generator loss: 0.6936160326004028, Discriminator loss: 0.6927653551101685\n",
      "Epoch 207, Generator loss: 0.6954507231712341, Discriminator loss: 0.6921197175979614\n",
      "Epoch 208, Generator loss: 0.693910539150238, Discriminator loss: 0.6919584274291992\n",
      "Epoch 209, Generator loss: 0.6976558566093445, Discriminator loss: 0.6902462244033813\n",
      "Epoch 210, Generator loss: 0.6972572207450867, Discriminator loss: 0.69025057554245\n",
      "Epoch 211, Generator loss: 0.6962302327156067, Discriminator loss: 0.6906691789627075\n",
      "Epoch 212, Generator loss: 0.6967980861663818, Discriminator loss: 0.6894516944885254\n",
      "Epoch 213, Generator loss: 0.6955168843269348, Discriminator loss: 0.690717875957489\n",
      "Epoch 214, Generator loss: 0.697995126247406, Discriminator loss: 0.6889716386795044\n",
      "Epoch 215, Generator loss: 0.6982725262641907, Discriminator loss: 0.6879059076309204\n",
      "Epoch 216, Generator loss: 0.6987289190292358, Discriminator loss: 0.687292754650116\n",
      "Epoch 217, Generator loss: 0.6945890188217163, Discriminator loss: 0.6879453659057617\n",
      "Epoch 218, Generator loss: 0.6963988542556763, Discriminator loss: 0.6878188252449036\n",
      "Epoch 219, Generator loss: 0.7000546455383301, Discriminator loss: 0.6866425275802612\n",
      "Epoch 220, Generator loss: 0.6984197497367859, Discriminator loss: 0.6856424808502197\n",
      "Epoch 221, Generator loss: 0.704230785369873, Discriminator loss: 0.6815946102142334\n",
      "Epoch 222, Generator loss: 0.715385377407074, Discriminator loss: 0.6749627590179443\n",
      "Epoch 223, Generator loss: 0.710506021976471, Discriminator loss: 0.678693413734436\n",
      "Epoch 224, Generator loss: 0.6846819519996643, Discriminator loss: 0.6880386471748352\n",
      "Epoch 225, Generator loss: 0.724144697189331, Discriminator loss: 0.6711912155151367\n",
      "Epoch 226, Generator loss: 0.7151218056678772, Discriminator loss: 0.6748546957969666\n",
      "Epoch 227, Generator loss: 0.6913264989852905, Discriminator loss: 0.6798890829086304\n",
      "Epoch 228, Generator loss: 0.7214624285697937, Discriminator loss: 0.6698800921440125\n",
      "Epoch 229, Generator loss: 0.7033830881118774, Discriminator loss: 0.6749145984649658\n",
      "Epoch 230, Generator loss: 0.7055765986442566, Discriminator loss: 0.6747469305992126\n",
      "Epoch 231, Generator loss: 0.7100303173065186, Discriminator loss: 0.679181694984436\n",
      "Epoch 232, Generator loss: 0.7086985111236572, Discriminator loss: 0.6807155609130859\n",
      "Epoch 233, Generator loss: 0.7133340239524841, Discriminator loss: 0.6895397305488586\n",
      "Epoch 234, Generator loss: 0.6891623139381409, Discriminator loss: 0.7086808681488037\n",
      "Epoch 235, Generator loss: 0.5802962779998779, Discriminator loss: 0.7771409749984741\n",
      "Epoch 236, Generator loss: 0.641482949256897, Discriminator loss: 0.7509039640426636\n",
      "Epoch 237, Generator loss: 0.6569384336471558, Discriminator loss: 0.7384405136108398\n",
      "Epoch 238, Generator loss: 0.6682652831077576, Discriminator loss: 0.7157125473022461\n",
      "Epoch 239, Generator loss: 0.6005799770355225, Discriminator loss: 0.7573844194412231\n",
      "Epoch 240, Generator loss: 0.6328781843185425, Discriminator loss: 0.74092698097229\n",
      "Epoch 241, Generator loss: 0.6326755881309509, Discriminator loss: 0.73589688539505\n",
      "Epoch 242, Generator loss: 0.6859012842178345, Discriminator loss: 0.707047700881958\n",
      "Epoch 243, Generator loss: 0.6765568256378174, Discriminator loss: 0.7092770338058472\n",
      "Epoch 244, Generator loss: 0.6924633979797363, Discriminator loss: 0.6991124153137207\n",
      "Epoch 245, Generator loss: 0.6811522245407104, Discriminator loss: 0.7032257318496704\n",
      "Epoch 246, Generator loss: 0.6821539998054504, Discriminator loss: 0.7027773857116699\n",
      "Epoch 247, Generator loss: 0.685009777545929, Discriminator loss: 0.7005323767662048\n",
      "Epoch 248, Generator loss: 0.6811376214027405, Discriminator loss: 0.7022200226783752\n",
      "Epoch 249, Generator loss: 0.6866905689239502, Discriminator loss: 0.7001931667327881\n",
      "Epoch 250, Generator loss: 0.6858030557632446, Discriminator loss: 0.6990388035774231\n",
      "Epoch 251, Generator loss: 0.6860092878341675, Discriminator loss: 0.7000857591629028\n",
      "Epoch 252, Generator loss: 0.688639223575592, Discriminator loss: 0.6991087794303894\n",
      "Epoch 253, Generator loss: 0.6865097284317017, Discriminator loss: 0.6976920962333679\n",
      "Epoch 254, Generator loss: 0.6856773495674133, Discriminator loss: 0.6975672841072083\n",
      "Epoch 255, Generator loss: 0.6902050375938416, Discriminator loss: 0.6963367462158203\n",
      "Epoch 256, Generator loss: 0.6824743151664734, Discriminator loss: 0.6992529630661011\n",
      "Epoch 257, Generator loss: 0.6899096965789795, Discriminator loss: 0.6954590082168579\n",
      "Epoch 258, Generator loss: 0.6889124512672424, Discriminator loss: 0.6954332590103149\n",
      "Epoch 259, Generator loss: 0.6904903054237366, Discriminator loss: 0.6949163675308228\n",
      "Epoch 260, Generator loss: 0.6902519464492798, Discriminator loss: 0.6943073272705078\n",
      "Epoch 261, Generator loss: 0.6896124482154846, Discriminator loss: 0.6942607760429382\n",
      "Epoch 262, Generator loss: 0.6915340423583984, Discriminator loss: 0.693654477596283\n",
      "Epoch 263, Generator loss: 0.6921307444572449, Discriminator loss: 0.6929076313972473\n",
      "Epoch 264, Generator loss: 0.6906757950782776, Discriminator loss: 0.6930941939353943\n",
      "Epoch 265, Generator loss: 0.6926038265228271, Discriminator loss: 0.6926324963569641\n",
      "Epoch 266, Generator loss: 0.6930762529373169, Discriminator loss: 0.6912534236907959\n",
      "Epoch 267, Generator loss: 0.6915640234947205, Discriminator loss: 0.6918978095054626\n",
      "Epoch 268, Generator loss: 0.6924232244491577, Discriminator loss: 0.6912500262260437\n",
      "Epoch 269, Generator loss: 0.6910482048988342, Discriminator loss: 0.6917755603790283\n",
      "Epoch 270, Generator loss: 0.6977397799491882, Discriminator loss: 0.6889045238494873\n",
      "Epoch 271, Generator loss: 0.7011322379112244, Discriminator loss: 0.6872276067733765\n",
      "Epoch 272, Generator loss: 0.6930153369903564, Discriminator loss: 0.6898998618125916\n",
      "Epoch 273, Generator loss: 0.6884719133377075, Discriminator loss: 0.6913135051727295\n",
      "Epoch 274, Generator loss: 0.6985416412353516, Discriminator loss: 0.6881102323532104\n",
      "Epoch 275, Generator loss: 0.6975898742675781, Discriminator loss: 0.6865646839141846\n",
      "Epoch 276, Generator loss: 0.7088101506233215, Discriminator loss: 0.6810798645019531\n",
      "Epoch 277, Generator loss: 0.6941742300987244, Discriminator loss: 0.6858668923377991\n",
      "Epoch 278, Generator loss: 0.7027564644813538, Discriminator loss: 0.6831914186477661\n",
      "Epoch 279, Generator loss: 0.6909066438674927, Discriminator loss: 0.6865134239196777\n",
      "Epoch 280, Generator loss: 0.7054505944252014, Discriminator loss: 0.6800123453140259\n",
      "Epoch 281, Generator loss: 0.7063338160514832, Discriminator loss: 0.6772527098655701\n",
      "Epoch 282, Generator loss: 0.6872872710227966, Discriminator loss: 0.6833724975585938\n",
      "Epoch 283, Generator loss: 0.7382568717002869, Discriminator loss: 0.663616418838501\n",
      "Epoch 284, Generator loss: 0.7144607305526733, Discriminator loss: 0.6697638630867004\n",
      "Epoch 285, Generator loss: 0.6716251373291016, Discriminator loss: 0.6906797885894775\n",
      "Epoch 286, Generator loss: 0.7013113498687744, Discriminator loss: 0.6778294444084167\n",
      "Epoch 287, Generator loss: 0.7521748542785645, Discriminator loss: 0.649377167224884\n",
      "Epoch 288, Generator loss: 0.7518503665924072, Discriminator loss: 0.6387042999267578\n",
      "Epoch 289, Generator loss: 0.8639806509017944, Discriminator loss: 0.6019662618637085\n",
      "Epoch 290, Generator loss: 0.6391095519065857, Discriminator loss: 0.6849808692932129\n",
      "Epoch 291, Generator loss: 1.0094467401504517, Discriminator loss: 0.5695862770080566\n",
      "Epoch 292, Generator loss: 0.6541823744773865, Discriminator loss: 0.6726275682449341\n",
      "Epoch 293, Generator loss: 0.7931198477745056, Discriminator loss: 0.6622415781021118\n",
      "Epoch 294, Generator loss: 0.6769436597824097, Discriminator loss: 0.7276357412338257\n",
      "Epoch 295, Generator loss: 0.6982465982437134, Discriminator loss: 0.7613011002540588\n",
      "Epoch 296, Generator loss: 0.6118840575218201, Discriminator loss: 0.7813979387283325\n",
      "Epoch 297, Generator loss: 0.6518045663833618, Discriminator loss: 0.7646646499633789\n",
      "Epoch 298, Generator loss: 0.6297467350959778, Discriminator loss: 0.7557016611099243\n",
      "Epoch 299, Generator loss: 0.6152930855751038, Discriminator loss: 0.750716507434845\n",
      "Epoch 300, Generator loss: 0.6499829888343811, Discriminator loss: 0.7292998433113098\n",
      "Epoch 301, Generator loss: 0.6926064491271973, Discriminator loss: 0.7089706659317017\n",
      "Epoch 302, Generator loss: 0.685567319393158, Discriminator loss: 0.704826831817627\n",
      "Epoch 303, Generator loss: 0.6702826619148254, Discriminator loss: 0.712824821472168\n",
      "Epoch 304, Generator loss: 0.680954098701477, Discriminator loss: 0.7077643871307373\n",
      "Epoch 305, Generator loss: 0.6809576749801636, Discriminator loss: 0.7051547765731812\n",
      "Epoch 306, Generator loss: 0.6807278990745544, Discriminator loss: 0.7065449357032776\n",
      "Epoch 307, Generator loss: 0.6856107711791992, Discriminator loss: 0.7021219730377197\n",
      "Epoch 308, Generator loss: 0.6970885396003723, Discriminator loss: 0.6960150003433228\n",
      "Epoch 309, Generator loss: 0.6925514340400696, Discriminator loss: 0.6991466283798218\n",
      "Epoch 310, Generator loss: 0.692205011844635, Discriminator loss: 0.6984508037567139\n",
      "Epoch 311, Generator loss: 0.6857963800430298, Discriminator loss: 0.7006058096885681\n",
      "Epoch 312, Generator loss: 0.6819171905517578, Discriminator loss: 0.7029784917831421\n",
      "Epoch 313, Generator loss: 0.6933266520500183, Discriminator loss: 0.6973203420639038\n",
      "Epoch 314, Generator loss: 0.6885204315185547, Discriminator loss: 0.6984381079673767\n",
      "Epoch 315, Generator loss: 0.6933633685112, Discriminator loss: 0.6970624923706055\n",
      "Epoch 316, Generator loss: 0.6888411045074463, Discriminator loss: 0.6980859041213989\n",
      "Epoch 317, Generator loss: 0.6860529780387878, Discriminator loss: 0.6990237236022949\n",
      "Epoch 318, Generator loss: 0.6876269578933716, Discriminator loss: 0.6986924409866333\n",
      "Epoch 319, Generator loss: 0.6869232654571533, Discriminator loss: 0.6991525292396545\n",
      "Epoch 320, Generator loss: 0.6918356418609619, Discriminator loss: 0.6964767575263977\n",
      "Epoch 321, Generator loss: 0.6941716074943542, Discriminator loss: 0.6950058341026306\n",
      "Epoch 322, Generator loss: 0.685120701789856, Discriminator loss: 0.69854736328125\n",
      "Epoch 323, Generator loss: 0.6916149854660034, Discriminator loss: 0.6958122253417969\n",
      "Epoch 324, Generator loss: 0.6929817199707031, Discriminator loss: 0.6952005624771118\n",
      "Epoch 325, Generator loss: 0.68569016456604, Discriminator loss: 0.6980335712432861\n",
      "Epoch 326, Generator loss: 0.6869229078292847, Discriminator loss: 0.6976826190948486\n",
      "Epoch 327, Generator loss: 0.692196249961853, Discriminator loss: 0.6952054500579834\n",
      "Epoch 328, Generator loss: 0.6925117373466492, Discriminator loss: 0.6948201656341553\n",
      "Epoch 329, Generator loss: 0.6929959058761597, Discriminator loss: 0.6941291093826294\n",
      "Epoch 330, Generator loss: 0.692392885684967, Discriminator loss: 0.6943742036819458\n",
      "Epoch 331, Generator loss: 0.6932312250137329, Discriminator loss: 0.6941171884536743\n",
      "Epoch 332, Generator loss: 0.6936901211738586, Discriminator loss: 0.6937669515609741\n",
      "Epoch 333, Generator loss: 0.6918970942497253, Discriminator loss: 0.6941052079200745\n",
      "Epoch 334, Generator loss: 0.6936649084091187, Discriminator loss: 0.6936203837394714\n",
      "Epoch 335, Generator loss: 0.6931353807449341, Discriminator loss: 0.6935068368911743\n",
      "Epoch 336, Generator loss: 0.69259113073349, Discriminator loss: 0.6935465335845947\n",
      "Epoch 337, Generator loss: 0.6943272948265076, Discriminator loss: 0.693259060382843\n",
      "Epoch 338, Generator loss: 0.6933644413948059, Discriminator loss: 0.6931394934654236\n",
      "Epoch 339, Generator loss: 0.6928651928901672, Discriminator loss: 0.6930397748947144\n",
      "Epoch 340, Generator loss: 0.6944854855537415, Discriminator loss: 0.6926048994064331\n",
      "Epoch 341, Generator loss: 0.6943801045417786, Discriminator loss: 0.6925159692764282\n",
      "Epoch 342, Generator loss: 0.6957048773765564, Discriminator loss: 0.6918103694915771\n",
      "Epoch 343, Generator loss: 0.696022629737854, Discriminator loss: 0.6912896633148193\n",
      "Epoch 344, Generator loss: 0.6936371326446533, Discriminator loss: 0.6921541690826416\n",
      "Epoch 345, Generator loss: 0.694968581199646, Discriminator loss: 0.6915203332901001\n",
      "Epoch 346, Generator loss: 0.6924327611923218, Discriminator loss: 0.692418098449707\n",
      "Epoch 347, Generator loss: 0.696812629699707, Discriminator loss: 0.6901717185974121\n",
      "Epoch 348, Generator loss: 0.6966748833656311, Discriminator loss: 0.6905921697616577\n",
      "Epoch 349, Generator loss: 0.6983696222305298, Discriminator loss: 0.6899791955947876\n",
      "Epoch 350, Generator loss: 0.6958852410316467, Discriminator loss: 0.6905987858772278\n",
      "Epoch 351, Generator loss: 0.6972737908363342, Discriminator loss: 0.6892932653427124\n",
      "Epoch 352, Generator loss: 0.6953001618385315, Discriminator loss: 0.6910743117332458\n",
      "Epoch 353, Generator loss: 0.6963467597961426, Discriminator loss: 0.6894257664680481\n",
      "Epoch 354, Generator loss: 0.698037326335907, Discriminator loss: 0.688994288444519\n",
      "Epoch 355, Generator loss: 0.691715657711029, Discriminator loss: 0.69168621301651\n",
      "Epoch 356, Generator loss: 0.6944853663444519, Discriminator loss: 0.689797043800354\n",
      "Epoch 357, Generator loss: 0.6986345648765564, Discriminator loss: 0.6880369186401367\n",
      "Epoch 358, Generator loss: 0.6996486783027649, Discriminator loss: 0.6868685483932495\n",
      "Epoch 359, Generator loss: 0.6935074925422668, Discriminator loss: 0.6894187331199646\n",
      "Epoch 360, Generator loss: 0.700217068195343, Discriminator loss: 0.6861732006072998\n",
      "Epoch 361, Generator loss: 0.7004721760749817, Discriminator loss: 0.6854336261749268\n",
      "Epoch 362, Generator loss: 0.6957964301109314, Discriminator loss: 0.6886737942695618\n",
      "Epoch 363, Generator loss: 0.6980088353157043, Discriminator loss: 0.6866610050201416\n",
      "Epoch 364, Generator loss: 0.6960449814796448, Discriminator loss: 0.687267541885376\n",
      "Epoch 365, Generator loss: 0.7024057507514954, Discriminator loss: 0.6866241693496704\n",
      "Epoch 366, Generator loss: 0.7021465301513672, Discriminator loss: 0.6817864179611206\n",
      "Epoch 367, Generator loss: 0.690547525882721, Discriminator loss: 0.6880010962486267\n",
      "Epoch 368, Generator loss: 0.722199559211731, Discriminator loss: 0.6752826571464539\n",
      "Epoch 369, Generator loss: 0.697246253490448, Discriminator loss: 0.6823769807815552\n",
      "Epoch 370, Generator loss: 0.7338789105415344, Discriminator loss: 0.6688185930252075\n",
      "Epoch 371, Generator loss: 0.7160693407058716, Discriminator loss: 0.6717453002929688\n",
      "Epoch 372, Generator loss: 0.7353569269180298, Discriminator loss: 0.6605904698371887\n",
      "Epoch 373, Generator loss: 0.6907063722610474, Discriminator loss: 0.6833741664886475\n",
      "Epoch 374, Generator loss: 0.7106215953826904, Discriminator loss: 0.6742041707038879\n",
      "Epoch 375, Generator loss: 0.723007321357727, Discriminator loss: 0.6644719839096069\n",
      "Epoch 376, Generator loss: 0.6981827616691589, Discriminator loss: 0.6754828691482544\n",
      "Epoch 377, Generator loss: 0.7767109274864197, Discriminator loss: 0.6393046379089355\n",
      "Epoch 378, Generator loss: 0.6989230513572693, Discriminator loss: 0.6718828678131104\n",
      "Epoch 379, Generator loss: 0.7794219255447388, Discriminator loss: 0.6277356743812561\n",
      "Epoch 380, Generator loss: 0.7525531649589539, Discriminator loss: 0.6559122800827026\n",
      "Epoch 381, Generator loss: 0.6993629336357117, Discriminator loss: 0.6653885841369629\n",
      "Epoch 382, Generator loss: 0.5742385983467102, Discriminator loss: 0.8129094839096069\n",
      "Epoch 383, Generator loss: 0.7061951160430908, Discriminator loss: 0.7817822694778442\n",
      "Epoch 384, Generator loss: 0.6250499486923218, Discriminator loss: 0.7466199398040771\n",
      "Epoch 385, Generator loss: 0.6150742173194885, Discriminator loss: 0.7667742967605591\n",
      "Epoch 386, Generator loss: 0.6916836500167847, Discriminator loss: 0.7255443930625916\n",
      "Epoch 387, Generator loss: 0.667018473148346, Discriminator loss: 0.7185208797454834\n",
      "Epoch 388, Generator loss: 0.6871174573898315, Discriminator loss: 0.7120965719223022\n",
      "Epoch 389, Generator loss: 0.673059344291687, Discriminator loss: 0.7131640911102295\n",
      "Epoch 390, Generator loss: 0.6720571517944336, Discriminator loss: 0.7116612195968628\n",
      "Epoch 391, Generator loss: 0.6470904350280762, Discriminator loss: 0.7266114950180054\n",
      "Epoch 392, Generator loss: 0.6748299598693848, Discriminator loss: 0.7098696231842041\n",
      "Epoch 393, Generator loss: 0.6826391220092773, Discriminator loss: 0.7024498581886292\n",
      "Epoch 394, Generator loss: 0.675597608089447, Discriminator loss: 0.7084046602249146\n",
      "Epoch 395, Generator loss: 0.6759856343269348, Discriminator loss: 0.7084873914718628\n",
      "Epoch 396, Generator loss: 0.6804895401000977, Discriminator loss: 0.7031814455986023\n",
      "Epoch 397, Generator loss: 0.6836655735969543, Discriminator loss: 0.7010749578475952\n",
      "Epoch 398, Generator loss: 0.6864384412765503, Discriminator loss: 0.7002990245819092\n",
      "Epoch 399, Generator loss: 0.6876137852668762, Discriminator loss: 0.6983906030654907\n",
      "Epoch 400, Generator loss: 0.6876197457313538, Discriminator loss: 0.6993866562843323\n",
      "Epoch 401, Generator loss: 0.6854487657546997, Discriminator loss: 0.6992719173431396\n",
      "Epoch 402, Generator loss: 0.6888360977172852, Discriminator loss: 0.6975502967834473\n",
      "Epoch 403, Generator loss: 0.6888188719749451, Discriminator loss: 0.6976680755615234\n",
      "Epoch 404, Generator loss: 0.6920233368873596, Discriminator loss: 0.6960592269897461\n",
      "Epoch 405, Generator loss: 0.6894424557685852, Discriminator loss: 0.6966873407363892\n",
      "Epoch 406, Generator loss: 0.6940692067146301, Discriminator loss: 0.6948409676551819\n",
      "Epoch 407, Generator loss: 0.6907798647880554, Discriminator loss: 0.6956110000610352\n",
      "Epoch 408, Generator loss: 0.6918166279792786, Discriminator loss: 0.6952528953552246\n",
      "Epoch 409, Generator loss: 0.6915228366851807, Discriminator loss: 0.6950119733810425\n",
      "Epoch 410, Generator loss: 0.6899223923683167, Discriminator loss: 0.6953455209732056\n",
      "Epoch 411, Generator loss: 0.6922783851623535, Discriminator loss: 0.6942624449729919\n",
      "Epoch 412, Generator loss: 0.6888850331306458, Discriminator loss: 0.6954085826873779\n",
      "Epoch 413, Generator loss: 0.6907270550727844, Discriminator loss: 0.6944741010665894\n",
      "Epoch 414, Generator loss: 0.6904740333557129, Discriminator loss: 0.6946709156036377\n",
      "Epoch 415, Generator loss: 0.6933050155639648, Discriminator loss: 0.6934971213340759\n",
      "Epoch 416, Generator loss: 0.6916339993476868, Discriminator loss: 0.6938031911849976\n",
      "Epoch 417, Generator loss: 0.6916654109954834, Discriminator loss: 0.6936283111572266\n",
      "Epoch 418, Generator loss: 0.6924722194671631, Discriminator loss: 0.6931635141372681\n",
      "Epoch 419, Generator loss: 0.6926990747451782, Discriminator loss: 0.6928748488426208\n",
      "Epoch 420, Generator loss: 0.6926069259643555, Discriminator loss: 0.6928587555885315\n",
      "Epoch 421, Generator loss: 0.6926302909851074, Discriminator loss: 0.6923806667327881\n",
      "Epoch 422, Generator loss: 0.6934561133384705, Discriminator loss: 0.6920499801635742\n",
      "Epoch 423, Generator loss: 0.6926349401473999, Discriminator loss: 0.6923588514328003\n",
      "Epoch 424, Generator loss: 0.6933255791664124, Discriminator loss: 0.6919394731521606\n",
      "Epoch 425, Generator loss: 0.6925351023674011, Discriminator loss: 0.6918940544128418\n",
      "Epoch 426, Generator loss: 0.6958276629447937, Discriminator loss: 0.6898415088653564\n",
      "Epoch 427, Generator loss: 0.6948851346969604, Discriminator loss: 0.6904774904251099\n",
      "Epoch 428, Generator loss: 0.693922221660614, Discriminator loss: 0.6909611821174622\n",
      "Epoch 429, Generator loss: 0.6958084106445312, Discriminator loss: 0.6900412440299988\n",
      "Epoch 430, Generator loss: 0.6952923536300659, Discriminator loss: 0.6904315948486328\n",
      "Epoch 431, Generator loss: 0.694923460483551, Discriminator loss: 0.6897462606430054\n",
      "Epoch 432, Generator loss: 0.6993253231048584, Discriminator loss: 0.6876978874206543\n",
      "Epoch 433, Generator loss: 0.6981364488601685, Discriminator loss: 0.6882422566413879\n",
      "Epoch 434, Generator loss: 0.6906870007514954, Discriminator loss: 0.6898552179336548\n",
      "Epoch 435, Generator loss: 0.6955580115318298, Discriminator loss: 0.6880553960800171\n",
      "Epoch 436, Generator loss: 0.6991108059883118, Discriminator loss: 0.6865476369857788\n",
      "Epoch 437, Generator loss: 0.6996060013771057, Discriminator loss: 0.6854596138000488\n",
      "Epoch 438, Generator loss: 0.704216718673706, Discriminator loss: 0.6829769015312195\n",
      "Epoch 439, Generator loss: 0.6988033652305603, Discriminator loss: 0.6860707998275757\n",
      "Epoch 440, Generator loss: 0.7097203135490417, Discriminator loss: 0.679511308670044\n",
      "Epoch 441, Generator loss: 0.7049065232276917, Discriminator loss: 0.6811928749084473\n",
      "Epoch 442, Generator loss: 0.7017061710357666, Discriminator loss: 0.6825946569442749\n",
      "Epoch 443, Generator loss: 0.6874904036521912, Discriminator loss: 0.6890108585357666\n",
      "Epoch 444, Generator loss: 0.7038509845733643, Discriminator loss: 0.6818572282791138\n",
      "Epoch 445, Generator loss: 0.7121596336364746, Discriminator loss: 0.6752642393112183\n",
      "Epoch 446, Generator loss: 0.6858593821525574, Discriminator loss: 0.6836487054824829\n",
      "Epoch 447, Generator loss: 0.7059031128883362, Discriminator loss: 0.6754474639892578\n",
      "Epoch 448, Generator loss: 0.690626323223114, Discriminator loss: 0.6826184988021851\n",
      "Epoch 449, Generator loss: 0.7213424444198608, Discriminator loss: 0.6660516262054443\n",
      "Epoch 450, Generator loss: 0.7022462487220764, Discriminator loss: 0.6595298051834106\n",
      "Epoch 451, Generator loss: 0.7742682695388794, Discriminator loss: 0.6500898003578186\n",
      "Epoch 452, Generator loss: 0.6941676139831543, Discriminator loss: 0.6664865612983704\n",
      "Epoch 453, Generator loss: 0.7670038938522339, Discriminator loss: 0.6577431559562683\n",
      "Epoch 454, Generator loss: 0.6969301700592041, Discriminator loss: 0.6490034461021423\n",
      "Epoch 455, Generator loss: 0.7392734885215759, Discriminator loss: 0.6550472378730774\n",
      "Epoch 456, Generator loss: 0.6948592662811279, Discriminator loss: 0.6835383176803589\n",
      "Epoch 457, Generator loss: 0.659152090549469, Discriminator loss: 0.7398281097412109\n",
      "Epoch 458, Generator loss: 0.7877963781356812, Discriminator loss: 0.7594923973083496\n",
      "Epoch 459, Generator loss: 0.5872358679771423, Discriminator loss: 0.7832971811294556\n",
      "Epoch 460, Generator loss: 0.671369731426239, Discriminator loss: 0.7425422668457031\n",
      "Epoch 461, Generator loss: 0.6487731337547302, Discriminator loss: 0.7464141845703125\n",
      "Epoch 462, Generator loss: 0.6862642765045166, Discriminator loss: 0.715857982635498\n",
      "Epoch 463, Generator loss: 0.6788397431373596, Discriminator loss: 0.7094289064407349\n",
      "Epoch 464, Generator loss: 0.6905469298362732, Discriminator loss: 0.7072858214378357\n",
      "Epoch 465, Generator loss: 0.6788076758384705, Discriminator loss: 0.7097126841545105\n",
      "Epoch 466, Generator loss: 0.6685277223587036, Discriminator loss: 0.7159979939460754\n",
      "Epoch 467, Generator loss: 0.6839399933815002, Discriminator loss: 0.7046988010406494\n",
      "Epoch 468, Generator loss: 0.6858705878257751, Discriminator loss: 0.7038688659667969\n",
      "Epoch 469, Generator loss: 0.6917334198951721, Discriminator loss: 0.7006042003631592\n",
      "Epoch 470, Generator loss: 0.6834607124328613, Discriminator loss: 0.7037172317504883\n",
      "Epoch 471, Generator loss: 0.6885625720024109, Discriminator loss: 0.6999849081039429\n",
      "Epoch 472, Generator loss: 0.6929728984832764, Discriminator loss: 0.6966114044189453\n",
      "Epoch 473, Generator loss: 0.6782671809196472, Discriminator loss: 0.705233097076416\n",
      "Epoch 474, Generator loss: 0.6954538822174072, Discriminator loss: 0.6954036951065063\n",
      "Epoch 475, Generator loss: 0.68865567445755, Discriminator loss: 0.6971356272697449\n",
      "Epoch 476, Generator loss: 0.6921817064285278, Discriminator loss: 0.6967819929122925\n",
      "Epoch 477, Generator loss: 0.6917993426322937, Discriminator loss: 0.6967558860778809\n",
      "Epoch 478, Generator loss: 0.6906766295433044, Discriminator loss: 0.6967248320579529\n",
      "Epoch 479, Generator loss: 0.6916645765304565, Discriminator loss: 0.6955993175506592\n",
      "Epoch 480, Generator loss: 0.6926248073577881, Discriminator loss: 0.6951369643211365\n",
      "Epoch 481, Generator loss: 0.6927486062049866, Discriminator loss: 0.6946194171905518\n",
      "Epoch 482, Generator loss: 0.6933438181877136, Discriminator loss: 0.694087028503418\n",
      "Epoch 483, Generator loss: 0.6935262084007263, Discriminator loss: 0.6938920617103577\n",
      "Epoch 484, Generator loss: 0.6928408741950989, Discriminator loss: 0.6939095854759216\n",
      "Epoch 485, Generator loss: 0.6931110620498657, Discriminator loss: 0.6934888362884521\n",
      "Epoch 486, Generator loss: 0.6934697031974792, Discriminator loss: 0.6931625008583069\n",
      "Epoch 487, Generator loss: 0.6948401927947998, Discriminator loss: 0.6925158500671387\n",
      "Epoch 488, Generator loss: 0.6955016255378723, Discriminator loss: 0.6920562386512756\n",
      "Epoch 489, Generator loss: 0.6945461630821228, Discriminator loss: 0.6922913789749146\n",
      "Epoch 490, Generator loss: 0.694701611995697, Discriminator loss: 0.6917418241500854\n",
      "Epoch 491, Generator loss: 0.6979813575744629, Discriminator loss: 0.6901522874832153\n",
      "Epoch 492, Generator loss: 0.6952152252197266, Discriminator loss: 0.6912927627563477\n",
      "Epoch 493, Generator loss: 0.6950108408927917, Discriminator loss: 0.6906757950782776\n",
      "Epoch 494, Generator loss: 0.6970838308334351, Discriminator loss: 0.6899872422218323\n",
      "Epoch 495, Generator loss: 0.6943997740745544, Discriminator loss: 0.6905901432037354\n",
      "Epoch 496, Generator loss: 0.6995299458503723, Discriminator loss: 0.6883665323257446\n",
      "Epoch 497, Generator loss: 0.7005327343940735, Discriminator loss: 0.6866147518157959\n",
      "Epoch 498, Generator loss: 0.6962267160415649, Discriminator loss: 0.688828706741333\n",
      "Epoch 499, Generator loss: 0.6937621235847473, Discriminator loss: 0.6905216574668884\n"
     ]
    }
   ],
   "source": [
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "for epoch in range(500):\n",
    "    for _, (signal_tensor, params_tensor) in enumerate(train_loader):\n",
    "        z = torch.randn(1, num_latent_variables, 1).to(device)\n",
    "        g_loss = gan.train_generator(signal_tensor, z)\n",
    "        d_loss = gan.train_discriminator(signal_tensor, params_tensor, z)\n",
    "        g_loss_list.append(g_loss)\n",
    "        d_loss_list.append(d_loss)\n",
    "    print(f\"Epoch {epoch}, Generator loss: {g_loss}, Discriminator loss: {d_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.919144588534241, 0.24315132286719604, 2.431613006718278]\n",
      "[-209.3833   453.31693  563.98047]\n"
     ]
    }
   ],
   "source": [
    "generator = gan.generator\n",
    "generator.eval()\n",
    "\n",
    "TS = Signal_Generator(num_sources=1, noise_amplitude=1)\n",
    "test_data = TS.generating_signal()\n",
    "params = TS.printing_parameters()\n",
    "\n",
    "input_signal = test_data['Signal'].values\n",
    "input_signal_tensor = torch.tensor(input_signal, dtype=torch.float).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_params = generator(input_signal_tensor, z).squeeze().cpu().numpy()\n",
    "\n",
    "print(params)\n",
    "print(generated_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN():\n",
    "    def __init__(self, dataset, num_latent_variables, lr, weight_clip):\n",
    "        self.dataset = dataset\n",
    "        self.num_latent_variables = num_latent_variables\n",
    "        self.lr = lr\n",
    "        self.weight_clip = weight_clip\n",
    "\n",
    "        # Networks\n",
    "        self.generator = Generator(in_channels=1, num_latent_variables=num_latent_variables, length=len(signal), num_parameters=len(params)).to(device)\n",
    "        self.discriminator = Discriminator(input_channels=1, length=len(signal), num_parameters=len(params)).to(device)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_g = optim.Adam(self.generator.parameters(), lr=self.lr)\n",
    "        self.optimizer_d = optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "\n",
    "    def wasserstein_loss(self, output_d, y):\n",
    "        return torch.mean(output_d * y)\n",
    "    \n",
    "    def train_generator(self, signal_tensor, z):\n",
    "        generated_params = self.generator(signal_tensor, z)\n",
    "        fake_output = self.discriminator(signal_tensor, generated_params)\n",
    "        g_loss = -torch.mean(fake_output)\n",
    "\n",
    "        self.optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_g.step()\n",
    "\n",
    "        return g_loss.item()\n",
    "    \n",
    "    def train_discriminator(self, signal_tensor, params_tensor, z):\n",
    "        fake_params = self.generator(signal_tensor, z).detach()\n",
    "        real_output = self.discriminator(signal_tensor, params_tensor)\n",
    "        fake_output = self.discriminator(signal_tensor, fake_params)\n",
    "\n",
    "        d_loss = -(torch.mean(real_output) - torch.mean(fake_output))\n",
    "\n",
    "        self.optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_d.step()\n",
    "\n",
    "        # Weight clipping\n",
    "        for p in self.discriminator.parameters():\n",
    "            p.data.clamp_(-self.weight_clip, self.weight_clip)\n",
    "\n",
    "\n",
    "        return d_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Generator loss: -0.49621060490608215, Discriminator loss: -0.0007853806018829346\n",
      "Epoch 2/500, Generator loss: -0.498079776763916, Discriminator loss: 0.00013130903244018555\n",
      "Epoch 3/500, Generator loss: -0.4979937970638275, Discriminator loss: -0.00041872262954711914\n",
      "Epoch 4/500, Generator loss: -0.49821776151657104, Discriminator loss: -0.00043952465057373047\n",
      "Epoch 5/500, Generator loss: -0.4977778494358063, Discriminator loss: -0.0004678070545196533\n",
      "Epoch 6/500, Generator loss: -0.49880266189575195, Discriminator loss: -9.953975677490234e-06\n",
      "Epoch 7/500, Generator loss: -0.49870944023132324, Discriminator loss: 0.000183790922164917\n",
      "Epoch 8/500, Generator loss: -0.49898388981819153, Discriminator loss: 0.0001569986343383789\n",
      "Epoch 9/500, Generator loss: -0.4985247254371643, Discriminator loss: -0.0007760822772979736\n",
      "Epoch 10/500, Generator loss: -0.49864208698272705, Discriminator loss: -0.00023487210273742676\n",
      "Epoch 11/500, Generator loss: -0.4989367723464966, Discriminator loss: -0.0004093945026397705\n",
      "Epoch 12/500, Generator loss: -0.4983118176460266, Discriminator loss: -0.0003987252712249756\n",
      "Epoch 13/500, Generator loss: -0.4985142648220062, Discriminator loss: -0.00046753883361816406\n",
      "Epoch 14/500, Generator loss: -0.4991954565048218, Discriminator loss: 5.9038400650024414e-05\n",
      "Epoch 15/500, Generator loss: -0.4989330470561981, Discriminator loss: -0.0002047121524810791\n",
      "Epoch 16/500, Generator loss: -0.49900567531585693, Discriminator loss: -0.00020706653594970703\n",
      "Epoch 17/500, Generator loss: -0.4992130994796753, Discriminator loss: -0.00011965632438659668\n",
      "Epoch 18/500, Generator loss: -0.49901828169822693, Discriminator loss: -9.995698928833008e-05\n",
      "Epoch 19/500, Generator loss: -0.49893322587013245, Discriminator loss: 0.00022998452186584473\n",
      "Epoch 20/500, Generator loss: -0.4972558617591858, Discriminator loss: 3.0040740966796875e-05\n",
      "Epoch 21/500, Generator loss: -0.49886295199394226, Discriminator loss: -0.0002474486827850342\n",
      "Epoch 22/500, Generator loss: -0.4991404414176941, Discriminator loss: 0.0005751252174377441\n",
      "Epoch 23/500, Generator loss: -0.4991225004196167, Discriminator loss: -0.00012934207916259766\n",
      "Epoch 24/500, Generator loss: -0.49912747740745544, Discriminator loss: 0.00011035799980163574\n",
      "Epoch 25/500, Generator loss: -0.49931204319000244, Discriminator loss: -0.0005398094654083252\n",
      "Epoch 26/500, Generator loss: -0.49914613366127014, Discriminator loss: -0.00035053491592407227\n",
      "Epoch 27/500, Generator loss: -0.49879923462867737, Discriminator loss: -2.0712614059448242e-05\n",
      "Epoch 28/500, Generator loss: -0.4987969696521759, Discriminator loss: -0.0005477666854858398\n",
      "Epoch 29/500, Generator loss: -0.4995327889919281, Discriminator loss: -0.0008372962474822998\n",
      "Epoch 30/500, Generator loss: -0.4990711808204651, Discriminator loss: 2.1785497665405273e-05\n",
      "Epoch 31/500, Generator loss: -0.49924275279045105, Discriminator loss: 6.064772605895996e-05\n",
      "Epoch 32/500, Generator loss: -0.4993192255496979, Discriminator loss: 0.0003800392150878906\n",
      "Epoch 33/500, Generator loss: -0.4982130229473114, Discriminator loss: 0.0005705058574676514\n",
      "Epoch 34/500, Generator loss: -0.49932482838630676, Discriminator loss: -0.00023865699768066406\n",
      "Epoch 35/500, Generator loss: -0.49960431456565857, Discriminator loss: -6.052851676940918e-05\n",
      "Epoch 36/500, Generator loss: -0.49941501021385193, Discriminator loss: -5.0395727157592773e-05\n",
      "Epoch 37/500, Generator loss: -0.49901774525642395, Discriminator loss: -0.0005570650100708008\n",
      "Epoch 38/500, Generator loss: -0.49973568320274353, Discriminator loss: -0.0002547800540924072\n",
      "Epoch 39/500, Generator loss: -0.4994039535522461, Discriminator loss: -2.3186206817626953e-05\n",
      "Epoch 40/500, Generator loss: -0.49978259205818176, Discriminator loss: 0.0003803074359893799\n",
      "Epoch 41/500, Generator loss: -0.49949100613594055, Discriminator loss: -0.0006241500377655029\n",
      "Epoch 42/500, Generator loss: -0.4983779489994049, Discriminator loss: -0.00031694769859313965\n",
      "Epoch 43/500, Generator loss: -0.4993327856063843, Discriminator loss: -0.000501781702041626\n",
      "Epoch 44/500, Generator loss: -0.4998667538166046, Discriminator loss: -0.000945359468460083\n",
      "Epoch 45/500, Generator loss: -0.5003464221954346, Discriminator loss: 0.0005346238613128662\n",
      "Epoch 46/500, Generator loss: -0.4997265636920929, Discriminator loss: -0.00015693902969360352\n",
      "Epoch 47/500, Generator loss: -0.4997355043888092, Discriminator loss: 0.00024646520614624023\n",
      "Epoch 48/500, Generator loss: -0.4992046058177948, Discriminator loss: -0.0003559887409210205\n",
      "Epoch 49/500, Generator loss: -0.49900171160697937, Discriminator loss: -0.0003053247928619385\n",
      "Epoch 50/500, Generator loss: -0.4998317360877991, Discriminator loss: -0.00028440356254577637\n",
      "Epoch 51/500, Generator loss: -0.499416321516037, Discriminator loss: -0.0005404949188232422\n",
      "Epoch 52/500, Generator loss: -0.4997253119945526, Discriminator loss: -0.00016248226165771484\n",
      "Epoch 53/500, Generator loss: -0.4993620812892914, Discriminator loss: 4.13358211517334e-05\n",
      "Epoch 54/500, Generator loss: -0.49944737553596497, Discriminator loss: -7.402896881103516e-05\n",
      "Epoch 55/500, Generator loss: -0.4988948702812195, Discriminator loss: 0.00015032291412353516\n",
      "Epoch 56/500, Generator loss: -0.4980545938014984, Discriminator loss: -0.0001850128173828125\n",
      "Epoch 57/500, Generator loss: -0.49842971563339233, Discriminator loss: 0.0005335211753845215\n",
      "Epoch 58/500, Generator loss: -0.4992694854736328, Discriminator loss: -0.0006573796272277832\n",
      "Epoch 59/500, Generator loss: -0.49883270263671875, Discriminator loss: 0.0006073117256164551\n",
      "Epoch 60/500, Generator loss: -0.4999929666519165, Discriminator loss: -0.0010117590427398682\n",
      "Epoch 61/500, Generator loss: -0.5000001192092896, Discriminator loss: -0.000270843505859375\n",
      "Epoch 62/500, Generator loss: -0.5009931921958923, Discriminator loss: -0.0006734132766723633\n",
      "Epoch 63/500, Generator loss: -0.5008915066719055, Discriminator loss: -0.001416325569152832\n",
      "Epoch 64/500, Generator loss: -0.5003601312637329, Discriminator loss: -0.0004755258560180664\n",
      "Epoch 65/500, Generator loss: -0.5008883476257324, Discriminator loss: -0.0013560950756072998\n",
      "Epoch 66/500, Generator loss: -0.4998556673526764, Discriminator loss: 0.0007054805755615234\n",
      "Epoch 67/500, Generator loss: -0.5004211068153381, Discriminator loss: -0.0005488693714141846\n",
      "Epoch 68/500, Generator loss: -0.5010117888450623, Discriminator loss: -0.002430558204650879\n",
      "Epoch 69/500, Generator loss: -0.5001916289329529, Discriminator loss: -0.0005075931549072266\n",
      "Epoch 70/500, Generator loss: -0.5001896023750305, Discriminator loss: -0.0003586411476135254\n",
      "Epoch 71/500, Generator loss: -0.5002357363700867, Discriminator loss: -0.0005702376365661621\n",
      "Epoch 72/500, Generator loss: -0.4998888671398163, Discriminator loss: 0.0005068778991699219\n",
      "Epoch 73/500, Generator loss: -0.4994223117828369, Discriminator loss: 0.0004191398620605469\n",
      "Epoch 74/500, Generator loss: -0.5009353160858154, Discriminator loss: -0.0004987120628356934\n",
      "Epoch 75/500, Generator loss: -0.5002618432044983, Discriminator loss: 0.0008405148983001709\n",
      "Epoch 76/500, Generator loss: -0.5003155469894409, Discriminator loss: -0.000606834888458252\n",
      "Epoch 77/500, Generator loss: -0.500596284866333, Discriminator loss: -0.0009060800075531006\n",
      "Epoch 78/500, Generator loss: -0.4993973970413208, Discriminator loss: -2.3484230041503906e-05\n",
      "Epoch 79/500, Generator loss: -0.4997466802597046, Discriminator loss: -0.0009551644325256348\n",
      "Epoch 80/500, Generator loss: -0.4987480938434601, Discriminator loss: 0.00018864870071411133\n",
      "Epoch 81/500, Generator loss: -0.49749964475631714, Discriminator loss: -7.238984107971191e-05\n",
      "Epoch 82/500, Generator loss: -0.49851301312446594, Discriminator loss: -0.001504749059677124\n",
      "Epoch 83/500, Generator loss: -0.499053418636322, Discriminator loss: 0.0001531541347503662\n",
      "Epoch 84/500, Generator loss: -0.4993053674697876, Discriminator loss: -9.199976921081543e-05\n",
      "Epoch 85/500, Generator loss: -0.49932345747947693, Discriminator loss: 0.00010725855827331543\n",
      "Epoch 86/500, Generator loss: -0.49946117401123047, Discriminator loss: -3.2782554626464844e-05\n",
      "Epoch 87/500, Generator loss: -0.49987122416496277, Discriminator loss: 0.0007029175758361816\n",
      "Epoch 88/500, Generator loss: -0.49942049384117126, Discriminator loss: -0.0004512369632720947\n",
      "Epoch 89/500, Generator loss: -0.4991418719291687, Discriminator loss: -3.37064266204834e-05\n",
      "Epoch 90/500, Generator loss: -0.49940556287765503, Discriminator loss: 0.00014579296112060547\n",
      "Epoch 91/500, Generator loss: -0.4995611608028412, Discriminator loss: -0.00015652179718017578\n",
      "Epoch 92/500, Generator loss: -0.49901121854782104, Discriminator loss: 0.00014954805374145508\n",
      "Epoch 93/500, Generator loss: -0.4990728497505188, Discriminator loss: -0.0004031658172607422\n",
      "Epoch 94/500, Generator loss: -0.4988071918487549, Discriminator loss: -0.00020170211791992188\n",
      "Epoch 95/500, Generator loss: -0.49884527921676636, Discriminator loss: -0.00010129809379577637\n",
      "Epoch 96/500, Generator loss: -0.49863579869270325, Discriminator loss: -0.00025004148483276367\n",
      "Epoch 97/500, Generator loss: -0.49913591146469116, Discriminator loss: -0.0008734464645385742\n",
      "Epoch 98/500, Generator loss: -0.5004705190658569, Discriminator loss: -0.0014252066612243652\n",
      "Epoch 99/500, Generator loss: -0.5014153718948364, Discriminator loss: 0.00016134977340698242\n",
      "Epoch 100/500, Generator loss: -0.5005009770393372, Discriminator loss: -0.0005556344985961914\n",
      "Epoch 101/500, Generator loss: -0.501384973526001, Discriminator loss: -0.0004958212375640869\n",
      "Epoch 102/500, Generator loss: -0.5001258254051208, Discriminator loss: -0.00026744604110717773\n",
      "Epoch 103/500, Generator loss: -0.5007039904594421, Discriminator loss: -0.0007438063621520996\n",
      "Epoch 104/500, Generator loss: -0.5014111995697021, Discriminator loss: 5.9664249420166016e-05\n",
      "Epoch 105/500, Generator loss: -0.5010043382644653, Discriminator loss: -0.00016945600509643555\n",
      "Epoch 106/500, Generator loss: -0.4995560944080353, Discriminator loss: -0.00013709068298339844\n",
      "Epoch 107/500, Generator loss: -0.4998213052749634, Discriminator loss: -4.410743713378906e-06\n",
      "Epoch 108/500, Generator loss: -0.50042325258255, Discriminator loss: 0.0003337264060974121\n",
      "Epoch 109/500, Generator loss: -0.49825426936149597, Discriminator loss: 9.450316429138184e-05\n",
      "Epoch 110/500, Generator loss: -0.4981590509414673, Discriminator loss: -1.9073486328125e-06\n",
      "Epoch 111/500, Generator loss: -0.49816545844078064, Discriminator loss: -5.906820297241211e-05\n",
      "Epoch 112/500, Generator loss: -0.4982477128505707, Discriminator loss: -0.00028821825981140137\n",
      "Epoch 113/500, Generator loss: -0.49868372082710266, Discriminator loss: -0.001230865716934204\n",
      "Epoch 114/500, Generator loss: -0.5003085136413574, Discriminator loss: 9.357929229736328e-05\n",
      "Epoch 115/500, Generator loss: -0.5005096197128296, Discriminator loss: -0.0012916922569274902\n",
      "Epoch 116/500, Generator loss: -0.500584065914154, Discriminator loss: -3.504753112792969e-05\n",
      "Epoch 117/500, Generator loss: -0.5005706548690796, Discriminator loss: -0.0004737973213195801\n",
      "Epoch 118/500, Generator loss: -0.5014011859893799, Discriminator loss: -0.0002766847610473633\n",
      "Epoch 119/500, Generator loss: -0.5002502799034119, Discriminator loss: 0.00070151686668396\n",
      "Epoch 120/500, Generator loss: -0.4997027516365051, Discriminator loss: -0.0006945133209228516\n",
      "Epoch 121/500, Generator loss: -0.49885985255241394, Discriminator loss: 0.00034502148628234863\n",
      "Epoch 122/500, Generator loss: -0.49946436285972595, Discriminator loss: -8.299946784973145e-05\n",
      "Epoch 123/500, Generator loss: -0.4992537796497345, Discriminator loss: -0.0004729628562927246\n",
      "Epoch 124/500, Generator loss: -0.4995201528072357, Discriminator loss: -0.00015810132026672363\n",
      "Epoch 125/500, Generator loss: -0.5000692009925842, Discriminator loss: -5.900859832763672e-05\n",
      "Epoch 126/500, Generator loss: -0.49984681606292725, Discriminator loss: -0.0003891885280609131\n",
      "Epoch 127/500, Generator loss: -0.5001943707466125, Discriminator loss: -4.678964614868164e-05\n",
      "Epoch 128/500, Generator loss: -0.5003253221511841, Discriminator loss: -1.2814998626708984e-05\n",
      "Epoch 129/500, Generator loss: -0.5001916289329529, Discriminator loss: 4.8160552978515625e-05\n",
      "Epoch 130/500, Generator loss: -0.5003457069396973, Discriminator loss: -1.3709068298339844e-06\n",
      "Epoch 131/500, Generator loss: -0.5003842711448669, Discriminator loss: 8.928775787353516e-05\n",
      "Epoch 132/500, Generator loss: -0.5002591609954834, Discriminator loss: -2.4020671844482422e-05\n",
      "Epoch 133/500, Generator loss: -0.4999927878379822, Discriminator loss: -6.312131881713867e-05\n",
      "Epoch 134/500, Generator loss: -0.5001593828201294, Discriminator loss: -0.0003992617130279541\n",
      "Epoch 135/500, Generator loss: -0.5005722641944885, Discriminator loss: -0.0004779398441314697\n",
      "Epoch 136/500, Generator loss: -0.5003684163093567, Discriminator loss: -6.514787673950195e-05\n",
      "Epoch 137/500, Generator loss: -0.5003811717033386, Discriminator loss: -0.0003991127014160156\n",
      "Epoch 138/500, Generator loss: -0.5004217028617859, Discriminator loss: -3.224611282348633e-05\n",
      "Epoch 139/500, Generator loss: -0.5005933046340942, Discriminator loss: 1.5020370483398438e-05\n",
      "Epoch 140/500, Generator loss: -0.49907010793685913, Discriminator loss: -0.0010949373245239258\n",
      "Epoch 141/500, Generator loss: -0.49897903203964233, Discriminator loss: -0.000367581844329834\n",
      "Epoch 142/500, Generator loss: -0.5002920627593994, Discriminator loss: 0.0004277825355529785\n",
      "Epoch 143/500, Generator loss: -0.5013604164123535, Discriminator loss: -3.0279159545898438e-05\n",
      "Epoch 144/500, Generator loss: -0.5000400543212891, Discriminator loss: 8.720159530639648e-05\n",
      "Epoch 145/500, Generator loss: -0.5004487633705139, Discriminator loss: 0.0002643465995788574\n",
      "Epoch 146/500, Generator loss: -0.49949219822883606, Discriminator loss: -0.0002601146697998047\n",
      "Epoch 147/500, Generator loss: -0.5004207491874695, Discriminator loss: -0.0008127391338348389\n",
      "Epoch 148/500, Generator loss: -0.5007950663566589, Discriminator loss: -0.00028389692306518555\n",
      "Epoch 149/500, Generator loss: -0.5009523034095764, Discriminator loss: -0.0006317198276519775\n",
      "Epoch 150/500, Generator loss: -0.5009257793426514, Discriminator loss: 1.1444091796875e-05\n",
      "Epoch 151/500, Generator loss: -0.5002196431159973, Discriminator loss: -0.0005462765693664551\n",
      "Epoch 152/500, Generator loss: -0.5002350807189941, Discriminator loss: -0.0010520219802856445\n",
      "Epoch 153/500, Generator loss: -0.4990684390068054, Discriminator loss: -0.0006520748138427734\n",
      "Epoch 154/500, Generator loss: -0.5004836320877075, Discriminator loss: -0.00014221668243408203\n",
      "Epoch 155/500, Generator loss: -0.500356137752533, Discriminator loss: -0.00037658214569091797\n",
      "Epoch 156/500, Generator loss: -0.49991917610168457, Discriminator loss: 0.00013709068298339844\n",
      "Epoch 157/500, Generator loss: -0.5005123019218445, Discriminator loss: -0.0003027915954589844\n",
      "Epoch 158/500, Generator loss: -0.5007871985435486, Discriminator loss: -0.0006031990051269531\n",
      "Epoch 159/500, Generator loss: -0.49923595786094666, Discriminator loss: -0.00047969818115234375\n",
      "Epoch 160/500, Generator loss: -0.5007131695747375, Discriminator loss: -0.00021445751190185547\n",
      "Epoch 161/500, Generator loss: -0.5004206895828247, Discriminator loss: 0.00013560056686401367\n",
      "Epoch 162/500, Generator loss: -0.49918925762176514, Discriminator loss: -0.000585019588470459\n",
      "Epoch 163/500, Generator loss: -0.500534176826477, Discriminator loss: -0.0001468062400817871\n",
      "Epoch 164/500, Generator loss: -0.5003464221954346, Discriminator loss: 0.00013208389282226562\n",
      "Epoch 165/500, Generator loss: -0.5004848837852478, Discriminator loss: -0.00012624263763427734\n",
      "Epoch 166/500, Generator loss: -0.5000699758529663, Discriminator loss: -0.0002294778823852539\n",
      "Epoch 167/500, Generator loss: -0.49748876690864563, Discriminator loss: -0.00010159611701965332\n",
      "Epoch 168/500, Generator loss: -0.49815893173217773, Discriminator loss: -0.000356525182723999\n",
      "Epoch 169/500, Generator loss: -0.49823763966560364, Discriminator loss: -6.347894668579102e-05\n",
      "Epoch 170/500, Generator loss: -0.49826565384864807, Discriminator loss: 2.6792287826538086e-05\n",
      "Epoch 171/500, Generator loss: -0.4987208843231201, Discriminator loss: -0.00013568997383117676\n",
      "Epoch 172/500, Generator loss: -0.49983084201812744, Discriminator loss: 9.417533874511719e-06\n",
      "Epoch 173/500, Generator loss: -0.49986425042152405, Discriminator loss: 2.1457672119140625e-06\n",
      "Epoch 174/500, Generator loss: -0.5000837445259094, Discriminator loss: -3.355741500854492e-05\n",
      "Epoch 175/500, Generator loss: -0.49997127056121826, Discriminator loss: -4.762411117553711e-05\n",
      "Epoch 176/500, Generator loss: -0.4998459219932556, Discriminator loss: -1.3530254364013672e-05\n",
      "Epoch 177/500, Generator loss: -0.4997636079788208, Discriminator loss: 3.5762786865234375e-06\n",
      "Epoch 178/500, Generator loss: -0.4996558427810669, Discriminator loss: -2.637505531311035e-05\n",
      "Epoch 179/500, Generator loss: -0.49961423873901367, Discriminator loss: -7.796287536621094e-05\n",
      "Epoch 180/500, Generator loss: -0.49859514832496643, Discriminator loss: 0.0001221299171447754\n",
      "Epoch 181/500, Generator loss: -0.49904778599739075, Discriminator loss: -5.874037742614746e-05\n",
      "Epoch 182/500, Generator loss: -0.4988453984260559, Discriminator loss: -0.0001990795135498047\n",
      "Epoch 183/500, Generator loss: -0.49899280071258545, Discriminator loss: -1.4901161193847656e-06\n",
      "Epoch 184/500, Generator loss: -0.4995557367801666, Discriminator loss: 0.00018727779388427734\n",
      "Epoch 185/500, Generator loss: -0.500176727771759, Discriminator loss: 4.6312808990478516e-05\n",
      "Epoch 186/500, Generator loss: -0.5002122521400452, Discriminator loss: -5.91278076171875e-05\n",
      "Epoch 187/500, Generator loss: -0.4996292293071747, Discriminator loss: -7.519125938415527e-05\n",
      "Epoch 188/500, Generator loss: -0.5004927515983582, Discriminator loss: 3.784894943237305e-05\n",
      "Epoch 189/500, Generator loss: -0.5008652806282043, Discriminator loss: -2.014636993408203e-05\n",
      "Epoch 190/500, Generator loss: -0.5005501508712769, Discriminator loss: -0.00033086538314819336\n",
      "Epoch 191/500, Generator loss: -0.5004948973655701, Discriminator loss: 0.00011491775512695312\n",
      "Epoch 192/500, Generator loss: -0.5000295639038086, Discriminator loss: -0.00039446353912353516\n",
      "Epoch 193/500, Generator loss: -0.498577356338501, Discriminator loss: -1.475214958190918e-05\n",
      "Epoch 194/500, Generator loss: -0.49817728996276855, Discriminator loss: 5.066394805908203e-05\n",
      "Epoch 195/500, Generator loss: -0.4985755383968353, Discriminator loss: -7.170438766479492e-05\n",
      "Epoch 196/500, Generator loss: -0.5005868077278137, Discriminator loss: -0.0002269148826599121\n",
      "Epoch 197/500, Generator loss: -0.5002550482749939, Discriminator loss: 8.475780487060547e-05\n",
      "Epoch 198/500, Generator loss: -0.5004044771194458, Discriminator loss: 0.0006982386112213135\n",
      "Epoch 199/500, Generator loss: -0.5001673698425293, Discriminator loss: -0.0004608631134033203\n",
      "Epoch 200/500, Generator loss: -0.4999735355377197, Discriminator loss: -0.0003464818000793457\n",
      "Epoch 201/500, Generator loss: -0.4998219609260559, Discriminator loss: -1.0132789611816406e-05\n",
      "Epoch 202/500, Generator loss: -0.5005778074264526, Discriminator loss: -4.708766937255859e-06\n",
      "Epoch 203/500, Generator loss: -0.5003783702850342, Discriminator loss: 3.0994415283203125e-05\n",
      "Epoch 204/500, Generator loss: -0.4997372329235077, Discriminator loss: -1.7464160919189453e-05\n",
      "Epoch 205/500, Generator loss: -0.499899297952652, Discriminator loss: 0.00010699033737182617\n",
      "Epoch 206/500, Generator loss: -0.4993968605995178, Discriminator loss: -0.00010535120964050293\n",
      "Epoch 207/500, Generator loss: -0.49915826320648193, Discriminator loss: -0.00033801794052124023\n",
      "Epoch 208/500, Generator loss: -0.5001507997512817, Discriminator loss: 0.00011691451072692871\n",
      "Epoch 209/500, Generator loss: -0.4995502829551697, Discriminator loss: -0.00038185715675354004\n",
      "Epoch 210/500, Generator loss: -0.49979445338249207, Discriminator loss: -0.0016315877437591553\n",
      "Epoch 211/500, Generator loss: -0.5001659989356995, Discriminator loss: -7.69495964050293e-05\n",
      "Epoch 212/500, Generator loss: -0.5003712177276611, Discriminator loss: -0.0008755028247833252\n",
      "Epoch 213/500, Generator loss: -0.5013228058815002, Discriminator loss: 0.00020998716354370117\n",
      "Epoch 214/500, Generator loss: -0.5010055303573608, Discriminator loss: -0.00017577409744262695\n",
      "Epoch 215/500, Generator loss: -0.5000426769256592, Discriminator loss: 0.00017079710960388184\n",
      "Epoch 216/500, Generator loss: -0.4992486834526062, Discriminator loss: -0.00012439489364624023\n",
      "Epoch 217/500, Generator loss: -0.49970823526382446, Discriminator loss: 2.7805566787719727e-05\n",
      "Epoch 218/500, Generator loss: -0.49951887130737305, Discriminator loss: 0.00011792778968811035\n",
      "Epoch 219/500, Generator loss: -0.499222069978714, Discriminator loss: -6.917119026184082e-05\n",
      "Epoch 220/500, Generator loss: -0.5000040531158447, Discriminator loss: -0.001185685396194458\n",
      "Epoch 221/500, Generator loss: -0.5002013444900513, Discriminator loss: -0.00021183490753173828\n",
      "Epoch 222/500, Generator loss: -0.4995824098587036, Discriminator loss: -1.3560056686401367e-05\n",
      "Epoch 223/500, Generator loss: -0.5000001192092896, Discriminator loss: -7.50422477722168e-05\n",
      "Epoch 224/500, Generator loss: -0.49946388602256775, Discriminator loss: 0.00014469027519226074\n",
      "Epoch 225/500, Generator loss: -0.49918636679649353, Discriminator loss: 8.374452590942383e-06\n",
      "Epoch 226/500, Generator loss: -0.49996405839920044, Discriminator loss: -0.00043958425521850586\n",
      "Epoch 227/500, Generator loss: -0.500262439250946, Discriminator loss: -0.0003147125244140625\n",
      "Epoch 228/500, Generator loss: -0.498130202293396, Discriminator loss: -0.0002537071704864502\n",
      "Epoch 229/500, Generator loss: -0.498290091753006, Discriminator loss: -0.0001055598258972168\n",
      "Epoch 230/500, Generator loss: -0.4985700845718384, Discriminator loss: 6.034970283508301e-05\n",
      "Epoch 231/500, Generator loss: -0.4991372227668762, Discriminator loss: -0.00014418363571166992\n",
      "Epoch 232/500, Generator loss: -0.4990736246109009, Discriminator loss: -6.16908073425293e-05\n",
      "Epoch 233/500, Generator loss: -0.4997846782207489, Discriminator loss: -3.316998481750488e-05\n",
      "Epoch 234/500, Generator loss: -0.5006781220436096, Discriminator loss: 0.00011873245239257812\n",
      "Epoch 235/500, Generator loss: -0.500700831413269, Discriminator loss: 9.518861770629883e-05\n",
      "Epoch 236/500, Generator loss: -0.5000857710838318, Discriminator loss: -1.52587890625e-05\n",
      "Epoch 237/500, Generator loss: -0.50030118227005, Discriminator loss: 4.2319297790527344e-05\n",
      "Epoch 238/500, Generator loss: -0.5002632141113281, Discriminator loss: 3.641843795776367e-05\n",
      "Epoch 239/500, Generator loss: -0.5002226829528809, Discriminator loss: -1.341104507446289e-05\n",
      "Epoch 240/500, Generator loss: -0.5001500248908997, Discriminator loss: -3.516674041748047e-05\n",
      "Epoch 241/500, Generator loss: -0.5001225471496582, Discriminator loss: -9.40561294555664e-05\n",
      "Epoch 242/500, Generator loss: -0.5006459355354309, Discriminator loss: -8.213520050048828e-05\n",
      "Epoch 243/500, Generator loss: -0.5008721947669983, Discriminator loss: -0.00017011165618896484\n",
      "Epoch 244/500, Generator loss: -0.49990227818489075, Discriminator loss: -6.598234176635742e-05\n",
      "Epoch 245/500, Generator loss: -0.4995688199996948, Discriminator loss: -7.021427154541016e-05\n",
      "Epoch 246/500, Generator loss: -0.5000820755958557, Discriminator loss: -0.00018262863159179688\n",
      "Epoch 247/500, Generator loss: -0.4996603727340698, Discriminator loss: 7.173418998718262e-05\n",
      "Epoch 248/500, Generator loss: -0.49910348653793335, Discriminator loss: -3.981590270996094e-05\n",
      "Epoch 249/500, Generator loss: -0.5001111626625061, Discriminator loss: -0.0009115040302276611\n",
      "Epoch 250/500, Generator loss: -0.5002724528312683, Discriminator loss: -2.092123031616211e-05\n",
      "Epoch 251/500, Generator loss: -0.5002840757369995, Discriminator loss: -0.00029975175857543945\n",
      "Epoch 252/500, Generator loss: -0.5003228187561035, Discriminator loss: -0.0002925395965576172\n",
      "Epoch 253/500, Generator loss: -0.5004256963729858, Discriminator loss: -0.00027740001678466797\n",
      "Epoch 254/500, Generator loss: -0.49935197830200195, Discriminator loss: 3.173947334289551e-05\n",
      "Epoch 255/500, Generator loss: -0.49922361969947815, Discriminator loss: 0.0006920695304870605\n",
      "Epoch 256/500, Generator loss: -0.5005766153335571, Discriminator loss: -0.0006939172744750977\n",
      "Epoch 257/500, Generator loss: -0.5000569224357605, Discriminator loss: 0.00014391541481018066\n",
      "Epoch 258/500, Generator loss: -0.4992715120315552, Discriminator loss: -0.00023949146270751953\n",
      "Epoch 259/500, Generator loss: -0.5000750422477722, Discriminator loss: 0.00019466876983642578\n",
      "Epoch 260/500, Generator loss: -0.49882084131240845, Discriminator loss: -3.358721733093262e-05\n",
      "Epoch 261/500, Generator loss: -0.49810415506362915, Discriminator loss: 2.1398067474365234e-05\n",
      "Epoch 262/500, Generator loss: -0.4978199005126953, Discriminator loss: -3.0100345611572266e-06\n",
      "Epoch 263/500, Generator loss: -0.49788811802864075, Discriminator loss: -2.09808349609375e-05\n",
      "Epoch 264/500, Generator loss: -0.4983951151371002, Discriminator loss: -0.00010073184967041016\n",
      "Epoch 265/500, Generator loss: -0.49930456280708313, Discriminator loss: -0.0004787147045135498\n",
      "Epoch 266/500, Generator loss: -0.5012260675430298, Discriminator loss: -4.363059997558594e-05\n",
      "Epoch 267/500, Generator loss: -0.5014545321464539, Discriminator loss: -0.00019663572311401367\n",
      "Epoch 268/500, Generator loss: -0.5001015663146973, Discriminator loss: -0.00010290741920471191\n",
      "Epoch 269/500, Generator loss: -0.4990891218185425, Discriminator loss: -8.937716484069824e-05\n",
      "Epoch 270/500, Generator loss: -0.49803176522254944, Discriminator loss: -0.0003216564655303955\n",
      "Epoch 271/500, Generator loss: -0.499520868062973, Discriminator loss: 0.00014597177505493164\n",
      "Epoch 272/500, Generator loss: -0.5006111860275269, Discriminator loss: -6.41942024230957e-05\n",
      "Epoch 273/500, Generator loss: -0.499455988407135, Discriminator loss: 0.000341951847076416\n",
      "Epoch 274/500, Generator loss: -0.49900972843170166, Discriminator loss: 7.12275505065918e-06\n",
      "Epoch 275/500, Generator loss: -0.49913424253463745, Discriminator loss: -9.447336196899414e-06\n",
      "Epoch 276/500, Generator loss: -0.5000365376472473, Discriminator loss: 0.00010803341865539551\n",
      "Epoch 277/500, Generator loss: -0.4995866119861603, Discriminator loss: 7.298588752746582e-05\n",
      "Epoch 278/500, Generator loss: -0.4997294247150421, Discriminator loss: -1.0132789611816406e-06\n",
      "Epoch 279/500, Generator loss: -0.4994966685771942, Discriminator loss: -0.000794827938079834\n",
      "Epoch 280/500, Generator loss: -0.5000505447387695, Discriminator loss: -0.0004120171070098877\n",
      "Epoch 281/500, Generator loss: -0.499590128660202, Discriminator loss: -0.00016313791275024414\n",
      "Epoch 282/500, Generator loss: -0.4997655153274536, Discriminator loss: 0.00010186433792114258\n",
      "Epoch 283/500, Generator loss: -0.4992443919181824, Discriminator loss: -0.0001684725284576416\n",
      "Epoch 284/500, Generator loss: -0.500514566898346, Discriminator loss: -0.0005727112293243408\n",
      "Epoch 285/500, Generator loss: -0.49950969219207764, Discriminator loss: -6.67273998260498e-05\n",
      "Epoch 286/500, Generator loss: -0.5003299713134766, Discriminator loss: -0.00011807680130004883\n",
      "Epoch 287/500, Generator loss: -0.4996575117111206, Discriminator loss: -0.0007358789443969727\n",
      "Epoch 288/500, Generator loss: -0.4983352720737457, Discriminator loss: -2.8312206268310547e-05\n",
      "Epoch 289/500, Generator loss: -0.499750018119812, Discriminator loss: -0.0005718469619750977\n",
      "Epoch 290/500, Generator loss: -0.5011137127876282, Discriminator loss: -0.0009669065475463867\n",
      "Epoch 291/500, Generator loss: -0.49992597103118896, Discriminator loss: -2.5331974029541016e-05\n",
      "Epoch 292/500, Generator loss: -0.4995923936367035, Discriminator loss: -0.0006687343120574951\n",
      "Epoch 293/500, Generator loss: -0.49980220198631287, Discriminator loss: 0.00015482306480407715\n",
      "Epoch 294/500, Generator loss: -0.499884694814682, Discriminator loss: 0.0008456707000732422\n",
      "Epoch 295/500, Generator loss: -0.4992850422859192, Discriminator loss: -0.00020706653594970703\n",
      "Epoch 296/500, Generator loss: -0.5015754699707031, Discriminator loss: 0.00022751092910766602\n",
      "Epoch 297/500, Generator loss: -0.5004327297210693, Discriminator loss: -0.0002161264419555664\n",
      "Epoch 298/500, Generator loss: -0.49844664335250854, Discriminator loss: 3.892183303833008e-05\n",
      "Epoch 299/500, Generator loss: -0.49912428855895996, Discriminator loss: 7.995963096618652e-05\n",
      "Epoch 300/500, Generator loss: -0.5004611611366272, Discriminator loss: -0.00024694204330444336\n",
      "Epoch 301/500, Generator loss: -0.5008349418640137, Discriminator loss: -0.00023704767227172852\n",
      "Epoch 302/500, Generator loss: -0.5000113844871521, Discriminator loss: -0.0007018446922302246\n",
      "Epoch 303/500, Generator loss: -0.4992262125015259, Discriminator loss: 7.748603820800781e-06\n",
      "Epoch 304/500, Generator loss: -0.5005597472190857, Discriminator loss: -6.973743438720703e-06\n",
      "Epoch 305/500, Generator loss: -0.49988940358161926, Discriminator loss: -0.0004311800003051758\n",
      "Epoch 306/500, Generator loss: -0.4995110034942627, Discriminator loss: -7.236003875732422e-05\n",
      "Epoch 307/500, Generator loss: -0.5003266930580139, Discriminator loss: 0.00020438432693481445\n",
      "Epoch 308/500, Generator loss: -0.4994075298309326, Discriminator loss: -0.00018128752708435059\n",
      "Epoch 309/500, Generator loss: -0.4991700053215027, Discriminator loss: -2.79843807220459e-05\n",
      "Epoch 310/500, Generator loss: -0.5012681484222412, Discriminator loss: 8.881092071533203e-05\n",
      "Epoch 311/500, Generator loss: -0.5002905130386353, Discriminator loss: -0.00030678510665893555\n",
      "Epoch 312/500, Generator loss: -0.4980407655239105, Discriminator loss: 0.0001322627067565918\n",
      "Epoch 313/500, Generator loss: -0.4971844255924225, Discriminator loss: 8.219480514526367e-05\n",
      "Epoch 314/500, Generator loss: -0.49846288561820984, Discriminator loss: 5.266070365905762e-05\n",
      "Epoch 315/500, Generator loss: -0.4998207092285156, Discriminator loss: -6.541609764099121e-05\n",
      "Epoch 316/500, Generator loss: -0.5008094310760498, Discriminator loss: 5.5730342864990234e-05\n",
      "Epoch 317/500, Generator loss: -0.5009152889251709, Discriminator loss: 0.00022554397583007812\n",
      "Epoch 318/500, Generator loss: -0.5009170174598694, Discriminator loss: 8.624792098999023e-05\n",
      "Epoch 319/500, Generator loss: -0.5010584592819214, Discriminator loss: -0.0002714991569519043\n",
      "Epoch 320/500, Generator loss: -0.5006057620048523, Discriminator loss: -9.357929229736328e-06\n",
      "Epoch 321/500, Generator loss: -0.500647246837616, Discriminator loss: 4.667043685913086e-05\n",
      "Epoch 322/500, Generator loss: -0.5007150173187256, Discriminator loss: 4.0411949157714844e-05\n",
      "Epoch 323/500, Generator loss: -0.5009185075759888, Discriminator loss: -0.00015860795974731445\n",
      "Epoch 324/500, Generator loss: -0.5006465911865234, Discriminator loss: -1.0132789611816406e-06\n",
      "Epoch 325/500, Generator loss: -0.5008636713027954, Discriminator loss: 2.4437904357910156e-06\n",
      "Epoch 326/500, Generator loss: -0.5011808276176453, Discriminator loss: 7.325410842895508e-05\n",
      "Epoch 327/500, Generator loss: -0.5004234313964844, Discriminator loss: 0.0007726550102233887\n",
      "Epoch 328/500, Generator loss: -0.499624639749527, Discriminator loss: -0.0006119012832641602\n",
      "Epoch 329/500, Generator loss: -0.49942630529403687, Discriminator loss: -0.0012385845184326172\n",
      "Epoch 330/500, Generator loss: -0.4997411370277405, Discriminator loss: -1.996755599975586e-05\n",
      "Epoch 331/500, Generator loss: -0.5002055764198303, Discriminator loss: 0.0007624030113220215\n",
      "Epoch 332/500, Generator loss: -0.4991903305053711, Discriminator loss: -0.0010078251361846924\n",
      "Epoch 333/500, Generator loss: -0.5001642107963562, Discriminator loss: -0.0008193850517272949\n",
      "Epoch 334/500, Generator loss: -0.5010242462158203, Discriminator loss: -0.00015938282012939453\n",
      "Epoch 335/500, Generator loss: -0.5006019473075867, Discriminator loss: -0.0004611015319824219\n",
      "Epoch 336/500, Generator loss: -0.5000118613243103, Discriminator loss: -0.00022736191749572754\n",
      "Epoch 337/500, Generator loss: -0.49947935342788696, Discriminator loss: 8.425116539001465e-05\n",
      "Epoch 338/500, Generator loss: -0.4997301995754242, Discriminator loss: 0.0002847015857696533\n",
      "Epoch 339/500, Generator loss: -0.5003512501716614, Discriminator loss: 0.00043767690658569336\n",
      "Epoch 340/500, Generator loss: -0.4993703067302704, Discriminator loss: -0.0012104213237762451\n",
      "Epoch 341/500, Generator loss: -0.49949660897254944, Discriminator loss: -0.00015056133270263672\n",
      "Epoch 342/500, Generator loss: -0.49915528297424316, Discriminator loss: -3.787875175476074e-05\n",
      "Epoch 343/500, Generator loss: -0.49986836314201355, Discriminator loss: -0.0003056824207305908\n",
      "Epoch 344/500, Generator loss: -0.4998672306537628, Discriminator loss: -0.00038498640060424805\n",
      "Epoch 345/500, Generator loss: -0.49948960542678833, Discriminator loss: -3.7789344787597656e-05\n",
      "Epoch 346/500, Generator loss: -0.49999022483825684, Discriminator loss: 5.066394805908203e-05\n",
      "Epoch 347/500, Generator loss: -0.4987248480319977, Discriminator loss: -9.712576866149902e-05\n",
      "Epoch 348/500, Generator loss: -0.49863117933273315, Discriminator loss: 6.118416786193848e-05\n",
      "Epoch 349/500, Generator loss: -0.4988163113594055, Discriminator loss: -7.05718994140625e-05\n",
      "Epoch 350/500, Generator loss: -0.49958929419517517, Discriminator loss: -0.00017249584197998047\n",
      "Epoch 351/500, Generator loss: -0.49942439794540405, Discriminator loss: -0.00014951825141906738\n",
      "Epoch 352/500, Generator loss: -0.49915990233421326, Discriminator loss: 5.84721565246582e-05\n",
      "Epoch 353/500, Generator loss: -0.4999651312828064, Discriminator loss: 0.00011679530143737793\n",
      "Epoch 354/500, Generator loss: -0.49957966804504395, Discriminator loss: 7.510185241699219e-05\n",
      "Epoch 355/500, Generator loss: -0.49962037801742554, Discriminator loss: 5.835294723510742e-05\n",
      "Epoch 356/500, Generator loss: -0.49973252415657043, Discriminator loss: -0.00037109851837158203\n",
      "Epoch 357/500, Generator loss: -0.5003738403320312, Discriminator loss: 0.0002206563949584961\n",
      "Epoch 358/500, Generator loss: -0.5013375878334045, Discriminator loss: 1.2099742889404297e-05\n",
      "Epoch 359/500, Generator loss: -0.500851035118103, Discriminator loss: -0.00016236305236816406\n",
      "Epoch 360/500, Generator loss: -0.5013871788978577, Discriminator loss: 0.00011014938354492188\n",
      "Epoch 361/500, Generator loss: -0.5010142922401428, Discriminator loss: -0.0001327991485595703\n",
      "Epoch 362/500, Generator loss: -0.5001375675201416, Discriminator loss: -4.953145980834961e-05\n",
      "Epoch 363/500, Generator loss: -0.49953943490982056, Discriminator loss: -0.0002607405185699463\n",
      "Epoch 364/500, Generator loss: -0.49984419345855713, Discriminator loss: -0.00015228986740112305\n",
      "Epoch 365/500, Generator loss: -0.49999821186065674, Discriminator loss: -0.00020319223403930664\n",
      "Epoch 366/500, Generator loss: -0.49987223744392395, Discriminator loss: -5.760788917541504e-05\n",
      "Epoch 367/500, Generator loss: -0.5000242590904236, Discriminator loss: -0.00011420249938964844\n",
      "Epoch 368/500, Generator loss: -0.49957454204559326, Discriminator loss: 6.0617923736572266e-05\n",
      "Epoch 369/500, Generator loss: -0.49958229064941406, Discriminator loss: -0.0002264082431793213\n",
      "Epoch 370/500, Generator loss: -0.5002502799034119, Discriminator loss: -0.0002695322036743164\n",
      "Epoch 371/500, Generator loss: -0.5002865195274353, Discriminator loss: -0.0007795393466949463\n",
      "Epoch 372/500, Generator loss: -0.4995500445365906, Discriminator loss: -7.534027099609375e-05\n",
      "Epoch 373/500, Generator loss: -0.5004692077636719, Discriminator loss: -0.0001386404037475586\n",
      "Epoch 374/500, Generator loss: -0.49996650218963623, Discriminator loss: 9.843707084655762e-05\n",
      "Epoch 375/500, Generator loss: -0.5004125833511353, Discriminator loss: -0.00039327144622802734\n",
      "Epoch 376/500, Generator loss: -0.5005645751953125, Discriminator loss: -0.00030803680419921875\n",
      "Epoch 377/500, Generator loss: -0.5002390742301941, Discriminator loss: -0.0001913309097290039\n",
      "Epoch 378/500, Generator loss: -0.5004030466079712, Discriminator loss: 0.00010776519775390625\n",
      "Epoch 379/500, Generator loss: -0.5005444884300232, Discriminator loss: 3.707408905029297e-05\n",
      "Epoch 380/500, Generator loss: -0.5001677870750427, Discriminator loss: -0.0001266002655029297\n",
      "Epoch 381/500, Generator loss: -0.4992001950740814, Discriminator loss: 0.00022515654563903809\n",
      "Epoch 382/500, Generator loss: -0.49891361594200134, Discriminator loss: 0.00024333596229553223\n",
      "Epoch 383/500, Generator loss: -0.5013654828071594, Discriminator loss: -2.562999725341797e-06\n",
      "Epoch 384/500, Generator loss: -0.500145673751831, Discriminator loss: 1.7881393432617188e-07\n",
      "Epoch 385/500, Generator loss: -0.49958521127700806, Discriminator loss: -0.0001182854175567627\n",
      "Epoch 386/500, Generator loss: -0.4988473355770111, Discriminator loss: -0.00014495849609375\n",
      "Epoch 387/500, Generator loss: -0.49955087900161743, Discriminator loss: 2.47955322265625e-05\n",
      "Epoch 388/500, Generator loss: -0.4988122284412384, Discriminator loss: -3.9070844650268555e-05\n",
      "Epoch 389/500, Generator loss: -0.4996930956840515, Discriminator loss: -0.0005308389663696289\n",
      "Epoch 390/500, Generator loss: -0.501147985458374, Discriminator loss: 0.0002484917640686035\n",
      "Epoch 391/500, Generator loss: -0.5021074414253235, Discriminator loss: 6.300210952758789e-05\n",
      "Epoch 392/500, Generator loss: -0.5013120174407959, Discriminator loss: 0.00022983551025390625\n",
      "Epoch 393/500, Generator loss: -0.49961167573928833, Discriminator loss: -4.118680953979492e-05\n",
      "Epoch 394/500, Generator loss: -0.4994789958000183, Discriminator loss: -5.0961971282958984e-05\n",
      "Epoch 395/500, Generator loss: -0.49929675459861755, Discriminator loss: 3.635883331298828e-06\n",
      "Epoch 396/500, Generator loss: -0.4991637170314789, Discriminator loss: 0.0002587437629699707\n",
      "Epoch 397/500, Generator loss: -0.49920767545700073, Discriminator loss: -7.033348083496094e-06\n",
      "Epoch 398/500, Generator loss: -0.49917668104171753, Discriminator loss: 3.382563591003418e-05\n",
      "Epoch 399/500, Generator loss: -0.4993831515312195, Discriminator loss: -0.00012153387069702148\n",
      "Epoch 400/500, Generator loss: -0.49987468123435974, Discriminator loss: -0.00030156970024108887\n",
      "Epoch 401/500, Generator loss: -0.5000435709953308, Discriminator loss: -1.1026859283447266e-05\n",
      "Epoch 402/500, Generator loss: -0.4999350309371948, Discriminator loss: 2.7418136596679688e-05\n",
      "Epoch 403/500, Generator loss: -0.5001384019851685, Discriminator loss: 2.753734588623047e-05\n",
      "Epoch 404/500, Generator loss: -0.5001428723335266, Discriminator loss: 7.265806198120117e-05\n",
      "Epoch 405/500, Generator loss: -0.5001415014266968, Discriminator loss: -1.043081283569336e-05\n",
      "Epoch 406/500, Generator loss: -0.5001028180122375, Discriminator loss: -2.7120113372802734e-05\n",
      "Epoch 407/500, Generator loss: -0.4999074637889862, Discriminator loss: -0.000130385160446167\n",
      "Epoch 408/500, Generator loss: -0.49992090463638306, Discriminator loss: 0.00017684698104858398\n",
      "Epoch 409/500, Generator loss: -0.5001559853553772, Discriminator loss: -5.7637691497802734e-05\n",
      "Epoch 410/500, Generator loss: -0.4999856948852539, Discriminator loss: -0.00014656782150268555\n",
      "Epoch 411/500, Generator loss: -0.500282883644104, Discriminator loss: -8.761882781982422e-06\n",
      "Epoch 412/500, Generator loss: -0.5001891255378723, Discriminator loss: -7.665157318115234e-05\n",
      "Epoch 413/500, Generator loss: -0.5005491971969604, Discriminator loss: -0.00010955333709716797\n",
      "Epoch 414/500, Generator loss: -0.500371515750885, Discriminator loss: -0.00019025802612304688\n",
      "Epoch 415/500, Generator loss: -0.49964383244514465, Discriminator loss: -0.00031107664108276367\n",
      "Epoch 416/500, Generator loss: -0.500080943107605, Discriminator loss: 5.179643630981445e-05\n",
      "Epoch 417/500, Generator loss: -0.5005978345870972, Discriminator loss: -0.00020706653594970703\n",
      "Epoch 418/500, Generator loss: -0.49981915950775146, Discriminator loss: -0.00012865662574768066\n",
      "Epoch 419/500, Generator loss: -0.4995317757129669, Discriminator loss: -2.2143125534057617e-05\n",
      "Epoch 420/500, Generator loss: -0.4993375837802887, Discriminator loss: -0.00012546777725219727\n",
      "Epoch 421/500, Generator loss: -0.499190092086792, Discriminator loss: 0.0001361072063446045\n",
      "Epoch 422/500, Generator loss: -0.4987561106681824, Discriminator loss: 0.0003923773765563965\n",
      "Epoch 423/500, Generator loss: -0.49880313873291016, Discriminator loss: -6.726384162902832e-05\n",
      "Epoch 424/500, Generator loss: -0.500203013420105, Discriminator loss: 5.030632019042969e-05\n",
      "Epoch 425/500, Generator loss: -0.5001488327980042, Discriminator loss: 0.00019425153732299805\n",
      "Epoch 426/500, Generator loss: -0.5003747344017029, Discriminator loss: -2.1457672119140625e-05\n",
      "Epoch 427/500, Generator loss: -0.5001686215400696, Discriminator loss: -3.8564205169677734e-05\n",
      "Epoch 428/500, Generator loss: -0.5001280903816223, Discriminator loss: -0.000298917293548584\n",
      "Epoch 429/500, Generator loss: -0.49995994567871094, Discriminator loss: 9.888410568237305e-05\n",
      "Epoch 430/500, Generator loss: -0.49985215067863464, Discriminator loss: -0.0003775656223297119\n",
      "Epoch 431/500, Generator loss: -0.5000266432762146, Discriminator loss: -5.84721565246582e-05\n",
      "Epoch 432/500, Generator loss: -0.500171422958374, Discriminator loss: 0.00016450881958007812\n",
      "Epoch 433/500, Generator loss: -0.500450849533081, Discriminator loss: 3.9577484130859375e-05\n",
      "Epoch 434/500, Generator loss: -0.50025874376297, Discriminator loss: -0.00021332502365112305\n",
      "Epoch 435/500, Generator loss: -0.5004289746284485, Discriminator loss: 2.110004425048828e-05\n",
      "Epoch 436/500, Generator loss: -0.5000643730163574, Discriminator loss: 5.054473876953125e-05\n",
      "Epoch 437/500, Generator loss: -0.5003685355186462, Discriminator loss: 0.00017970800399780273\n",
      "Epoch 438/500, Generator loss: -0.500532329082489, Discriminator loss: 1.3828277587890625e-05\n",
      "Epoch 439/500, Generator loss: -0.500399649143219, Discriminator loss: -0.0015234649181365967\n",
      "Epoch 440/500, Generator loss: -0.5000733137130737, Discriminator loss: -0.00045624375343322754\n",
      "Epoch 441/500, Generator loss: -0.500051736831665, Discriminator loss: 4.589557647705078e-05\n",
      "Epoch 442/500, Generator loss: -0.49979647994041443, Discriminator loss: -7.37905502319336e-05\n",
      "Epoch 443/500, Generator loss: -0.5007331371307373, Discriminator loss: -4.744529724121094e-05\n",
      "Epoch 444/500, Generator loss: -0.5005912184715271, Discriminator loss: -0.0007967054843902588\n",
      "Epoch 445/500, Generator loss: -0.5001997351646423, Discriminator loss: -0.0005814731121063232\n",
      "Epoch 446/500, Generator loss: -0.5005360841751099, Discriminator loss: -0.0003584623336791992\n",
      "Epoch 447/500, Generator loss: -0.5002039670944214, Discriminator loss: -7.265806198120117e-05\n",
      "Epoch 448/500, Generator loss: -0.5003452897071838, Discriminator loss: 4.470348358154297e-06\n",
      "Epoch 449/500, Generator loss: -0.5012730360031128, Discriminator loss: -0.001534193754196167\n",
      "Epoch 450/500, Generator loss: -0.4999725818634033, Discriminator loss: -0.0002225637435913086\n",
      "Epoch 451/500, Generator loss: -0.4994108974933624, Discriminator loss: -0.0009645521640777588\n",
      "Epoch 452/500, Generator loss: -0.5000774264335632, Discriminator loss: -0.00035077333450317383\n",
      "Epoch 453/500, Generator loss: -0.49958038330078125, Discriminator loss: -4.9114227294921875e-05\n",
      "Epoch 454/500, Generator loss: -0.5007208585739136, Discriminator loss: -0.0003255009651184082\n",
      "Epoch 455/500, Generator loss: -0.5006429553031921, Discriminator loss: -0.00016921758651733398\n",
      "Epoch 456/500, Generator loss: -0.4989393353462219, Discriminator loss: -7.092952728271484e-05\n",
      "Epoch 457/500, Generator loss: -0.499985933303833, Discriminator loss: -0.0007213056087493896\n",
      "Epoch 458/500, Generator loss: -0.4998478293418884, Discriminator loss: 0.00019025802612304688\n",
      "Epoch 459/500, Generator loss: -0.5004838109016418, Discriminator loss: -0.00018346309661865234\n",
      "Epoch 460/500, Generator loss: -0.4968624413013458, Discriminator loss: -8.597970008850098e-05\n",
      "Epoch 461/500, Generator loss: -0.5014647245407104, Discriminator loss: -0.001837223768234253\n",
      "Epoch 462/500, Generator loss: -0.5021145343780518, Discriminator loss: -0.0001773238182067871\n",
      "Epoch 463/500, Generator loss: -0.5001592636108398, Discriminator loss: -0.0007231235504150391\n",
      "Epoch 464/500, Generator loss: -0.500242292881012, Discriminator loss: -0.0001958608627319336\n",
      "Epoch 465/500, Generator loss: -0.500198245048523, Discriminator loss: 0.0004684031009674072\n",
      "Epoch 466/500, Generator loss: -0.4997834861278534, Discriminator loss: 0.00042378902435302734\n",
      "Epoch 467/500, Generator loss: -0.49951696395874023, Discriminator loss: -0.0008935630321502686\n",
      "Epoch 468/500, Generator loss: -0.4999305009841919, Discriminator loss: -0.0001131892204284668\n",
      "Epoch 469/500, Generator loss: -0.4996926784515381, Discriminator loss: 0.0001157224178314209\n",
      "Epoch 470/500, Generator loss: -0.49974894523620605, Discriminator loss: -7.081031799316406e-05\n",
      "Epoch 471/500, Generator loss: -0.4993172585964203, Discriminator loss: -0.00017833709716796875\n",
      "Epoch 472/500, Generator loss: -0.49971893429756165, Discriminator loss: -0.00030156970024108887\n",
      "Epoch 473/500, Generator loss: -0.49965357780456543, Discriminator loss: 0.00030300021171569824\n",
      "Epoch 474/500, Generator loss: -0.4991251528263092, Discriminator loss: -0.0001074075698852539\n",
      "Epoch 475/500, Generator loss: -0.4993917644023895, Discriminator loss: 0.0006451308727264404\n",
      "Epoch 476/500, Generator loss: -0.49861302971839905, Discriminator loss: -0.00028887391090393066\n",
      "Epoch 477/500, Generator loss: -0.49888846278190613, Discriminator loss: -0.0009319484233856201\n",
      "Epoch 478/500, Generator loss: -0.5009118318557739, Discriminator loss: -0.0003230571746826172\n",
      "Epoch 479/500, Generator loss: -0.501098096370697, Discriminator loss: -0.00033539533615112305\n",
      "Epoch 480/500, Generator loss: -0.5007094740867615, Discriminator loss: -1.6927719116210938e-05\n",
      "Epoch 481/500, Generator loss: -0.5004642009735107, Discriminator loss: -0.00024896860122680664\n",
      "Epoch 482/500, Generator loss: -0.5006064772605896, Discriminator loss: -0.0001551508903503418\n",
      "Epoch 483/500, Generator loss: -0.5005531907081604, Discriminator loss: -4.172325134277344e-06\n",
      "Epoch 484/500, Generator loss: -0.5003738403320312, Discriminator loss: -0.000448763370513916\n",
      "Epoch 485/500, Generator loss: -0.49991264939308167, Discriminator loss: 0.0001830458641052246\n",
      "Epoch 486/500, Generator loss: -0.500203549861908, Discriminator loss: 0.00015014410018920898\n",
      "Epoch 487/500, Generator loss: -0.49857309460639954, Discriminator loss: -5.4448843002319336e-05\n",
      "Epoch 488/500, Generator loss: -0.49887874722480774, Discriminator loss: -0.00016894936561584473\n",
      "Epoch 489/500, Generator loss: -0.5000224113464355, Discriminator loss: -0.00042942166328430176\n",
      "Epoch 490/500, Generator loss: -0.5002596974372864, Discriminator loss: -3.159046173095703e-06\n",
      "Epoch 491/500, Generator loss: -0.5007882714271545, Discriminator loss: 9.000301361083984e-05\n",
      "Epoch 492/500, Generator loss: -0.49998044967651367, Discriminator loss: -2.9265880584716797e-05\n",
      "Epoch 493/500, Generator loss: -0.4993104338645935, Discriminator loss: -7.718801498413086e-05\n",
      "Epoch 494/500, Generator loss: -0.49913519620895386, Discriminator loss: -3.93986701965332e-05\n",
      "Epoch 495/500, Generator loss: -0.49960049986839294, Discriminator loss: -0.0007879734039306641\n",
      "Epoch 496/500, Generator loss: -0.5004357099533081, Discriminator loss: -1.0788440704345703e-05\n",
      "Epoch 497/500, Generator loss: -0.5004873275756836, Discriminator loss: -0.0003268122673034668\n",
      "Epoch 498/500, Generator loss: -0.5001175403594971, Discriminator loss: -7.56978988647461e-05\n",
      "Epoch 499/500, Generator loss: -0.4999205470085144, Discriminator loss: -0.0004317760467529297\n",
      "Epoch 500/500, Generator loss: -0.49986448884010315, Discriminator loss: 0.00024256110191345215\n"
     ]
    }
   ],
   "source": [
    "wgan = WGAN(dataset, num_latent_variables=num_latent_variables, lr=learning_rate, weight_clip=0.01)\n",
    "\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "num_epochs = 500\n",
    "num_critic = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for _, (signal_tensor, params_tensor) in enumerate(train_loader):\n",
    "        z = torch.randn(1, num_latent_variables, 1).to(device)\n",
    "        signal_tensor = signal_tensor.to(device)\n",
    "        params_tensor = params_tensor.to(device)\n",
    "        \n",
    "        for _ in range(num_critic):\n",
    "            d_loss = wgan.train_discriminator(signal_tensor, params_tensor, z)\n",
    "        \n",
    "        for _ in range(3):\n",
    "            g_loss = wgan.train_generator(signal_tensor, z)\n",
    "        \n",
    "        g_loss_list.append(g_loss)\n",
    "        d_loss_list.append(d_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Generator loss: {g_loss}, Discriminator loss: {d_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.516662638377852, 0.35131111238062546, 0.7761564027388659]\n",
      "[8.10767    0.03027448 1.7619146 ]\n",
      "[11.386962099568967, 0.2691621545406089, 5.227371338521358]\n",
      "[8.9362335 0.8399225 1.6329257]\n",
      "[11.24123948164049, 0.3038955280462225, 2.936260514270033]\n",
      "[7.901222  0.9078885 2.0599055]\n",
      "[9.211320514478482, 0.24156178786844032, 3.04782728482806]\n",
      "[ 8.445425   -0.01911383  2.1105208 ]\n",
      "[8.2615674281253, 0.3644684677908152, 3.04634042010434]\n",
      "[ 8.28656   -1.0420303  2.4901261]\n",
      "[12.295280524941665, 0.4624376439106851, 2.5764263003770385]\n",
      "[ 8.960342   -0.17930752  1.3982818 ]\n",
      "[12.627943419968261, 0.25454700894297994, 1.8112882596879538]\n",
      "[ 8.485913   -0.05174568  1.833298  ]\n",
      "[9.326511704200678, 0.2657468423789364, 4.029232419880975]\n",
      "[8.686554  0.4992325 1.6702275]\n",
      "[10.856494266360775, 0.2507966523264037, 0.10004731788375429]\n",
      "[9.125658  1.20937   2.1629944]\n",
      "[11.075478086171902, 0.3624165609784602, 0.43073710118276265]\n",
      "[7.0215025  0.57749665 0.42306072]\n"
     ]
    }
   ],
   "source": [
    "generator = wgan.generator\n",
    "generator.eval()\n",
    "\n",
    "for i in range(10):\n",
    "    TS = Signal_Generator(num_sources=1, noise_amplitude=1)\n",
    "    test_data = TS.generating_signal()\n",
    "    params = TS.printing_parameters()\n",
    "\n",
    "    input_signal = test_data['Signal'].values\n",
    "    input_signal_tensor = torch.tensor(input_signal, dtype=torch.float).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_params = generator(input_signal_tensor, z).squeeze().cpu().numpy()\n",
    "\n",
    "    print(params)\n",
    "    print(generated_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasLISA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
